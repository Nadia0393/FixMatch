{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Preprocessing","metadata":{}},{"cell_type":"code","source":"pip install imutils","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:08.062251Z","iopub.execute_input":"2023-05-08T23:01:08.062686Z","iopub.status.idle":"2023-05-08T23:01:10.043452Z","shell.execute_reply.started":"2023-05-08T23:01:08.062656Z","shell.execute_reply":"2023-05-08T23:01:10.042073Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nfrom tqdm import tqdm\nimport cv2\nimport os\nimport imutils\n\n\n\ndef crop_img(img):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n\n    # threshold the image, then perform a series of erosions +\n    # dilations to remove any small regions of noise\n    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n    thresh = cv2.erode(thresh, None, iterations=2)\n    thresh = cv2.dilate(thresh, None, iterations=2)\n\n    # find contours in thresholded image, then grab the largest one\n    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    c = max(cnts, key=cv2.contourArea)\n\n    # find the extreme points\n    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n    extRight = tuple(c[c[:, :, 0].argmax()][0])\n    extTop = tuple(c[c[:, :, 1].argmin()][0])\n    extBot = tuple(c[c[:, :, 1].argmax()][0])\n    ADD_PIXELS = 0\n    new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n\n    return new_img\n\nif __name__ == \"__main__\":\n    training = \"/kaggle/input/brain-tumor-mri-dataset/Training\"\n    testing = \"/kaggle/input/brain-tumor-mri-dataset/Testing\"\n    training_dir = os.listdir(training)\n    testing_dir = os.listdir(testing)\n    IMG_SIZE = 256\n\n    for dir in training_dir:\n        save_path = '/kaggle/working/cleaned/Training/'+ dir\n        path = os.path.join(training,dir)\n        image_dir = os.listdir(path)\n        for img in image_dir:\n            image = cv2.imread(os.path.join(path,img))\n            new_img = crop_img(image)\n            new_img = cv2.resize(new_img,(IMG_SIZE,IMG_SIZE))\n            if not os.path.exists(save_path):\n                os.makedirs(save_path)\n            cv2.imwrite(save_path+'/'+img, new_img)\n\n    for dir in testing_dir:\n        save_path = '/kaggle/working/cleaned/Testing/'+ dir\n        path = os.path.join(testing,dir)\n        image_dir = os.listdir(path)\n        for img in image_dir:\n            image = cv2.imread(os.path.join(path,img))\n            new_img = crop_img(image)\n            new_img = cv2.resize(new_img,(IMG_SIZE,IMG_SIZE))\n            if not os.path.exists(save_path):\n                os.makedirs(save_path)\n            cv2.imwrite(save_path+'/'+img, new_img)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:10.046189Z","iopub.execute_input":"2023-05-08T23:01:10.046671Z","iopub.status.idle":"2023-05-08T23:01:15.519341Z","shell.execute_reply.started":"2023-05-08T23:01:10.046627Z","shell.execute_reply":"2023-05-08T23:01:15.516535Z"},"trusted":true},"execution_count":28,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(path)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image_dir:\n\u001b[0;32m---> 49\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     new_img \u001b[38;5;241m=\u001b[39m crop_img(image)\n\u001b[1;32m     51\u001b[0m     new_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(new_img,(IMG_SIZE,IMG_SIZE))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"import logging\nimport math\n\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.datasets import DatasetFolder\nimport random\n\nimport numpy as np\nimport PIL\nimport PIL.ImageOps\nimport PIL.ImageEnhance\nimport PIL.ImageDraw\nfrom PIL import Image\n\nimport torch\nimport argparse\nimport logging\nimport math\nimport os\nimport random\nimport shutil\nimport time\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nimport pandas as pd\nfrom torchvision.datasets import DatasetFolder\nfrom torchvision.datasets.folder import default_loader\nfrom torchvision.transforms import ToTensor\n\nfrom copy import deepcopy\n\nlogger = logging.getLogger(__name__)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.520156Z","iopub.status.idle":"2023-05-08T23:01:15.520494Z","shell.execute_reply.started":"2023-05-08T23:01:15.520338Z","shell.execute_reply":"2023-05-08T23:01:15.520354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# randaugment.py","metadata":{}},{"cell_type":"code","source":"PARAMETER_MAX = 10\n\n\ndef AutoContrast(img, **kwarg):\n    return PIL.ImageOps.autocontrast(img)\n\n\ndef Brightness(img, v, max_v, bias=0):\n    v = _float_parameter(v, max_v) + bias\n    return PIL.ImageEnhance.Brightness(img).enhance(v)\n\n\ndef Color(img, v, max_v, bias=0):\n    v = _float_parameter(v, max_v) + bias\n    return PIL.ImageEnhance.Color(img).enhance(v)\n\n\ndef Contrast(img, v, max_v, bias=0):\n    v = _float_parameter(v, max_v) + bias\n    return PIL.ImageEnhance.Contrast(img).enhance(v)\n\n\ndef Cutout(img, v, max_v, bias=0):\n    if v == 0:\n        return img\n    v = _float_parameter(v, max_v) + bias\n    v = int(v * min(img.size))\n    return CutoutAbs(img, v)\n\n\ndef CutoutAbs(img, v, **kwarg):\n    w, h = img.size\n    x0 = np.random.uniform(0, w)\n    y0 = np.random.uniform(0, h)\n    x0 = int(max(0, x0 - v / 2.))\n    y0 = int(max(0, y0 - v / 2.))\n    x1 = int(min(w, x0 + v))\n    y1 = int(min(h, y0 + v))\n    xy = (x0, y0, x1, y1)\n    # gray\n    color = (127, 127, 127)\n    img = img.copy()\n    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n    return img\n\n\ndef Equalize(img, **kwarg):\n    return PIL.ImageOps.equalize(img)\n\n\ndef Identity(img, **kwarg):\n    return img\n\n\ndef Invert(img, **kwarg):\n    return PIL.ImageOps.invert(img)\n\n\ndef Posterize(img, v, max_v, bias=0):\n    v = _int_parameter(v, max_v) + bias\n    return PIL.ImageOps.posterize(img, v)\n\n\ndef Rotate(img, v, max_v, bias=0):\n    v = _int_parameter(v, max_v) + bias\n    if random.random() < 0.5:\n        v = -v\n    return img.rotate(v)\n\n\ndef Sharpness(img, v, max_v, bias=0):\n    v = _float_parameter(v, max_v) + bias\n    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n\n\ndef ShearX(img, v, max_v, bias=0):\n    v = _float_parameter(v, max_v) + bias\n    if random.random() < 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n\n\ndef ShearY(img, v, max_v, bias=0):\n    v = _float_parameter(v, max_v) + bias\n    if random.random() < 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n\n\ndef Solarize(img, v, max_v, bias=0):\n    v = _int_parameter(v, max_v) + bias\n    return PIL.ImageOps.solarize(img, 256 - v)\n\n\ndef SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n    v = _int_parameter(v, max_v) + bias\n    if random.random() < 0.5:\n        v = -v\n    img_np = np.array(img).astype(np.int)\n    img_np = img_np + v\n    img_np = np.clip(img_np, 0, 255)\n    img_np = img_np.astype(np.uint8)\n    img = Image.fromarray(img_np)\n    return PIL.ImageOps.solarize(img, threshold)\n\n\ndef TranslateX(img, v, max_v, bias=0):\n    v = _float_parameter(v, max_v) + bias\n    if random.random() < 0.5:\n        v = -v\n    v = int(v * img.size[0])\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n\n\ndef TranslateY(img, v, max_v, bias=0):\n    v = _float_parameter(v, max_v) + bias\n    if random.random() < 0.5:\n        v = -v\n    v = int(v * img.size[1])\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n\n\ndef _float_parameter(v, max_v):\n    return float(v) * max_v / PARAMETER_MAX\n\n\ndef _int_parameter(v, max_v):\n    return int(v * max_v / PARAMETER_MAX)\n\n\ndef fixmatch_augment_pool():\n    # FixMatch paper\n    augs = [(AutoContrast, None, None),\n            (Brightness, 0.9, 0.05),\n            (Color, 0.9, 0.05),\n            (Contrast, 0.9, 0.05),\n            (Equalize, None, None),\n            (Identity, None, None),\n            (Posterize, 4, 4),\n            (Rotate, 30, 0),\n            (Sharpness, 0.9, 0.05),\n            (ShearX, 0.3, 0),\n            (ShearY, 0.3, 0),\n            (Solarize, 256, 0),\n            (TranslateX, 0.3, 0),\n            (TranslateY, 0.3, 0)]\n    return augs\n\n\ndef my_augment_pool():\n    # Test\n    augs = [(AutoContrast, None, None),\n            (Brightness, 1.8, 0.1),\n            (Color, 1.8, 0.1),\n            (Contrast, 1.8, 0.1),\n            (Cutout, 0.2, 0),\n            (Equalize, None, None),\n            (Invert, None, None),\n            (Posterize, 4, 4),\n            (Rotate, 30, 0),\n            (Sharpness, 1.8, 0.1),\n            (ShearX, 0.3, 0),\n            (ShearY, 0.3, 0),\n            (Solarize, 256, 0),\n            (SolarizeAdd, 110, 0),\n            (TranslateX, 0.45, 0),\n            (TranslateY, 0.45, 0)]\n    return augs\n\n\nclass RandAugmentPC(object):\n    def __init__(self, n, m):\n        assert n >= 1\n        assert 1 <= m <= 10\n        self.n = n\n        self.m = m\n        self.augment_pool = my_augment_pool()\n\n    def __call__(self, img):\n        ops = random.choices(self.augment_pool, k=self.n)\n        for op, max_v, bias in ops:\n            prob = np.random.uniform(0.2, 0.8)\n            if random.random() + prob >= 1:\n                img = op(img, v=self.m, max_v=max_v, bias=bias)\n        img = CutoutAbs(img, int(32*0.5))\n        return img\n\n\nclass RandAugmentMC(object):\n    def __init__(self, n, m):\n        assert n >= 1\n        assert 1 <= m <= 10\n        self.n = n\n        self.m = m\n        self.augment_pool = fixmatch_augment_pool()\n\n    def __call__(self, img):\n        ops = random.choices(self.augment_pool, k=self.n)\n        for op, max_v, bias in ops:\n            v = np.random.randint(1, self.m)\n            if random.random() < 0.5:\n                img = op(img, v=v, max_v=max_v, bias=bias)\n        img = CutoutAbs(img, int(32*0.5))\n        return img","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.522248Z","iopub.status.idle":"2023-05-08T23:01:15.523178Z","shell.execute_reply.started":"2023-05-08T23:01:15.522841Z","shell.execute_reply":"2023-05-08T23:01:15.522866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dataset.py","metadata":{}},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\n\ncifar10_mean = (0.4914, 0.4822, 0.4465)\ncifar10_std = (0.2471, 0.2435, 0.2616)\ncifar100_mean = (0.5071, 0.4867, 0.4408)\ncifar100_std = (0.2675, 0.2565, 0.2761)\nnormal_mean = (0.5, 0.5, 0.5)\nnormal_std = (0.5, 0.5, 0.5)\nimg_mean = (0.485, 0.456, 0.406)\nimg_std = (0.229, 0.224, 0.225)\ntrain_dir = '/kaggle/working/cleaned/Training'\ntest_dir = '/kaggle/working/cleaned/Testing'\n#Custom SSL function \nclass CustomSSL(DatasetFolder):\n    def __init__(self, root, indexs=None, transform=None, target_transform=None):\n        super().__init__(root=root, loader=default_loader, extensions=['.jpg'], transform=transform, target_transform=target_transform)\n        if indexs is not None:\n            self.samples = [self.samples[i] for i in indexs]\n            self.transform = transform if transform is not None else ToTensor()\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n\n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n    @staticmethod\n    def collate_fn(batch):#function to create batch files similar to cifar10\n        images = []\n        targets = []\n        for sample in batch:\n            image, target = sample\n            if isinstance(image, PIL.Image.Image):\n                image = transforms.ToTensor()(image)\n            images.append(image)\n            targets.append(target)\n        return torch.stack(images), torch.tensor(targets)\n\ndef get_cifar10(args, root):\n    transform_labeled = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomCrop(size=32,\n                              padding=int(32*0.125),\n                              padding_mode='reflect'),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n    ])\n    transform_val = transforms.Compose([transforms.Resize(256),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n    ])\n    train_labeled_idxs, train_unlabeled_idxs = x_u_split(args, base_dataset.targets)\n    train_labeled_dataset = CustomSSL(train_dir, train_labeled_idxs,transform=transform_labeled)\n    train_unlabeled_dataset = CustomSSL(train_dir, train_unlabeled_idxs,transform=TransformFixMatch(mean=cifar10_mean, std=cifar10_std))\n    test_dataset = datasets.ImageFolder(root=test_dir,transform=transform_val)\n    return train_labeled_dataset, train_unlabeled_dataset,test_dataset \ndef x_u_split(args, labels):\n    label_per_class = args.num_labeled // args.num_classes\n    labels = np.array(labels)\n    labeled_idx = []\n    unlabeled_idx = np.array(range(len(labels)))\n    for i in range(args.num_classes):\n        idx = np.where(labels == i)[0]\n        idx = np.random.choice(idx, label_per_class, False)\n        labeled_idx.extend(idx)\n    labeled_idx = np.array(labeled_idx)\n    assert len(labeled_idx) == args.num_labeled\n\n    if args.expand_labels or args.num_labeled < args.batch_size:\n        num_expand_x = math.ceil(\n            args.batch_size * args.eval_step / args.num_labeled)\n        labeled_idx = np.hstack([labeled_idx for _ in range(num_expand_x)])\n    np.random.shuffle(labeled_idx)\n    return labeled_idx, unlabeled_idx\n\n\nclass TransformFixMatch(object):\n    def __init__(self, mean, std):\n        self.weak = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomCrop(size=32,\n                                  padding=int(32*0.125),\n                                  padding_mode='reflect')])\n        self.strong = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomCrop(size=32,\n                                  padding=int(32*0.125),\n                                  padding_mode='reflect'),\n            RandAugmentMC(n=2, m=10)])\n        self.normalize = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)])\n\n    def __call__(self, x):\n        weak = self.weak(x)\n        strong = self.strong(x)\n        return self.normalize(weak), self.normalize(strong)\nbase_dataset = datasets.ImageFolder(root=train_dir)\ndataloader = DataLoader(base_dataset, batch_size=32, shuffle=True, collate_fn=CustomSSL.collate_fn)#function call to create batch files\nDATASET_GETTERS = {'cifar10': get_cifar10}","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.524640Z","iopub.status.idle":"2023-05-08T23:01:15.525296Z","shell.execute_reply.started":"2023-05-08T23:01:15.525045Z","shell.execute_reply":"2023-05-08T23:01:15.525067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# utils.py","metadata":{}},{"cell_type":"code","source":"__all__ = ['get_mean_and_std', 'accuracy', 'AverageMeter']\n\n\ndef get_mean_and_std(dataset):\n    '''Compute the mean and std value of dataset.'''\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=False)\n\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    logger.info('==> Computing mean and std..')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\n       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.526701Z","iopub.status.idle":"2023-05-08T23:01:15.527294Z","shell.execute_reply.started":"2023-05-08T23:01:15.527059Z","shell.execute_reply":"2023-05-08T23:01:15.527080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import model","metadata":{}},{"cell_type":"code","source":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n    elif model_name == \"resnet50\":\n        \"\"\" Resnet50\n        \"\"\"\n        model_ft = models.resnet50(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n        \"\"\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n        \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n    elif model_name == \"densenet201\":\n        \"\"\" Densenet 201\n        \"\"\"\n        model_ft = models.densenet201(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n    \n    return model_ft, input_size","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.528974Z","iopub.status.idle":"2023-05-08T23:01:15.529812Z","shell.execute_reply.started":"2023-05-08T23:01:15.529566Z","shell.execute_reply":"2023-05-08T23:01:15.529592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.531317Z","iopub.status.idle":"2023-05-08T23:01:15.531793Z","shell.execute_reply.started":"2023-05-08T23:01:15.531548Z","shell.execute_reply":"2023-05-08T23:01:15.531586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize the model for this run","metadata":{}},{"cell_type":"code","source":"model_name = \"resnet50\"\nnum_classes = 4\n# Batch size for training (change depending on how much memory you have)\nbatch_size =  10\n# Number of epochs to train for\nnum_epochs = 20\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\n\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract=True, use_pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.533391Z","iopub.status.idle":"2023-05-08T23:01:15.533890Z","shell.execute_reply.started":"2023-05-08T23:01:15.533643Z","shell.execute_reply":"2023-05-08T23:01:15.533665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model_ft)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.535493Z","iopub.status.idle":"2023-05-08T23:01:15.536370Z","shell.execute_reply.started":"2023-05-08T23:01:15.536138Z","shell.execute_reply":"2023-05-08T23:01:15.536159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelEMA(object):\n    def __init__(self, args, model, decay):\n        self.ema = deepcopy(model)\n        self.ema.to(args.device)\n        self.ema.eval()\n        self.decay = decay\n        self.ema_has_module = hasattr(self.ema, 'module')\n        # Fix EMA. https://github.com/valencebond/FixMatch_pytorch thank you!\n        self.param_keys = [k for k, _ in self.ema.named_parameters()]\n        self.buffer_keys = [k for k, _ in self.ema.named_buffers()]\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n\n    def update(self, model):\n        needs_module = hasattr(model, 'module') and not self.ema_has_module\n        with torch.no_grad():\n            msd = model.state_dict()\n            esd = self.ema.state_dict()\n            for k in self.param_keys:\n                if needs_module:\n                    j = 'module.' + k\n                else:\n                    j = k\n                model_v = msd[j].detach()\n                ema_v = esd[k]\n                esd[k].copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n\n            for k in self.buffer_keys:\n                if needs_module:\n                    j = 'module.' + k\n                else:\n                    j = k\n                esd[k].copy_(msd[j])","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.537403Z","iopub.status.idle":"2023-05-08T23:01:15.538366Z","shell.execute_reply.started":"2023-05-08T23:01:15.538132Z","shell.execute_reply":"2023-05-08T23:01:15.538156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resnext","metadata":{}},{"cell_type":"code","source":"import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nlogger = logging.getLogger(__name__)\n\n\ndef mish(x):\n    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function (https://arxiv.org/abs/1908.08681)\"\"\"\n    return x * torch.tanh(F.softplus(x))\n\n\nclass BatchNorm2d(nn.BatchNorm2d):\n    \"\"\"How Does BN Increase Collapsed Neural Network Filters? (https://arxiv.org/abs/2001.11216)\"\"\"\n\n    def __init__(self, num_features, alpha=0.1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n        self.alpha = alpha\n\n    def forward(self, x):\n        return super().forward(x) + self.alpha\n\n\nclass ResNeXtBottleneck(nn.Module):\n    \"\"\"\n    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, stride,\n                 cardinality, base_width, widen_factor):\n        \"\"\" Constructor\n        Args:\n            in_channels: input channel dimensionality\n            out_channels: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            cardinality: num of convolution groups.\n            base_width: base number of channels in each group.\n            widen_factor: factor to reduce the input dimensionality before convolution.\n        \"\"\"\n        super().__init__()\n        width_ratio = out_channels / (widen_factor * 64.)\n        D = cardinality * int(base_width * width_ratio)\n        self.conv_reduce = nn.Conv2d(\n            in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_reduce = nn.BatchNorm2d(D, momentum=0.001)\n        self.conv_conv = nn.Conv2d(D, D,\n                                   kernel_size=3, stride=stride, padding=1,\n                                   groups=cardinality, bias=False)\n        self.bn = nn.BatchNorm2d(D, momentum=0.001)\n        self.act = mish\n        self.conv_expand = nn.Conv2d(\n            D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_expand = nn.BatchNorm2d(out_channels, momentum=0.001)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module('shortcut_conv',\n                                     nn.Conv2d(in_channels, out_channels,\n                                               kernel_size=1,\n                                               stride=stride,\n                                               padding=0,\n                                               bias=False))\n            self.shortcut.add_module(\n                'shortcut_bn', nn.BatchNorm2d(out_channels, momentum=0.001))\n\n    def forward(self, x):\n        bottleneck = self.conv_reduce.forward(x)\n        bottleneck = self.act(self.bn_reduce.forward(bottleneck))\n        bottleneck = self.conv_conv.forward(bottleneck)\n        bottleneck = self.act(self.bn.forward(bottleneck))\n        bottleneck = self.conv_expand.forward(bottleneck)\n        bottleneck = self.bn_expand.forward(bottleneck)\n        residual = self.shortcut.forward(x)\n        return self.act(residual + bottleneck)\n\n\nclass CifarResNeXt(nn.Module):\n    \"\"\"\n    ResNext optimized for the Cifar dataset, as specified in\n    https://arxiv.org/pdf/1611.05431.pdf\n    \"\"\"\n\n    def __init__(self, cardinality, depth, num_classes,\n                 base_width, widen_factor=4):\n        \"\"\" Constructor\n        Args:\n            cardinality: number of convolution groups.\n            depth: number of layers.\n            nlabels: number of classes\n            base_width: base number of channels in each group.\n            widen_factor: factor to adjust the channel dimensionality\n        \"\"\"\n        super().__init__()\n        self.cardinality = cardinality\n        self.depth = depth\n        self.block_depth = (self.depth - 2) // 9\n        self.base_width = base_width\n        self.widen_factor = widen_factor\n        self.nlabels = num_classes\n        self.output_size = 64\n        self.stages = [64, 64 * self.widen_factor, 128 *\n                       self.widen_factor, 256 * self.widen_factor]\n\n        self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n        self.bn_1 = nn.BatchNorm2d(64, momentum=0.001)\n        self.act = mish\n        self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 1)\n        self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 2)\n        self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 2)\n        self.classifier = nn.Linear(self.stages[3], num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight,\n                                        mode='fan_out',\n                                        nonlinearity='leaky_relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1.0)\n                nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0.0)\n\n    def block(self, name, in_channels, out_channels, pool_stride=2):\n        \"\"\" Stack n bottleneck modules where n is inferred from the depth of the network.\n        Args:\n            name: string name of the current block.\n            in_channels: number of input channels\n            out_channels: number of output channels\n            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n        Returns: a Module consisting of n sequential bottlenecks.\n        \"\"\"\n        block = nn.Sequential()\n        for bottleneck in range(self.block_depth):\n            name_ = '%s_bottleneck_%d' % (name, bottleneck)\n            if bottleneck == 0:\n                block.add_module(name_, ResNeXtBottleneck(in_channels,\n                                                          out_channels,\n                                                          pool_stride,\n                                                          self.cardinality,\n                                                          self.base_width,\n                                                          self.widen_factor))\n            else:\n                block.add_module(name_,\n                                 ResNeXtBottleneck(out_channels,\n                                                   out_channels,\n                                                   1,\n                                                   self.cardinality,\n                                                   self.base_width,\n                                                   self.widen_factor))\n        return block\n\n    def forward(self, x):\n        x = self.conv_1_3x3.forward(x)\n        x = self.act(self.bn_1.forward(x))\n        x = self.stage_1.forward(x)\n        x = self.stage_2.forward(x)\n        x = self.stage_3.forward(x)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = x.view(-1, self.stages[3])\n        return self.classifier(x)\n\n\ndef build_resnext(cardinality, depth, width, num_classes):\n    logger.info(f\"Model: ResNeXt {depth+1}x{width}\")\n    return CifarResNeXt(cardinality=cardinality,\n                        depth=depth,\n                        base_width=width,\n                        num_classes=num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.539857Z","iopub.status.idle":"2023-05-08T23:01:15.540341Z","shell.execute_reply.started":"2023-05-08T23:01:15.540097Z","shell.execute_reply":"2023-05-08T23:01:15.540120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train.py","metadata":{}},{"cell_type":"code","source":"def get_confusion_matrix(test_dataset, device):\n    nb_classes = 4\n\n    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n    with torch.no_grad():\n        for i, (inputs, classes) in enumerate(test_dataset):\n            inputs = inputs.to(device)\n            classes = classes.to(device)\n            outputs = model_ft(inputs)\n            _, preds = torch.max(outputs, 1)\n            for t, p in zip(classes.view(-1), preds.view(-1)):\n                    confusion_matrix[t.long(), p.long()] += 1\n\n    print(confusion_matrix)\n    return confusion_matrix\n\ndef get_accuracy(confusion_matrix):\n    correct = torch.sum(torch.diag(confusion_matrix))\n    total = torch.sum(confusion_matrix)\n    accuracy = correct / total\n    return accuracy.item() * 100.0","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.542367Z","iopub.status.idle":"2023-05-08T23:01:15.543596Z","shell.execute_reply.started":"2023-05-08T23:01:15.543286Z","shell.execute_reply":"2023-05-08T23:01:15.543310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_acc = 0\n\nglobal test_loader\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n\ndef get_cosine_schedule_with_warmup(optimizer,\n                                    num_warmup_steps,\n                                    num_training_steps,\n                                    num_cycles=7./16.,\n                                    last_epoch=-1):\n    def _lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        no_progress = float(current_step - num_warmup_steps) / \\\n            float(max(1, num_training_steps - num_warmup_steps))\n        return max(0., math.cos(math.pi * num_cycles * no_progress))\n\n    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n\n\ndef interleave(x, size):\n    s = list(x.shape)\n    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n\n\ndef de_interleave(x, size):\n    s = list(x.shape)\n    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='PyTorch FixMatch Training')\n    parser.add_argument('--dataset', default='cifar10', type=str,\n                        choices=['cifar10'],\n                        help='dataset name')\n    parser.add_argument('--num-labeled', type=int, default=4000,\n                        help='number of labeled data')\n    parser.add_argument(\"--expand-labels\", action=\"store_true\", default=True,\n                        help=\"expand labels to fit eval steps\")\n    parser.add_argument('--arch', default='resnet', type=str,\n                        choices=['resnet'],\n                        help='dataset name')\n    parser.add_argument('--total-steps', default=2**14, type=int,\n                        help='number of total steps to run')\n    parser.add_argument('--eval-step', default=2**5, type=int,\n                        help='number of eval steps to run')\n    parser.add_argument('--start-epoch', default=0, type=int,\n                        help='manual epoch number (useful on restarts)')\n    parser.add_argument('--batch-size', default=2**6, type=int,\n                        help='train batchsize')\n    parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n                        help='initial learning rate')\n    parser.add_argument('--warmup', default=0, type=float,\n                        help='warmup epochs (unlabeled data based)')\n    parser.add_argument('--wdecay', default=5e-4, type=float,\n                        help='weight decay')\n    parser.add_argument('--nesterov', action='store_true', default=True,\n                        help='use nesterov momentum')\n    parser.add_argument('--mu', default=7, type=int,\n                        help='coefficient of unlabeled batch size')\n    parser.add_argument('--lambda-u', default=1, type=float,\n                        help='coefficient of unlabeled loss')\n    parser.add_argument('--T', default=1, type=float,\n                        help='pseudo label temperature')\n    parser.add_argument('--threshold', default=0.95, type=float,\n                        help='pseudo label threshold')\n    parser.add_argument('--out', default='result',\n                        help='directory to output the result')\n    parser.add_argument('--resume', default='', type=str,\n                        help='path to latest checkpoint (default: none)')\n    parser.add_argument('--seed', default=None, type=int,\n                        help=\"random seed\")\n    parser.add_argument('--no-progress', action='store_true',\n                        help=\"don't use progress bar\")\n    parser.add_argument('--use-ema', action='store_true', default=True,\n                        help='use EMA model')\n    parser.add_argument('--ema-decay', default=0.999, type=float,\n                        help='EMA decay rate')\n\n    args = parser.parse_args(\"\")\n    print(args)\n    global best_acc\n\n    def create_model(args):\n        if args.arch == 'resnet':\n            return model_ft\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    args.world_size = 1\n    args.device = device\n    os.makedirs(args.out, exist_ok=True)\n    args.writer = SummaryWriter(args.out)\n\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO)\n\n    logger.warning(\n        f\"device: {args.device}, \")\n\n    logger.info(dict(args._get_kwargs()))\n    \n    if args.dataset == 'cifar10':\n        args.num_classes = 4\n\n    elif args.dataset == 'cifar100':\n        args.num_classes = 100\n\n    if args.seed is not None:\n        set_seed(args)\n\n    labeled_dataset, unlabeled_dataset,test_dataset = DATASET_GETTERS[args.dataset](\n        args, './data')\n\n\n    train_sampler = RandomSampler\n\n    labeled_trainloader = DataLoader(\n        labeled_dataset,\n        sampler=train_sampler(labeled_dataset),\n        batch_size=args.batch_size,\n        drop_last=True)\n\n    unlabeled_trainloader = DataLoader(\n        unlabeled_dataset,\n        sampler=train_sampler(unlabeled_dataset),\n        batch_size=args.batch_size*args.mu,\n        drop_last=True)\n\n    test_loader = DataLoader(\n        test_dataset,\n        sampler=SequentialSampler(test_dataset),\n        batch_size=args.batch_size)\n\n    model = create_model(args)\n    model.to(args.device)\n\n    no_decay = ['bias', 'bn']\n    grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(\n            nd in n for nd in no_decay)], 'weight_decay': args.wdecay},\n        {'params': [p for n, p in model.named_parameters() if any(\n            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = optim.SGD(grouped_parameters, lr=args.lr,\n                          momentum=0.9, nesterov=args.nesterov)\n\n    args.epochs = math.ceil(args.total_steps / args.eval_step)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, args.warmup, args.total_steps)\n    \n    if args.use_ema:\n        ema_model = ModelEMA(args, model, args.ema_decay)\n        \n    args.start_epoch = 0\n\n    if args.resume:\n        logger.info(\"==> Resuming from checkpoint..\")\n        assert os.path.isfile(\n            args.resume), \"Error: no checkpoint directory found!\"\n        args.out = os.path.dirname(args.resume)\n        checkpoint = torch.load(args.resume)\n        best_acc = checkpoint['best_acc']\n        args.start_epoch = checkpoint['epoch']\n        model.load_state_dict(checkpoint['state_dict'])\n        if args.use_ema:\n            ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        scheduler.load_state_dict(checkpoint['scheduler'])\n\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Task = {args.dataset}@{args.num_labeled}\")\n    logger.info(f\"  Num Epochs = {args.epochs}\")\n    logger.info(f\"  Batch size = {args.batch_size}\")\n    logger.info(f\"  Total optimization steps = {args.total_steps}\")\n\n    model.zero_grad()\n    train(args, labeled_trainloader, unlabeled_trainloader,\n          model, optimizer, ema_model, scheduler)\n    cm = get_confusion_matrix(test_loader, args.device)\n    accuracy = get_accuracy(cm)\n    \ndef train(args, labeled_trainloader, unlabeled_trainloader,\n          model, optimizer, ema_model, scheduler):\n    global best_acc\n    \n    end = time.time()\n\n    if args.world_size > 1:\n        labeled_epoch = 0\n        unlabeled_epoch = 0\n        labeled_trainloader.sampler.set_epoch(labeled_epoch)\n        unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n\n    labeled_iter = iter(labeled_trainloader)\n    unlabeled_iter = iter(unlabeled_trainloader)\n\n    model.train()\n    for epoch in range(args.start_epoch, args.epochs):\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        losses_x = AverageMeter()\n        losses_u = AverageMeter()\n        mask_probs = AverageMeter()\n        if not args.no_progress:\n            p_bar = tqdm(range(args.eval_step),\n                         disable=False)\n        for batch_idx in range(args.eval_step):\n            try:\n                inputs_x, targets_x = next(labeled_iter)\n            except:\n                if args.world_size > 1:\n                    labeled_epoch += 1\n                    labeled_trainloader.sampler.set_epoch(labeled_epoch)\n                labeled_iter = iter(labeled_trainloader)\n                inputs_x, targets_x = next(labeled_iter)\n\n            try:\n                (inputs_u_w, inputs_u_s), _ = next(unlabeled_iter)\n            except:\n                if args.world_size > 1:\n                    unlabeled_epoch += 1\n                    unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n                unlabeled_iter = iter(unlabeled_trainloader)\n                (inputs_u_w, inputs_u_s), _ = next(unlabeled_iter)\n\n            data_time.update(time.time() - end)\n            batch_size = inputs_x.shape[0]\n            inputs = interleave(\n                torch.cat((inputs_x, inputs_u_w, inputs_u_s)), 2*args.mu+1).to(args.device)\n            targets_x = targets_x.to(args.device)\n            logits = model(inputs)\n            logits = de_interleave(logits, 2*args.mu+1)\n            logits_x = logits[:batch_size]\n            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n            del logits\n\n            Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n\n            pseudo_label = torch.softmax(logits_u_w.detach()/args.T, dim=-1)\n            max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n            mask = max_probs.ge(args.threshold).float()\n\n            Lu = (F.cross_entropy(logits_u_s, targets_u,\n                                  reduction='none') * mask).mean()\n\n            loss = Lx + args.lambda_u * Lu\n\n            loss.backward()\n\n            losses.update(loss.item())\n            losses_x.update(Lx.item())\n            losses_u.update(Lu.item())\n            optimizer.step()\n            scheduler.step()\n            if args.use_ema:\n                ema_model.update(model)\n            model.zero_grad()\n\n            batch_time.update(time.time() - end)\n            end = time.time()\n            mask_probs.update(mask.mean().item())\n            if not args.no_progress:\n                p_bar.set_description(\"Train Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. LR: {lr:.4f}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. Loss_x: {loss_x:.4f}. Loss_u: {loss_u:.4f}. Mask: {mask:.2f}. \".format(\n                    epoch=epoch + 1,\n                    epochs=args.epochs,\n                    batch=batch_idx + 1,\n                    iter=args.eval_step,\n                    lr=scheduler.get_last_lr()[0],\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    loss=losses.avg,\n                    loss_x=losses_x.avg,\n                    loss_u=losses_u.avg,\n                    mask=mask_probs.avg))\n                p_bar.update()\n\n        if not args.no_progress:\n            p_bar.close()\n\n        if args.use_ema:\n            test_model = ema_model.ema\n        else:\n            test_model = model\n        args.writer.add_scalar('train/1.train_loss', losses.avg, epoch)\n        args.writer.add_scalar('train/2.train_loss_x', losses_x.avg, epoch)\n        args.writer.add_scalar('train/3.train_loss_u', losses_u.avg, epoch)\n        args.writer.add_scalar('train/4.mask', mask_probs.avg, epoch)\n        model_to_save = model.module if hasattr(model, \"module\") else model\n\n    args.writer.close()\n\n\ndef test(args, model, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    end = time.time()\n\n    if not args.no_progress:\n        test_loader = tqdm(test_loader,\n                           disable=False)\n\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(test_loader):\n            data_time.update(time.time() - end)\n            model.eval()\n\n            inputs = inputs.to(args.device)\n            targets = targets.to(args.device)\n            outputs = model(inputs)\n            loss = F.cross_entropy(outputs, targets)\n\n            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n            losses.update(loss.item(), inputs.shape[0])\n            top1.update(prec1.item(), inputs.shape[0])\n            top5.update(prec5.item(), inputs.shape[0])\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if not args.no_progress:\n                test_loader.set_description(\"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n                    batch=batch_idx + 1,\n                    iter=len(test_loader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                ))\n        if not args.no_progress:\n            test_loader.close()\n\n    logger.info(\"top-1 acc: {:.2f}\".format(top1.avg))\n    logger.info(\"top-5 acc: {:.2f}\".format(top5.avg))\n    return losses.avg","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:07:13.748948Z","iopub.execute_input":"2023-05-08T23:07:13.749297Z","iopub.status.idle":"2023-05-08T23:07:13.792190Z","shell.execute_reply.started":"2023-05-08T23:07:13.749271Z","shell.execute_reply":"2023-05-08T23:07:13.791145Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:07:23.103092Z","iopub.execute_input":"2023-05-08T23:07:23.103444Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Namespace(dataset='cifar10', num_labeled=4000, expand_labels=True, arch='resnet', total_steps=16384, eval_step=32, start_epoch=0, batch_size=64, lr=0.03, warmup=0, wdecay=0.0005, nesterov=True, mu=7, lambda_u=1, T=1, threshold=0.95, out='result', resume='', seed=None, no_progress=False, use_ema=True, ema_decay=0.999)\n","output_type":"stream"},{"name":"stderr","text":"\n\n\n  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.142s. Batch: 1.339s. Loss: 4.0030. Loss_x: 3.4161. Loss_u: 0.5869. Mask: 0.33. :   0%|          | 0/32 [00:01<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.142s. Batch: 1.339s. Loss: 4.0030. Loss_x: 3.4161. Loss_u: 0.5869. Mask: 0.33. :   3%|▎         | 1/32 [00:01<00:41,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.132s. Batch: 1.305s. Loss: 3.9681. Loss_x: 3.4206. Loss_u: 0.5476. Mask: 0.33. :   3%|▎         | 1/32 [00:02<00:41,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.132s. Batch: 1.305s. Loss: 3.9681. Loss_x: 3.4206. Loss_u: 0.5476. Mask: 0.33. :   6%|▋         | 2/32 [00:02<00:38,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.148s. Batch: 1.310s. Loss: 3.7912. Loss_x: 3.2769. Loss_u: 0.5142. Mask: 0.34. :   6%|▋         | 2/32 [00:03<00:38,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.148s. Batch: 1.310s. Loss: 3.7912. Loss_x: 3.2769. Loss_u: 0.5142. Mask: 0.34. :   9%|▉         | 3/32 [00:03<00:37,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.141s. Batch: 1.297s. Loss: 3.5641. Loss_x: 3.0591. Loss_u: 0.5050. Mask: 0.33. :   9%|▉         | 3/32 [00:05<00:37,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.141s. Batch: 1.297s. Loss: 3.5641. Loss_x: 3.0591. Loss_u: 0.5050. Mask: 0.33. :  12%|█▎        | 4/32 [00:05<00:36,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.139s. Batch: 1.291s. Loss: 3.4758. Loss_x: 2.9660. Loss_u: 0.5098. Mask: 0.32. :  12%|█▎        | 4/32 [00:06<00:36,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.139s. Batch: 1.291s. Loss: 3.4758. Loss_x: 2.9660. Loss_u: 0.5098. Mask: 0.32. :  16%|█▌        | 5/32 [00:06<00:34,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.131s. Batch: 1.280s. Loss: 3.4079. Loss_x: 2.8980. Loss_u: 0.5099. Mask: 0.33. :  16%|█▌        | 5/32 [00:07<00:34,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.131s. Batch: 1.280s. Loss: 3.4079. Loss_x: 2.8980. Loss_u: 0.5099. Mask: 0.33. :  19%|█▉        | 6/32 [00:07<00:32,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.129s. Batch: 1.276s. Loss: 3.3797. Loss_x: 2.8836. Loss_u: 0.4961. Mask: 0.32. :  19%|█▉        | 6/32 [00:08<00:32,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.129s. Batch: 1.276s. Loss: 3.3797. Loss_x: 2.8836. Loss_u: 0.4961. Mask: 0.32. :  22%|██▏       | 7/32 [00:08<00:31,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.135s. Batch: 1.284s. Loss: 3.4059. Loss_x: 2.8965. Loss_u: 0.5094. Mask: 0.34. :  22%|██▏       | 7/32 [00:10<00:31,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.135s. Batch: 1.284s. Loss: 3.4059. Loss_x: 2.8965. Loss_u: 0.5094. Mask: 0.34. :  25%|██▌       | 8/32 [00:10<00:30,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.138s. Batch: 1.287s. Loss: 3.3747. Loss_x: 2.8480. Loss_u: 0.5267. Mask: 0.34. :  25%|██▌       | 8/32 [00:11<00:30,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.138s. Batch: 1.287s. Loss: 3.3747. Loss_x: 2.8480. Loss_u: 0.5267. Mask: 0.34. :  28%|██▊       | 9/32 [00:11<00:29,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.149s. Batch: 1.297s. Loss: 3.2585. Loss_x: 2.7319. Loss_u: 0.5267. Mask: 0.33. :  28%|██▊       | 9/32 [00:12<00:29,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.149s. Batch: 1.297s. Loss: 3.2585. Loss_x: 2.7319. Loss_u: 0.5267. Mask: 0.33. :  31%|███▏      | 10/32 [00:12<00:29,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.158s. Batch: 1.305s. Loss: 3.2264. Loss_x: 2.6901. Loss_u: 0.5362. Mask: 0.33. :  31%|███▏      | 10/32 [00:14<00:29,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.158s. Batch: 1.305s. Loss: 3.2264. Loss_x: 2.6901. Loss_u: 0.5362. Mask: 0.33. :  34%|███▍      | 11/32 [00:14<00:28,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.157s. Batch: 1.306s. Loss: 3.2842. Loss_x: 2.7494. Loss_u: 0.5348. Mask: 0.33. :  34%|███▍      | 11/32 [00:15<00:28,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.157s. Batch: 1.306s. Loss: 3.2842. Loss_x: 2.7494. Loss_u: 0.5348. Mask: 0.33. :  38%|███▊      | 12/32 [00:15<00:26,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.155s. Batch: 1.303s. Loss: 3.3593. Loss_x: 2.8203. Loss_u: 0.5389. Mask: 0.33. :  38%|███▊      | 12/32 [00:16<00:26,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.155s. Batch: 1.303s. Loss: 3.3593. Loss_x: 2.8203. Loss_u: 0.5389. Mask: 0.33. :  41%|████      | 13/32 [00:16<00:24,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.156s. Batch: 1.304s. Loss: 3.3629. Loss_x: 2.8086. Loss_u: 0.5543. Mask: 0.33. :  41%|████      | 13/32 [00:18<00:24,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.156s. Batch: 1.304s. Loss: 3.3629. Loss_x: 2.8086. Loss_u: 0.5543. Mask: 0.33. :  44%|████▍     | 14/32 [00:18<00:23,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.155s. Batch: 1.302s. Loss: 3.3717. Loss_x: 2.8188. Loss_u: 0.5529. Mask: 0.33. :  44%|████▍     | 14/32 [00:19<00:23,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.155s. Batch: 1.302s. Loss: 3.3717. Loss_x: 2.8188. Loss_u: 0.5529. Mask: 0.33. :  47%|████▋     | 15/32 [00:19<00:22,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.153s. Batch: 1.299s. Loss: 3.3270. Loss_x: 2.7767. Loss_u: 0.5504. Mask: 0.33. :  47%|████▋     | 15/32 [00:20<00:22,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.153s. Batch: 1.299s. Loss: 3.3270. Loss_x: 2.7767. Loss_u: 0.5504. Mask: 0.33. :  50%|█████     | 16/32 [00:20<00:20,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.154s. Batch: 1.301s. Loss: 3.3768. Loss_x: 2.8267. Loss_u: 0.5501. Mask: 0.34. :  50%|█████     | 16/32 [00:22<00:20,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.154s. Batch: 1.301s. Loss: 3.3768. Loss_x: 2.8267. Loss_u: 0.5501. Mask: 0.34. :  53%|█████▎    | 17/32 [00:22<00:19,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.153s. Batch: 1.299s. Loss: 3.3495. Loss_x: 2.7925. Loss_u: 0.5570. Mask: 0.34. :  53%|█████▎    | 17/32 [00:23<00:19,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.153s. Batch: 1.299s. Loss: 3.3495. Loss_x: 2.7925. Loss_u: 0.5570. Mask: 0.34. :  56%|█████▋    | 18/32 [00:23<00:18,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.161s. Batch: 1.308s. Loss: 3.3121. Loss_x: 2.7481. Loss_u: 0.5641. Mask: 0.33. :  56%|█████▋    | 18/32 [00:24<00:18,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.161s. Batch: 1.308s. Loss: 3.3121. Loss_x: 2.7481. Loss_u: 0.5641. Mask: 0.33. :  59%|█████▉    | 19/32 [00:24<00:17,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.175s. Batch: 1.322s. Loss: 3.2758. Loss_x: 2.7183. Loss_u: 0.5575. Mask: 0.34. :  59%|█████▉    | 19/32 [00:26<00:17,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.175s. Batch: 1.322s. Loss: 3.2758. Loss_x: 2.7183. Loss_u: 0.5575. Mask: 0.34. :  62%|██████▎   | 20/32 [00:26<00:16,  1.41s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.172s. Batch: 1.319s. Loss: 3.2902. Loss_x: 2.7327. Loss_u: 0.5575. Mask: 0.33. :  62%|██████▎   | 20/32 [00:27<00:16,  1.41s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.172s. Batch: 1.319s. Loss: 3.2902. Loss_x: 2.7327. Loss_u: 0.5575. Mask: 0.33. :  66%|██████▌   | 21/32 [00:27<00:15,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.171s. Batch: 1.317s. Loss: 3.2879. Loss_x: 2.7314. Loss_u: 0.5565. Mask: 0.33. :  66%|██████▌   | 21/32 [00:28<00:15,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.171s. Batch: 1.317s. Loss: 3.2879. Loss_x: 2.7314. Loss_u: 0.5565. Mask: 0.33. :  69%|██████▉   | 22/32 [00:28<00:13,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.170s. Batch: 1.316s. Loss: 3.2821. Loss_x: 2.7296. Loss_u: 0.5525. Mask: 0.33. :  69%|██████▉   | 22/32 [00:30<00:13,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.170s. Batch: 1.316s. Loss: 3.2821. Loss_x: 2.7296. Loss_u: 0.5525. Mask: 0.33. :  72%|███████▏  | 23/32 [00:30<00:11,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.171s. Batch: 1.316s. Loss: 3.2608. Loss_x: 2.7061. Loss_u: 0.5546. Mask: 0.33. :  72%|███████▏  | 23/32 [00:31<00:11,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.171s. Batch: 1.316s. Loss: 3.2608. Loss_x: 2.7061. Loss_u: 0.5546. Mask: 0.33. :  75%|███████▌  | 24/32 [00:31<00:10,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.170s. Batch: 1.315s. Loss: 3.2532. Loss_x: 2.6923. Loss_u: 0.5610. Mask: 0.33. :  75%|███████▌  | 24/32 [00:32<00:10,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.170s. Batch: 1.315s. Loss: 3.2532. Loss_x: 2.6923. Loss_u: 0.5610. Mask: 0.33. :  78%|███████▊  | 25/32 [00:32<00:09,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.168s. Batch: 1.313s. Loss: 3.2618. Loss_x: 2.7063. Loss_u: 0.5555. Mask: 0.33. :  78%|███████▊  | 25/32 [00:34<00:09,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.168s. Batch: 1.313s. Loss: 3.2618. Loss_x: 2.7063. Loss_u: 0.5555. Mask: 0.33. :  81%|████████▏ | 26/32 [00:34<00:07,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.166s. Batch: 1.311s. Loss: 3.2829. Loss_x: 2.7270. Loss_u: 0.5559. Mask: 0.33. :  81%|████████▏ | 26/32 [00:35<00:07,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.166s. Batch: 1.311s. Loss: 3.2829. Loss_x: 2.7270. Loss_u: 0.5559. Mask: 0.33. :  84%|████████▍ | 27/32 [00:35<00:06,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.169s. Batch: 1.313s. Loss: 3.2604. Loss_x: 2.7032. Loss_u: 0.5572. Mask: 0.32. :  84%|████████▍ | 27/32 [00:36<00:06,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.169s. Batch: 1.313s. Loss: 3.2604. Loss_x: 2.7032. Loss_u: 0.5572. Mask: 0.32. :  88%|████████▊ | 28/32 [00:36<00:05,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.167s. Batch: 1.311s. Loss: 3.2684. Loss_x: 2.7108. Loss_u: 0.5576. Mask: 0.32. :  88%|████████▊ | 28/32 [00:38<00:05,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.167s. Batch: 1.311s. Loss: 3.2684. Loss_x: 2.7108. Loss_u: 0.5576. Mask: 0.32. :  91%|█████████ | 29/32 [00:38<00:03,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.167s. Batch: 1.310s. Loss: 3.2650. Loss_x: 2.7099. Loss_u: 0.5551. Mask: 0.32. :  91%|█████████ | 29/32 [00:39<00:03,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.167s. Batch: 1.310s. Loss: 3.2650. Loss_x: 2.7099. Loss_u: 0.5551. Mask: 0.32. :  94%|█████████▍| 30/32 [00:39<00:02,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.165s. Batch: 1.308s. Loss: 3.2795. Loss_x: 2.7226. Loss_u: 0.5569. Mask: 0.32. :  94%|█████████▍| 30/32 [00:40<00:02,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.165s. Batch: 1.308s. Loss: 3.2795. Loss_x: 2.7226. Loss_u: 0.5569. Mask: 0.32. :  97%|█████████▋| 31/32 [00:40<00:01,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.165s. Batch: 1.308s. Loss: 3.2763. Loss_x: 2.7152. Loss_u: 0.5611. Mask: 0.32. :  97%|█████████▋| 31/32 [00:41<00:01,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.165s. Batch: 1.308s. Loss: 3.2763. Loss_x: 2.7152. Loss_u: 0.5611. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.263s. Batch: 1.400s. Loss: 2.9642. Loss_x: 2.0796. Loss_u: 0.8847. Mask: 0.33. :   0%|          | 0/32 [00:01<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.263s. Batch: 1.400s. Loss: 2.9642. Loss_x: 2.0796. Loss_u: 0.8847. Mask: 0.33. :   3%|▎         | 1/32 [00:01<00:43,  1.39s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.222s. Batch: 1.358s. Loss: 2.8885. Loss_x: 2.0617. Loss_u: 0.8268. Mask: 0.31. :   3%|▎         | 1/32 [00:02<00:43,  1.39s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.222s. Batch: 1.358s. Loss: 2.8885. Loss_x: 2.0617. Loss_u: 0.8268. Mask: 0.31. :   6%|▋         | 2/32 [00:02<00:40,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.193s. Batch: 1.332s. Loss: 2.8975. Loss_x: 2.1332. Loss_u: 0.7643. Mask: 0.29. :   6%|▋         | 2/32 [00:03<00:40,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.193s. Batch: 1.332s. Loss: 2.8975. Loss_x: 2.1332. Loss_u: 0.7643. Mask: 0.29. :   9%|▉         | 3/32 [00:03<00:38,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.193s. Batch: 1.332s. Loss: 3.0001. Loss_x: 2.2744. Loss_u: 0.7257. Mask: 0.30. :   9%|▉         | 3/32 [00:05<00:38,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.193s. Batch: 1.332s. Loss: 3.0001. Loss_x: 2.2744. Loss_u: 0.7257. Mask: 0.30. :  12%|█▎        | 4/32 [00:05<00:37,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.180s. Batch: 1.319s. Loss: 3.3164. Loss_x: 2.5654. Loss_u: 0.7510. Mask: 0.33. :  12%|█▎        | 4/32 [00:06<00:37,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.180s. Batch: 1.319s. Loss: 3.3164. Loss_x: 2.5654. Loss_u: 0.7510. Mask: 0.33. :  16%|█▌        | 5/32 [00:06<00:35,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.172s. Batch: 1.310s. Loss: 3.2590. Loss_x: 2.5033. Loss_u: 0.7557. Mask: 0.34. :  16%|█▌        | 5/32 [00:07<00:35,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.172s. Batch: 1.310s. Loss: 3.2590. Loss_x: 2.5033. Loss_u: 0.7557. Mask: 0.34. :  19%|█▉        | 6/32 [00:07<00:33,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.171s. Batch: 1.309s. Loss: 3.2796. Loss_x: 2.5222. Loss_u: 0.7574. Mask: 0.34. :  19%|█▉        | 6/32 [00:09<00:33,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.171s. Batch: 1.309s. Loss: 3.2796. Loss_x: 2.5222. Loss_u: 0.7574. Mask: 0.34. :  22%|██▏       | 7/32 [00:09<00:32,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.2241. Loss_x: 2.4920. Loss_u: 0.7321. Mask: 0.33. :  22%|██▏       | 7/32 [00:10<00:32,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.2241. Loss_x: 2.4920. Loss_u: 0.7321. Mask: 0.33. :  25%|██▌       | 8/32 [00:10<00:30,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.163s. Batch: 1.300s. Loss: 3.2326. Loss_x: 2.4868. Loss_u: 0.7458. Mask: 0.33. :  25%|██▌       | 8/32 [00:11<00:30,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.163s. Batch: 1.300s. Loss: 3.2326. Loss_x: 2.4868. Loss_u: 0.7458. Mask: 0.33. :  28%|██▊       | 9/32 [00:11<00:29,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.161s. Batch: 1.300s. Loss: 3.3917. Loss_x: 2.6415. Loss_u: 0.7501. Mask: 0.34. :  28%|██▊       | 9/32 [00:12<00:29,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.161s. Batch: 1.300s. Loss: 3.3917. Loss_x: 2.6415. Loss_u: 0.7501. Mask: 0.34. :  31%|███▏      | 10/32 [00:12<00:28,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.171s. Batch: 1.310s. Loss: 3.4859. Loss_x: 2.7342. Loss_u: 0.7517. Mask: 0.36. :  31%|███▏      | 10/32 [00:14<00:28,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.171s. Batch: 1.310s. Loss: 3.4859. Loss_x: 2.7342. Loss_u: 0.7517. Mask: 0.36. :  34%|███▍      | 11/32 [00:14<00:27,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.189s. Batch: 1.328s. Loss: 3.6523. Loss_x: 2.9128. Loss_u: 0.7395. Mask: 0.37. :  34%|███▍      | 11/32 [00:15<00:27,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.189s. Batch: 1.328s. Loss: 3.6523. Loss_x: 2.9128. Loss_u: 0.7395. Mask: 0.37. :  38%|███▊      | 12/32 [00:15<00:27,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.191s. Batch: 1.331s. Loss: 3.6592. Loss_x: 2.9198. Loss_u: 0.7394. Mask: 0.38. :  38%|███▊      | 12/32 [00:17<00:27,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.191s. Batch: 1.331s. Loss: 3.6592. Loss_x: 2.9198. Loss_u: 0.7394. Mask: 0.38. :  41%|████      | 13/32 [00:17<00:26,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.195s. Batch: 1.334s. Loss: 3.6650. Loss_x: 2.9186. Loss_u: 0.7465. Mask: 0.38. :  41%|████      | 13/32 [00:18<00:26,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.195s. Batch: 1.334s. Loss: 3.6650. Loss_x: 2.9186. Loss_u: 0.7465. Mask: 0.38. :  44%|████▍     | 14/32 [00:18<00:24,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.197s. Batch: 1.336s. Loss: 3.6196. Loss_x: 2.8590. Loss_u: 0.7606. Mask: 0.37. :  44%|████▍     | 14/32 [00:20<00:24,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.197s. Batch: 1.336s. Loss: 3.6196. Loss_x: 2.8590. Loss_u: 0.7606. Mask: 0.37. :  47%|████▋     | 15/32 [00:20<00:23,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.195s. Batch: 1.334s. Loss: 3.5767. Loss_x: 2.8111. Loss_u: 0.7656. Mask: 0.37. :  47%|████▋     | 15/32 [00:21<00:23,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.195s. Batch: 1.334s. Loss: 3.5767. Loss_x: 2.8111. Loss_u: 0.7656. Mask: 0.37. :  50%|█████     | 16/32 [00:21<00:21,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.192s. Batch: 1.331s. Loss: 3.5724. Loss_x: 2.8047. Loss_u: 0.7678. Mask: 0.37. :  50%|█████     | 16/32 [00:22<00:21,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.192s. Batch: 1.331s. Loss: 3.5724. Loss_x: 2.8047. Loss_u: 0.7678. Mask: 0.37. :  53%|█████▎    | 17/32 [00:22<00:19,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.189s. Batch: 1.328s. Loss: 3.5671. Loss_x: 2.7932. Loss_u: 0.7739. Mask: 0.37. :  53%|█████▎    | 17/32 [00:23<00:19,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.189s. Batch: 1.328s. Loss: 3.5671. Loss_x: 2.7932. Loss_u: 0.7739. Mask: 0.37. :  56%|█████▋    | 18/32 [00:23<00:18,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.186s. Batch: 1.325s. Loss: 3.5724. Loss_x: 2.7963. Loss_u: 0.7761. Mask: 0.37. :  56%|█████▋    | 18/32 [00:25<00:18,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.186s. Batch: 1.325s. Loss: 3.5724. Loss_x: 2.7963. Loss_u: 0.7761. Mask: 0.37. :  59%|█████▉    | 19/32 [00:25<00:16,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.184s. Batch: 1.323s. Loss: 3.5739. Loss_x: 2.7983. Loss_u: 0.7756. Mask: 0.37. :  59%|█████▉    | 19/32 [00:26<00:16,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.184s. Batch: 1.323s. Loss: 3.5739. Loss_x: 2.7983. Loss_u: 0.7756. Mask: 0.37. :  62%|██████▎   | 20/32 [00:26<00:15,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.185s. Batch: 1.324s. Loss: 3.5163. Loss_x: 2.7395. Loss_u: 0.7768. Mask: 0.37. :  62%|██████▎   | 20/32 [00:27<00:15,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.185s. Batch: 1.324s. Loss: 3.5163. Loss_x: 2.7395. Loss_u: 0.7768. Mask: 0.37. :  66%|██████▌   | 21/32 [00:27<00:14,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.184s. Batch: 1.325s. Loss: 3.4814. Loss_x: 2.6965. Loss_u: 0.7849. Mask: 0.37. :  66%|██████▌   | 21/32 [00:29<00:14,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.184s. Batch: 1.325s. Loss: 3.4814. Loss_x: 2.6965. Loss_u: 0.7849. Mask: 0.37. :  69%|██████▉   | 22/32 [00:29<00:13,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.184s. Batch: 1.325s. Loss: 3.4542. Loss_x: 2.6610. Loss_u: 0.7932. Mask: 0.37. :  69%|██████▉   | 22/32 [00:30<00:13,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.184s. Batch: 1.325s. Loss: 3.4542. Loss_x: 2.6610. Loss_u: 0.7932. Mask: 0.37. :  72%|███████▏  | 23/32 [00:30<00:11,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.188s. Batch: 1.329s. Loss: 3.4449. Loss_x: 2.6446. Loss_u: 0.8003. Mask: 0.37. :  72%|███████▏  | 23/32 [00:31<00:11,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.188s. Batch: 1.329s. Loss: 3.4449. Loss_x: 2.6446. Loss_u: 0.8003. Mask: 0.37. :  75%|███████▌  | 24/32 [00:31<00:10,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.188s. Batch: 1.328s. Loss: 3.4676. Loss_x: 2.6714. Loss_u: 0.7962. Mask: 0.37. :  75%|███████▌  | 24/32 [00:33<00:10,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.188s. Batch: 1.328s. Loss: 3.4676. Loss_x: 2.6714. Loss_u: 0.7962. Mask: 0.37. :  78%|███████▊  | 25/32 [00:33<00:09,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.186s. Batch: 1.326s. Loss: 3.5018. Loss_x: 2.7110. Loss_u: 0.7908. Mask: 0.37. :  78%|███████▊  | 25/32 [00:34<00:09,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.186s. Batch: 1.326s. Loss: 3.5018. Loss_x: 2.7110. Loss_u: 0.7908. Mask: 0.37. :  81%|████████▏ | 26/32 [00:34<00:07,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.184s. Batch: 1.324s. Loss: 3.4811. Loss_x: 2.6927. Loss_u: 0.7884. Mask: 0.37. :  81%|████████▏ | 26/32 [00:35<00:07,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.184s. Batch: 1.324s. Loss: 3.4811. Loss_x: 2.6927. Loss_u: 0.7884. Mask: 0.37. :  84%|████████▍ | 27/32 [00:35<00:06,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.184s. Batch: 1.324s. Loss: 3.5036. Loss_x: 2.7173. Loss_u: 0.7862. Mask: 0.37. :  84%|████████▍ | 27/32 [00:37<00:06,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.184s. Batch: 1.324s. Loss: 3.5036. Loss_x: 2.7173. Loss_u: 0.7862. Mask: 0.37. :  88%|████████▊ | 28/32 [00:37<00:05,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.185s. Batch: 1.326s. Loss: 3.5074. Loss_x: 2.7176. Loss_u: 0.7898. Mask: 0.37. :  88%|████████▊ | 28/32 [00:38<00:05,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.185s. Batch: 1.326s. Loss: 3.5074. Loss_x: 2.7176. Loss_u: 0.7898. Mask: 0.37. :  91%|█████████ | 29/32 [00:38<00:03,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.185s. Batch: 1.326s. Loss: 3.5053. Loss_x: 2.7135. Loss_u: 0.7918. Mask: 0.37. :  91%|█████████ | 29/32 [00:39<00:03,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.185s. Batch: 1.326s. Loss: 3.5053. Loss_x: 2.7135. Loss_u: 0.7918. Mask: 0.37. :  94%|█████████▍| 30/32 [00:39<00:02,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.184s. Batch: 1.324s. Loss: 3.5267. Loss_x: 2.7354. Loss_u: 0.7913. Mask: 0.37. :  94%|█████████▍| 30/32 [00:41<00:02,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.184s. Batch: 1.324s. Loss: 3.5267. Loss_x: 2.7354. Loss_u: 0.7913. Mask: 0.37. :  97%|█████████▋| 31/32 [00:41<00:01,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.183s. Batch: 1.323s. Loss: 3.5487. Loss_x: 2.7560. Loss_u: 0.7927. Mask: 0.37. :  97%|█████████▋| 31/32 [00:42<00:01,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 2/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.183s. Batch: 1.323s. Loss: 3.5487. Loss_x: 2.7560. Loss_u: 0.7927. Mask: 0.37. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.154s. Batch: 1.289s. Loss: 3.1300. Loss_x: 2.3631. Loss_u: 0.7668. Mask: 0.35. :   0%|          | 0/32 [00:01<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.154s. Batch: 1.289s. Loss: 3.1300. Loss_x: 2.3631. Loss_u: 0.7668. Mask: 0.35. :   3%|▎         | 1/32 [00:01<00:39,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.140s. Batch: 1.276s. Loss: 3.1661. Loss_x: 2.4061. Loss_u: 0.7599. Mask: 0.36. :   3%|▎         | 1/32 [00:02<00:39,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.140s. Batch: 1.276s. Loss: 3.1661. Loss_x: 2.4061. Loss_u: 0.7599. Mask: 0.36. :   6%|▋         | 2/32 [00:02<00:38,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.239s. Batch: 1.383s. Loss: 3.2081. Loss_x: 2.3961. Loss_u: 0.8120. Mask: 0.37. :   6%|▋         | 2/32 [00:04<00:38,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.239s. Batch: 1.383s. Loss: 3.2081. Loss_x: 2.3961. Loss_u: 0.8120. Mask: 0.37. :   9%|▉         | 3/32 [00:04<00:41,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.227s. Batch: 1.369s. Loss: 3.1408. Loss_x: 2.3698. Loss_u: 0.7710. Mask: 0.37. :   9%|▉         | 3/32 [00:05<00:41,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.227s. Batch: 1.369s. Loss: 3.1408. Loss_x: 2.3698. Loss_u: 0.7710. Mask: 0.37. :  12%|█▎        | 4/32 [00:05<00:38,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.208s. Batch: 1.352s. Loss: 3.2254. Loss_x: 2.4469. Loss_u: 0.7785. Mask: 0.36. :  12%|█▎        | 4/32 [00:06<00:38,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.208s. Batch: 1.352s. Loss: 3.2254. Loss_x: 2.4469. Loss_u: 0.7785. Mask: 0.36. :  16%|█▌        | 5/32 [00:06<00:36,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.214s. Batch: 1.357s. Loss: 3.2759. Loss_x: 2.5035. Loss_u: 0.7723. Mask: 0.35. :  16%|█▌        | 5/32 [00:08<00:36,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.214s. Batch: 1.357s. Loss: 3.2759. Loss_x: 2.5035. Loss_u: 0.7723. Mask: 0.35. :  19%|█▉        | 6/32 [00:08<00:35,  1.36s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.204s. Batch: 1.346s. Loss: 3.5222. Loss_x: 2.7272. Loss_u: 0.7951. Mask: 0.36. :  19%|█▉        | 6/32 [00:09<00:35,  1.36s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.204s. Batch: 1.346s. Loss: 3.5222. Loss_x: 2.7272. Loss_u: 0.7951. Mask: 0.36. :  22%|██▏       | 7/32 [00:09<00:33,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.194s. Batch: 1.335s. Loss: 3.6128. Loss_x: 2.8321. Loss_u: 0.7807. Mask: 0.36. :  22%|██▏       | 7/32 [00:10<00:33,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.194s. Batch: 1.335s. Loss: 3.6128. Loss_x: 2.8321. Loss_u: 0.7807. Mask: 0.36. :  25%|██▌       | 8/32 [00:10<00:31,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.191s. Batch: 1.331s. Loss: 3.5846. Loss_x: 2.8124. Loss_u: 0.7722. Mask: 0.36. :  25%|██▌       | 8/32 [00:11<00:31,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.191s. Batch: 1.331s. Loss: 3.5846. Loss_x: 2.8124. Loss_u: 0.7722. Mask: 0.36. :  28%|██▊       | 9/32 [00:11<00:30,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.186s. Batch: 1.326s. Loss: 3.5154. Loss_x: 2.7513. Loss_u: 0.7641. Mask: 0.36. :  28%|██▊       | 9/32 [00:13<00:30,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.186s. Batch: 1.326s. Loss: 3.5154. Loss_x: 2.7513. Loss_u: 0.7641. Mask: 0.36. :  31%|███▏      | 10/32 [00:13<00:28,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.182s. Batch: 1.322s. Loss: 3.6200. Loss_x: 2.8495. Loss_u: 0.7705. Mask: 0.36. :  31%|███▏      | 10/32 [00:14<00:28,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.182s. Batch: 1.322s. Loss: 3.6200. Loss_x: 2.8495. Loss_u: 0.7705. Mask: 0.36. :  34%|███▍      | 11/32 [00:14<00:27,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.177s. Batch: 1.317s. Loss: 3.5387. Loss_x: 2.7776. Loss_u: 0.7611. Mask: 0.36. :  34%|███▍      | 11/32 [00:15<00:27,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.177s. Batch: 1.317s. Loss: 3.5387. Loss_x: 2.7776. Loss_u: 0.7611. Mask: 0.36. :  38%|███▊      | 12/32 [00:15<00:25,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.173s. Batch: 1.312s. Loss: 3.4876. Loss_x: 2.7346. Loss_u: 0.7530. Mask: 0.36. :  38%|███▊      | 12/32 [00:17<00:25,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.173s. Batch: 1.312s. Loss: 3.4876. Loss_x: 2.7346. Loss_u: 0.7530. Mask: 0.36. :  41%|████      | 13/32 [00:17<00:24,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.176s. Batch: 1.315s. Loss: 3.4704. Loss_x: 2.7163. Loss_u: 0.7542. Mask: 0.36. :  41%|████      | 13/32 [00:18<00:24,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.176s. Batch: 1.315s. Loss: 3.4704. Loss_x: 2.7163. Loss_u: 0.7542. Mask: 0.36. :  44%|████▍     | 14/32 [00:18<00:23,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.179s. Batch: 1.318s. Loss: 3.4480. Loss_x: 2.6902. Loss_u: 0.7578. Mask: 0.36. :  44%|████▍     | 14/32 [00:19<00:23,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.179s. Batch: 1.318s. Loss: 3.4480. Loss_x: 2.6902. Loss_u: 0.7578. Mask: 0.36. :  47%|████▋     | 15/32 [00:19<00:22,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.180s. Batch: 1.319s. Loss: 3.4071. Loss_x: 2.6488. Loss_u: 0.7582. Mask: 0.35. :  47%|████▋     | 15/32 [00:21<00:22,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.180s. Batch: 1.319s. Loss: 3.4071. Loss_x: 2.6488. Loss_u: 0.7582. Mask: 0.35. :  50%|█████     | 16/32 [00:21<00:21,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.175s. Batch: 1.314s. Loss: 3.3914. Loss_x: 2.6272. Loss_u: 0.7642. Mask: 0.35. :  50%|█████     | 16/32 [00:22<00:21,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.175s. Batch: 1.314s. Loss: 3.3914. Loss_x: 2.6272. Loss_u: 0.7642. Mask: 0.35. :  53%|█████▎    | 17/32 [00:22<00:19,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.173s. Batch: 1.311s. Loss: 3.3626. Loss_x: 2.6005. Loss_u: 0.7621. Mask: 0.35. :  53%|█████▎    | 17/32 [00:23<00:19,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.173s. Batch: 1.311s. Loss: 3.3626. Loss_x: 2.6005. Loss_u: 0.7621. Mask: 0.35. :  56%|█████▋    | 18/32 [00:23<00:18,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.170s. Batch: 1.309s. Loss: 3.3596. Loss_x: 2.6076. Loss_u: 0.7520. Mask: 0.35. :  56%|█████▋    | 18/32 [00:24<00:18,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.170s. Batch: 1.309s. Loss: 3.3596. Loss_x: 2.6076. Loss_u: 0.7520. Mask: 0.35. :  59%|█████▉    | 19/32 [00:24<00:16,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.168s. Batch: 1.306s. Loss: 3.3594. Loss_x: 2.6130. Loss_u: 0.7464. Mask: 0.35. :  59%|█████▉    | 19/32 [00:26<00:16,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.168s. Batch: 1.306s. Loss: 3.3594. Loss_x: 2.6130. Loss_u: 0.7464. Mask: 0.35. :  62%|██████▎   | 20/32 [00:26<00:15,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.3410. Loss_x: 2.5957. Loss_u: 0.7453. Mask: 0.34. :  62%|██████▎   | 20/32 [00:27<00:15,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.3410. Loss_x: 2.5957. Loss_u: 0.7453. Mask: 0.34. :  66%|██████▌   | 21/32 [00:27<00:13,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.163s. Batch: 1.300s. Loss: 3.3740. Loss_x: 2.6378. Loss_u: 0.7362. Mask: 0.34. :  66%|██████▌   | 21/32 [00:28<00:13,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.163s. Batch: 1.300s. Loss: 3.3740. Loss_x: 2.6378. Loss_u: 0.7362. Mask: 0.34. :  69%|██████▉   | 22/32 [00:28<00:12,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.4717. Loss_x: 2.7475. Loss_u: 0.7243. Mask: 0.35. :  69%|██████▉   | 22/32 [00:29<00:12,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.4717. Loss_x: 2.7475. Loss_u: 0.7243. Mask: 0.35. :  72%|███████▏  | 23/32 [00:29<00:11,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.164s. Batch: 1.302s. Loss: 3.4686. Loss_x: 2.7508. Loss_u: 0.7178. Mask: 0.35. :  72%|███████▏  | 23/32 [00:31<00:11,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.164s. Batch: 1.302s. Loss: 3.4686. Loss_x: 2.7508. Loss_u: 0.7178. Mask: 0.35. :  75%|███████▌  | 24/32 [00:31<00:10,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.162s. Batch: 1.300s. Loss: 3.4246. Loss_x: 2.7182. Loss_u: 0.7064. Mask: 0.35. :  75%|███████▌  | 24/32 [00:32<00:10,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.162s. Batch: 1.300s. Loss: 3.4246. Loss_x: 2.7182. Loss_u: 0.7064. Mask: 0.35. :  78%|███████▊  | 25/32 [00:32<00:08,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.159s. Batch: 1.297s. Loss: 3.3876. Loss_x: 2.6824. Loss_u: 0.7052. Mask: 0.35. :  78%|███████▊  | 25/32 [00:33<00:08,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.159s. Batch: 1.297s. Loss: 3.3876. Loss_x: 2.6824. Loss_u: 0.7052. Mask: 0.35. :  81%|████████▏ | 26/32 [00:33<00:07,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.166s. Batch: 1.304s. Loss: 3.3761. Loss_x: 2.6708. Loss_u: 0.7053. Mask: 0.35. :  81%|████████▏ | 26/32 [00:35<00:07,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.166s. Batch: 1.304s. Loss: 3.3761. Loss_x: 2.6708. Loss_u: 0.7053. Mask: 0.35. :  84%|████████▍ | 27/32 [00:35<00:06,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.168s. Batch: 1.307s. Loss: 3.3479. Loss_x: 2.6439. Loss_u: 0.7041. Mask: 0.35. :  84%|████████▍ | 27/32 [00:36<00:06,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.168s. Batch: 1.307s. Loss: 3.3479. Loss_x: 2.6439. Loss_u: 0.7041. Mask: 0.35. :  88%|████████▊ | 28/32 [00:36<00:05,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.166s. Batch: 1.305s. Loss: 3.3664. Loss_x: 2.6595. Loss_u: 0.7070. Mask: 0.35. :  88%|████████▊ | 28/32 [00:37<00:05,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.166s. Batch: 1.305s. Loss: 3.3664. Loss_x: 2.6595. Loss_u: 0.7070. Mask: 0.35. :  91%|█████████ | 29/32 [00:37<00:03,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.165s. Batch: 1.305s. Loss: 3.3723. Loss_x: 2.6612. Loss_u: 0.7111. Mask: 0.35. :  91%|█████████ | 29/32 [00:39<00:03,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.165s. Batch: 1.305s. Loss: 3.3723. Loss_x: 2.6612. Loss_u: 0.7111. Mask: 0.35. :  94%|█████████▍| 30/32 [00:39<00:02,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.168s. Batch: 1.307s. Loss: 3.3903. Loss_x: 2.6800. Loss_u: 0.7102. Mask: 0.35. :  94%|█████████▍| 30/32 [00:40<00:02,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.168s. Batch: 1.307s. Loss: 3.3903. Loss_x: 2.6800. Loss_u: 0.7102. Mask: 0.35. :  97%|█████████▋| 31/32 [00:40<00:01,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.168s. Batch: 1.307s. Loss: 3.4101. Loss_x: 2.6949. Loss_u: 0.7152. Mask: 0.35. :  97%|█████████▋| 31/32 [00:41<00:01,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 3/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.168s. Batch: 1.307s. Loss: 3.4101. Loss_x: 2.6949. Loss_u: 0.7152. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.134s. Batch: 1.275s. Loss: 3.3065. Loss_x: 2.6700. Loss_u: 0.6365. Mask: 0.33. :   0%|          | 0/32 [00:01<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.134s. Batch: 1.275s. Loss: 3.3065. Loss_x: 2.6700. Loss_u: 0.6365. Mask: 0.33. :   3%|▎         | 1/32 [00:01<00:39,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.144s. Batch: 1.282s. Loss: 3.2476. Loss_x: 2.4609. Loss_u: 0.7867. Mask: 0.38. :   3%|▎         | 1/32 [00:02<00:39,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.144s. Batch: 1.282s. Loss: 3.2476. Loss_x: 2.4609. Loss_u: 0.7867. Mask: 0.38. :   6%|▋         | 2/32 [00:02<00:38,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.137s. Batch: 1.274s. Loss: 3.2053. Loss_x: 2.4375. Loss_u: 0.7678. Mask: 0.36. :   6%|▋         | 2/32 [00:03<00:38,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.137s. Batch: 1.274s. Loss: 3.2053. Loss_x: 2.4375. Loss_u: 0.7678. Mask: 0.36. :   9%|▉         | 3/32 [00:03<00:36,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.128s. Batch: 1.265s. Loss: 3.3351. Loss_x: 2.5822. Loss_u: 0.7529. Mask: 0.38. :   9%|▉         | 3/32 [00:05<00:36,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.128s. Batch: 1.265s. Loss: 3.3351. Loss_x: 2.5822. Loss_u: 0.7529. Mask: 0.38. :  12%|█▎        | 4/32 [00:05<00:35,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.146s. Batch: 1.282s. Loss: 3.1466. Loss_x: 2.4080. Loss_u: 0.7386. Mask: 0.37. :  12%|█▎        | 4/32 [00:06<00:35,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.146s. Batch: 1.282s. Loss: 3.1466. Loss_x: 2.4080. Loss_u: 0.7386. Mask: 0.37. :  16%|█▌        | 5/32 [00:06<00:34,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.158s. Batch: 1.298s. Loss: 3.1210. Loss_x: 2.3477. Loss_u: 0.7732. Mask: 0.37. :  16%|█▌        | 5/32 [00:07<00:34,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.158s. Batch: 1.298s. Loss: 3.1210. Loss_x: 2.3477. Loss_u: 0.7732. Mask: 0.37. :  19%|█▉        | 6/32 [00:07<00:34,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.160s. Batch: 1.301s. Loss: 3.1546. Loss_x: 2.3830. Loss_u: 0.7716. Mask: 0.37. :  19%|█▉        | 6/32 [00:09<00:34,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.160s. Batch: 1.301s. Loss: 3.1546. Loss_x: 2.3830. Loss_u: 0.7716. Mask: 0.37. :  22%|██▏       | 7/32 [00:09<00:32,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.159s. Batch: 1.299s. Loss: 3.2079. Loss_x: 2.4427. Loss_u: 0.7652. Mask: 0.37. :  22%|██▏       | 7/32 [00:10<00:32,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.159s. Batch: 1.299s. Loss: 3.2079. Loss_x: 2.4427. Loss_u: 0.7652. Mask: 0.37. :  25%|██▌       | 8/32 [00:10<00:31,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.153s. Batch: 1.292s. Loss: 3.2386. Loss_x: 2.4864. Loss_u: 0.7522. Mask: 0.36. :  25%|██▌       | 8/32 [00:11<00:31,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.153s. Batch: 1.292s. Loss: 3.2386. Loss_x: 2.4864. Loss_u: 0.7522. Mask: 0.36. :  28%|██▊       | 9/32 [00:11<00:29,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.152s. Batch: 1.294s. Loss: 3.1918. Loss_x: 2.4475. Loss_u: 0.7443. Mask: 0.36. :  28%|██▊       | 9/32 [00:12<00:29,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.152s. Batch: 1.294s. Loss: 3.1918. Loss_x: 2.4475. Loss_u: 0.7443. Mask: 0.36. :  31%|███▏      | 10/32 [00:12<00:28,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.149s. Batch: 1.289s. Loss: 3.2079. Loss_x: 2.4673. Loss_u: 0.7405. Mask: 0.36. :  31%|███▏      | 10/32 [00:14<00:28,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.149s. Batch: 1.289s. Loss: 3.2079. Loss_x: 2.4673. Loss_u: 0.7405. Mask: 0.36. :  34%|███▍      | 11/32 [00:14<00:26,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.146s. Batch: 1.287s. Loss: 3.1614. Loss_x: 2.4258. Loss_u: 0.7356. Mask: 0.36. :  34%|███▍      | 11/32 [00:15<00:26,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.146s. Batch: 1.287s. Loss: 3.1614. Loss_x: 2.4258. Loss_u: 0.7356. Mask: 0.36. :  38%|███▊      | 12/32 [00:15<00:25,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.148s. Batch: 1.289s. Loss: 3.2321. Loss_x: 2.4862. Loss_u: 0.7459. Mask: 0.36. :  38%|███▊      | 12/32 [00:16<00:25,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.148s. Batch: 1.289s. Loss: 3.2321. Loss_x: 2.4862. Loss_u: 0.7459. Mask: 0.36. :  41%|████      | 13/32 [00:16<00:24,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.148s. Batch: 1.288s. Loss: 3.2949. Loss_x: 2.5577. Loss_u: 0.7372. Mask: 0.35. :  41%|████      | 13/32 [00:18<00:24,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.148s. Batch: 1.288s. Loss: 3.2949. Loss_x: 2.5577. Loss_u: 0.7372. Mask: 0.35. :  44%|████▍     | 14/32 [00:18<00:23,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.146s. Batch: 1.287s. Loss: 3.2981. Loss_x: 2.5689. Loss_u: 0.7291. Mask: 0.35. :  44%|████▍     | 14/32 [00:19<00:23,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.146s. Batch: 1.287s. Loss: 3.2981. Loss_x: 2.5689. Loss_u: 0.7291. Mask: 0.35. :  47%|████▋     | 15/32 [00:19<00:21,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.151s. Batch: 1.291s. Loss: 3.2616. Loss_x: 2.5333. Loss_u: 0.7283. Mask: 0.35. :  47%|████▋     | 15/32 [00:20<00:21,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.151s. Batch: 1.291s. Loss: 3.2616. Loss_x: 2.5333. Loss_u: 0.7283. Mask: 0.35. :  50%|█████     | 16/32 [00:20<00:20,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.150s. Batch: 1.289s. Loss: 3.2460. Loss_x: 2.5266. Loss_u: 0.7194. Mask: 0.35. :  50%|█████     | 16/32 [00:21<00:20,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.150s. Batch: 1.289s. Loss: 3.2460. Loss_x: 2.5266. Loss_u: 0.7194. Mask: 0.35. :  53%|█████▎    | 17/32 [00:21<00:19,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.149s. Batch: 1.288s. Loss: 3.1931. Loss_x: 2.4845. Loss_u: 0.7086. Mask: 0.34. :  53%|█████▎    | 17/32 [00:23<00:19,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.149s. Batch: 1.288s. Loss: 3.1931. Loss_x: 2.4845. Loss_u: 0.7086. Mask: 0.34. :  56%|█████▋    | 18/32 [00:23<00:17,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.158s. Batch: 1.298s. Loss: 3.2199. Loss_x: 2.5228. Loss_u: 0.6971. Mask: 0.34. :  56%|█████▋    | 18/32 [00:24<00:17,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.158s. Batch: 1.298s. Loss: 3.2199. Loss_x: 2.5228. Loss_u: 0.6971. Mask: 0.34. :  59%|█████▉    | 19/32 [00:24<00:17,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.170s. Batch: 1.309s. Loss: 3.3295. Loss_x: 2.6539. Loss_u: 0.6756. Mask: 0.35. :  59%|█████▉    | 19/32 [00:26<00:17,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.170s. Batch: 1.309s. Loss: 3.3295. Loss_x: 2.6539. Loss_u: 0.6756. Mask: 0.35. :  62%|██████▎   | 20/32 [00:26<00:16,  1.40s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.168s. Batch: 1.307s. Loss: 3.3986. Loss_x: 2.7195. Loss_u: 0.6791. Mask: 0.36. :  62%|██████▎   | 20/32 [00:27<00:16,  1.40s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.168s. Batch: 1.307s. Loss: 3.3986. Loss_x: 2.7195. Loss_u: 0.6791. Mask: 0.36. :  66%|██████▌   | 21/32 [00:27<00:14,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.165s. Batch: 1.304s. Loss: 3.4042. Loss_x: 2.7331. Loss_u: 0.6711. Mask: 0.35. :  66%|██████▌   | 21/32 [00:28<00:14,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.165s. Batch: 1.304s. Loss: 3.4042. Loss_x: 2.7331. Loss_u: 0.6711. Mask: 0.35. :  69%|██████▉   | 22/32 [00:28<00:13,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.164s. Batch: 1.303s. Loss: 3.3929. Loss_x: 2.7123. Loss_u: 0.6805. Mask: 0.35. :  69%|██████▉   | 22/32 [00:29<00:13,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.164s. Batch: 1.303s. Loss: 3.3929. Loss_x: 2.7123. Loss_u: 0.6805. Mask: 0.35. :  72%|███████▏  | 23/32 [00:29<00:11,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.166s. Batch: 1.305s. Loss: 3.3898. Loss_x: 2.7115. Loss_u: 0.6783. Mask: 0.35. :  72%|███████▏  | 23/32 [00:31<00:11,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.166s. Batch: 1.305s. Loss: 3.3898. Loss_x: 2.7115. Loss_u: 0.6783. Mask: 0.35. :  75%|███████▌  | 24/32 [00:31<00:10,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.3641. Loss_x: 2.6890. Loss_u: 0.6751. Mask: 0.35. :  75%|███████▌  | 24/32 [00:32<00:10,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.3641. Loss_x: 2.6890. Loss_u: 0.6751. Mask: 0.35. :  78%|███████▊  | 25/32 [00:32<00:09,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.3423. Loss_x: 2.6668. Loss_u: 0.6755. Mask: 0.35. :  78%|███████▊  | 25/32 [00:33<00:09,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.165s. Batch: 1.303s. Loss: 3.3423. Loss_x: 2.6668. Loss_u: 0.6755. Mask: 0.35. :  81%|████████▏ | 26/32 [00:33<00:07,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.164s. Batch: 1.303s. Loss: 3.3216. Loss_x: 2.6505. Loss_u: 0.6711. Mask: 0.35. :  81%|████████▏ | 26/32 [00:35<00:07,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.164s. Batch: 1.303s. Loss: 3.3216. Loss_x: 2.6505. Loss_u: 0.6711. Mask: 0.35. :  84%|████████▍ | 27/32 [00:35<00:06,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.163s. Batch: 1.302s. Loss: 3.2994. Loss_x: 2.6265. Loss_u: 0.6730. Mask: 0.35. :  84%|████████▍ | 27/32 [00:36<00:06,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.163s. Batch: 1.302s. Loss: 3.2994. Loss_x: 2.6265. Loss_u: 0.6730. Mask: 0.35. :  88%|████████▊ | 28/32 [00:36<00:05,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.162s. Batch: 1.301s. Loss: 3.3324. Loss_x: 2.6584. Loss_u: 0.6740. Mask: 0.35. :  88%|████████▊ | 28/32 [00:37<00:05,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.162s. Batch: 1.301s. Loss: 3.3324. Loss_x: 2.6584. Loss_u: 0.6740. Mask: 0.35. :  91%|█████████ | 29/32 [00:37<00:03,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.161s. Batch: 1.300s. Loss: 3.3381. Loss_x: 2.6621. Loss_u: 0.6760. Mask: 0.35. :  91%|█████████ | 29/32 [00:38<00:03,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.161s. Batch: 1.300s. Loss: 3.3381. Loss_x: 2.6621. Loss_u: 0.6760. Mask: 0.35. :  94%|█████████▍| 30/32 [00:38<00:02,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.160s. Batch: 1.300s. Loss: 3.3298. Loss_x: 2.6519. Loss_u: 0.6779. Mask: 0.35. :  94%|█████████▍| 30/32 [00:40<00:02,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.160s. Batch: 1.300s. Loss: 3.3298. Loss_x: 2.6519. Loss_u: 0.6779. Mask: 0.35. :  97%|█████████▋| 31/32 [00:40<00:01,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.161s. Batch: 1.301s. Loss: 3.3264. Loss_x: 2.6464. Loss_u: 0.6800. Mask: 0.35. :  97%|█████████▋| 31/32 [00:41<00:01,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 4/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.161s. Batch: 1.301s. Loss: 3.3264. Loss_x: 2.6464. Loss_u: 0.6800. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.188s. Batch: 1.347s. Loss: 2.7765. Loss_x: 2.1077. Loss_u: 0.6687. Mask: 0.31. :   0%|          | 0/32 [00:01<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.188s. Batch: 1.347s. Loss: 2.7765. Loss_x: 2.1077. Loss_u: 0.6687. Mask: 0.31. :   3%|▎         | 1/32 [00:01<00:41,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.153s. Batch: 1.301s. Loss: 3.2012. Loss_x: 2.5660. Loss_u: 0.6352. Mask: 0.35. :   3%|▎         | 1/32 [00:02<00:41,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.153s. Batch: 1.301s. Loss: 3.2012. Loss_x: 2.5660. Loss_u: 0.6352. Mask: 0.35. :   6%|▋         | 2/32 [00:02<00:38,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.145s. Batch: 1.310s. Loss: 3.4472. Loss_x: 2.7884. Loss_u: 0.6588. Mask: 0.35. :   6%|▋         | 2/32 [00:03<00:38,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.145s. Batch: 1.310s. Loss: 3.4472. Loss_x: 2.7884. Loss_u: 0.6588. Mask: 0.35. :   9%|▉         | 3/32 [00:03<00:37,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.133s. Batch: 1.292s. Loss: 3.8918. Loss_x: 3.1841. Loss_u: 0.7077. Mask: 0.37. :   9%|▉         | 3/32 [00:05<00:37,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.133s. Batch: 1.292s. Loss: 3.8918. Loss_x: 3.1841. Loss_u: 0.7077. Mask: 0.37. :  12%|█▎        | 4/32 [00:05<00:35,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.129s. Batch: 1.283s. Loss: 3.7097. Loss_x: 2.9918. Loss_u: 0.7179. Mask: 0.38. :  12%|█▎        | 4/32 [00:06<00:35,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.129s. Batch: 1.283s. Loss: 3.7097. Loss_x: 2.9918. Loss_u: 0.7179. Mask: 0.38. :  16%|█▌        | 5/32 [00:06<00:34,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.126s. Batch: 1.280s. Loss: 3.5607. Loss_x: 2.7967. Loss_u: 0.7640. Mask: 0.38. :  16%|█▌        | 5/32 [00:07<00:34,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.126s. Batch: 1.280s. Loss: 3.5607. Loss_x: 2.7967. Loss_u: 0.7640. Mask: 0.38. :  19%|█▉        | 6/32 [00:07<00:32,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.123s. Batch: 1.274s. Loss: 3.5604. Loss_x: 2.8053. Loss_u: 0.7551. Mask: 0.37. :  19%|█▉        | 6/32 [00:08<00:32,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.123s. Batch: 1.274s. Loss: 3.5604. Loss_x: 2.8053. Loss_u: 0.7551. Mask: 0.37. :  22%|██▏       | 7/32 [00:08<00:31,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.124s. Batch: 1.273s. Loss: 3.5709. Loss_x: 2.8317. Loss_u: 0.7392. Mask: 0.37. :  22%|██▏       | 7/32 [00:10<00:31,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.124s. Batch: 1.273s. Loss: 3.5709. Loss_x: 2.8317. Loss_u: 0.7392. Mask: 0.37. :  25%|██▌       | 8/32 [00:10<00:30,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.124s. Batch: 1.273s. Loss: 3.5148. Loss_x: 2.7638. Loss_u: 0.7510. Mask: 0.37. :  25%|██▌       | 8/32 [00:11<00:30,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.124s. Batch: 1.273s. Loss: 3.5148. Loss_x: 2.7638. Loss_u: 0.7510. Mask: 0.37. :  28%|██▊       | 9/32 [00:11<00:29,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.126s. Batch: 1.276s. Loss: 3.5432. Loss_x: 2.7809. Loss_u: 0.7623. Mask: 0.36. :  28%|██▊       | 9/32 [00:12<00:29,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.126s. Batch: 1.276s. Loss: 3.5432. Loss_x: 2.7809. Loss_u: 0.7623. Mask: 0.36. :  31%|███▏      | 10/32 [00:12<00:28,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.132s. Batch: 1.281s. Loss: 3.4220. Loss_x: 2.6820. Loss_u: 0.7400. Mask: 0.36. :  31%|███▏      | 10/32 [00:14<00:28,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.132s. Batch: 1.281s. Loss: 3.4220. Loss_x: 2.6820. Loss_u: 0.7400. Mask: 0.36. :  34%|███▍      | 11/32 [00:14<00:27,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.154s. Batch: 1.302s. Loss: 3.4657. Loss_x: 2.7309. Loss_u: 0.7348. Mask: 0.36. :  34%|███▍      | 11/32 [00:15<00:27,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.154s. Batch: 1.302s. Loss: 3.4657. Loss_x: 2.7309. Loss_u: 0.7348. Mask: 0.36. :  38%|███▊      | 12/32 [00:15<00:27,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.151s. Batch: 1.299s. Loss: 3.4741. Loss_x: 2.7337. Loss_u: 0.7404. Mask: 0.36. :  38%|███▊      | 12/32 [00:16<00:27,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.151s. Batch: 1.299s. Loss: 3.4741. Loss_x: 2.7337. Loss_u: 0.7404. Mask: 0.36. :  41%|████      | 13/32 [00:16<00:25,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.155s. Batch: 1.303s. Loss: 3.4668. Loss_x: 2.7337. Loss_u: 0.7331. Mask: 0.35. :  41%|████      | 13/32 [00:18<00:25,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.155s. Batch: 1.303s. Loss: 3.4668. Loss_x: 2.7337. Loss_u: 0.7331. Mask: 0.35. :  44%|████▍     | 14/32 [00:18<00:24,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.152s. Batch: 1.299s. Loss: 3.4768. Loss_x: 2.7408. Loss_u: 0.7360. Mask: 0.35. :  44%|████▍     | 14/32 [00:19<00:24,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.152s. Batch: 1.299s. Loss: 3.4768. Loss_x: 2.7408. Loss_u: 0.7360. Mask: 0.35. :  47%|████▋     | 15/32 [00:19<00:22,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.149s. Batch: 1.298s. Loss: 3.4250. Loss_x: 2.6885. Loss_u: 0.7364. Mask: 0.35. :  47%|████▋     | 15/32 [00:20<00:22,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.149s. Batch: 1.298s. Loss: 3.4250. Loss_x: 2.6885. Loss_u: 0.7364. Mask: 0.35. :  50%|█████     | 16/32 [00:20<00:20,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.149s. Batch: 1.297s. Loss: 3.4003. Loss_x: 2.6552. Loss_u: 0.7451. Mask: 0.35. :  50%|█████     | 16/32 [00:22<00:20,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.149s. Batch: 1.297s. Loss: 3.4003. Loss_x: 2.6552. Loss_u: 0.7451. Mask: 0.35. :  53%|█████▎    | 17/32 [00:22<00:19,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.154s. Batch: 1.301s. Loss: 3.3573. Loss_x: 2.6173. Loss_u: 0.7400. Mask: 0.35. :  53%|█████▎    | 17/32 [00:23<00:19,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.154s. Batch: 1.301s. Loss: 3.3573. Loss_x: 2.6173. Loss_u: 0.7400. Mask: 0.35. :  56%|█████▋    | 18/32 [00:23<00:18,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.152s. Batch: 1.299s. Loss: 3.3210. Loss_x: 2.5777. Loss_u: 0.7433. Mask: 0.35. :  56%|█████▋    | 18/32 [00:24<00:18,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.152s. Batch: 1.299s. Loss: 3.3210. Loss_x: 2.5777. Loss_u: 0.7433. Mask: 0.35. :  59%|█████▉    | 19/32 [00:24<00:16,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.152s. Batch: 1.298s. Loss: 3.3131. Loss_x: 2.5701. Loss_u: 0.7430. Mask: 0.35. :  59%|█████▉    | 19/32 [00:25<00:16,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.152s. Batch: 1.298s. Loss: 3.3131. Loss_x: 2.5701. Loss_u: 0.7430. Mask: 0.35. :  62%|██████▎   | 20/32 [00:25<00:15,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.151s. Batch: 1.297s. Loss: 3.2919. Loss_x: 2.5472. Loss_u: 0.7447. Mask: 0.35. :  62%|██████▎   | 20/32 [00:27<00:15,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.151s. Batch: 1.297s. Loss: 3.2919. Loss_x: 2.5472. Loss_u: 0.7447. Mask: 0.35. :  66%|██████▌   | 21/32 [00:27<00:14,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.152s. Batch: 1.298s. Loss: 3.2921. Loss_x: 2.5468. Loss_u: 0.7453. Mask: 0.34. :  66%|██████▌   | 21/32 [00:28<00:14,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.152s. Batch: 1.298s. Loss: 3.2921. Loss_x: 2.5468. Loss_u: 0.7453. Mask: 0.34. :  69%|██████▉   | 22/32 [00:28<00:12,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.156s. Batch: 1.302s. Loss: 3.2867. Loss_x: 2.5387. Loss_u: 0.7480. Mask: 0.35. :  69%|██████▉   | 22/32 [00:29<00:12,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.156s. Batch: 1.302s. Loss: 3.2867. Loss_x: 2.5387. Loss_u: 0.7480. Mask: 0.35. :  72%|███████▏  | 23/32 [00:29<00:11,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.158s. Batch: 1.305s. Loss: 3.2562. Loss_x: 2.5133. Loss_u: 0.7429. Mask: 0.34. :  72%|███████▏  | 23/32 [00:31<00:11,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.158s. Batch: 1.305s. Loss: 3.2562. Loss_x: 2.5133. Loss_u: 0.7429. Mask: 0.34. :  75%|███████▌  | 24/32 [00:31<00:10,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.157s. Batch: 1.302s. Loss: 3.2651. Loss_x: 2.5217. Loss_u: 0.7434. Mask: 0.34. :  75%|███████▌  | 24/32 [00:32<00:10,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.157s. Batch: 1.302s. Loss: 3.2651. Loss_x: 2.5217. Loss_u: 0.7434. Mask: 0.34. :  78%|███████▊  | 25/32 [00:32<00:09,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.160s. Batch: 1.306s. Loss: 3.2767. Loss_x: 2.5305. Loss_u: 0.7462. Mask: 0.34. :  78%|███████▊  | 25/32 [00:33<00:09,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.160s. Batch: 1.306s. Loss: 3.2767. Loss_x: 2.5305. Loss_u: 0.7462. Mask: 0.34. :  81%|████████▏ | 26/32 [00:33<00:08,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.160s. Batch: 1.307s. Loss: 3.2571. Loss_x: 2.5171. Loss_u: 0.7399. Mask: 0.34. :  81%|████████▏ | 26/32 [00:35<00:08,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.160s. Batch: 1.307s. Loss: 3.2571. Loss_x: 2.5171. Loss_u: 0.7399. Mask: 0.34. :  84%|████████▍ | 27/32 [00:35<00:06,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.161s. Batch: 1.307s. Loss: 3.2167. Loss_x: 2.4807. Loss_u: 0.7360. Mask: 0.34. :  84%|████████▍ | 27/32 [00:36<00:06,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.161s. Batch: 1.307s. Loss: 3.2167. Loss_x: 2.4807. Loss_u: 0.7360. Mask: 0.34. :  88%|████████▊ | 28/32 [00:36<00:05,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.161s. Batch: 1.307s. Loss: 3.2117. Loss_x: 2.4758. Loss_u: 0.7359. Mask: 0.34. :  88%|████████▊ | 28/32 [00:37<00:05,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.161s. Batch: 1.307s. Loss: 3.2117. Loss_x: 2.4758. Loss_u: 0.7359. Mask: 0.34. :  91%|█████████ | 29/32 [00:37<00:03,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.160s. Batch: 1.306s. Loss: 3.1860. Loss_x: 2.4553. Loss_u: 0.7306. Mask: 0.34. :  91%|█████████ | 29/32 [00:39<00:03,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.160s. Batch: 1.306s. Loss: 3.1860. Loss_x: 2.4553. Loss_u: 0.7306. Mask: 0.34. :  94%|█████████▍| 30/32 [00:39<00:02,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.160s. Batch: 1.306s. Loss: 3.1893. Loss_x: 2.4651. Loss_u: 0.7243. Mask: 0.34. :  94%|█████████▍| 30/32 [00:40<00:02,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.160s. Batch: 1.306s. Loss: 3.1893. Loss_x: 2.4651. Loss_u: 0.7243. Mask: 0.34. :  97%|█████████▋| 31/32 [00:40<00:01,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.159s. Batch: 1.305s. Loss: 3.1963. Loss_x: 2.4831. Loss_u: 0.7132. Mask: 0.34. :  97%|█████████▋| 31/32 [00:41<00:01,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 5/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.159s. Batch: 1.305s. Loss: 3.1963. Loss_x: 2.4831. Loss_u: 0.7132. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.151s. Batch: 1.287s. Loss: 3.9561. Loss_x: 3.3068. Loss_u: 0.6493. Mask: 0.40. :   0%|          | 0/32 [00:01<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.151s. Batch: 1.287s. Loss: 3.9561. Loss_x: 3.3068. Loss_u: 0.6493. Mask: 0.40. :   3%|▎         | 1/32 [00:01<00:39,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.177s. Batch: 1.317s. Loss: 3.6610. Loss_x: 2.9657. Loss_u: 0.6953. Mask: 0.35. :   3%|▎         | 1/32 [00:02<00:39,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.177s. Batch: 1.317s. Loss: 3.6610. Loss_x: 2.9657. Loss_u: 0.6953. Mask: 0.35. :   6%|▋         | 2/32 [00:02<00:39,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.277s. Batch: 1.417s. Loss: 3.6380. Loss_x: 2.9493. Loss_u: 0.6887. Mask: 0.34. :   6%|▋         | 2/32 [00:04<00:39,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.277s. Batch: 1.417s. Loss: 3.6380. Loss_x: 2.9493. Loss_u: 0.6887. Mask: 0.34. :   9%|▉         | 3/32 [00:04<00:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.284s. Batch: 1.429s. Loss: 3.5335. Loss_x: 2.8215. Loss_u: 0.7120. Mask: 0.35. :   9%|▉         | 3/32 [00:05<00:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.284s. Batch: 1.429s. Loss: 3.5335. Loss_x: 2.8215. Loss_u: 0.7120. Mask: 0.35. :  12%|█▎        | 4/32 [00:05<00:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.254s. Batch: 1.397s. Loss: 3.5123. Loss_x: 2.8120. Loss_u: 0.7003. Mask: 0.35. :  12%|█▎        | 4/32 [00:06<00:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.254s. Batch: 1.397s. Loss: 3.5123. Loss_x: 2.8120. Loss_u: 0.7003. Mask: 0.35. :  16%|█▌        | 5/32 [00:06<00:37,  1.39s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.228s. Batch: 1.370s. Loss: 3.4577. Loss_x: 2.7682. Loss_u: 0.6894. Mask: 0.34. :  16%|█▌        | 5/32 [00:08<00:37,  1.39s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.228s. Batch: 1.370s. Loss: 3.4577. Loss_x: 2.7682. Loss_u: 0.6894. Mask: 0.34. :  19%|█▉        | 6/32 [00:08<00:34,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.210s. Batch: 1.351s. Loss: 3.4838. Loss_x: 2.8044. Loss_u: 0.6795. Mask: 0.34. :  19%|█▉        | 6/32 [00:09<00:34,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.210s. Batch: 1.351s. Loss: 3.4838. Loss_x: 2.8044. Loss_u: 0.6795. Mask: 0.34. :  22%|██▏       | 7/32 [00:09<00:32,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.201s. Batch: 1.342s. Loss: 3.3917. Loss_x: 2.6995. Loss_u: 0.6923. Mask: 0.34. :  22%|██▏       | 7/32 [00:10<00:32,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.201s. Batch: 1.342s. Loss: 3.3917. Loss_x: 2.6995. Loss_u: 0.6923. Mask: 0.34. :  25%|██▌       | 8/32 [00:10<00:31,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.193s. Batch: 1.333s. Loss: 3.3748. Loss_x: 2.6728. Loss_u: 0.7020. Mask: 0.35. :  25%|██▌       | 8/32 [00:11<00:31,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.193s. Batch: 1.333s. Loss: 3.3748. Loss_x: 2.6728. Loss_u: 0.7020. Mask: 0.35. :  28%|██▊       | 9/32 [00:11<00:29,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.185s. Batch: 1.324s. Loss: 3.3602. Loss_x: 2.6704. Loss_u: 0.6898. Mask: 0.34. :  28%|██▊       | 9/32 [00:13<00:29,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.185s. Batch: 1.324s. Loss: 3.3602. Loss_x: 2.6704. Loss_u: 0.6898. Mask: 0.34. :  31%|███▏      | 10/32 [00:13<00:28,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.188s. Batch: 1.327s. Loss: 3.3058. Loss_x: 2.6192. Loss_u: 0.6867. Mask: 0.34. :  31%|███▏      | 10/32 [00:14<00:28,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.188s. Batch: 1.327s. Loss: 3.3058. Loss_x: 2.6192. Loss_u: 0.6867. Mask: 0.34. :  34%|███▍      | 11/32 [00:14<00:27,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.183s. Batch: 1.322s. Loss: 3.2903. Loss_x: 2.6010. Loss_u: 0.6892. Mask: 0.33. :  34%|███▍      | 11/32 [00:15<00:27,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.183s. Batch: 1.322s. Loss: 3.2903. Loss_x: 2.6010. Loss_u: 0.6892. Mask: 0.33. :  38%|███▊      | 12/32 [00:15<00:25,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.180s. Batch: 1.319s. Loss: 3.2629. Loss_x: 2.5778. Loss_u: 0.6851. Mask: 0.33. :  38%|███▊      | 12/32 [00:17<00:25,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.180s. Batch: 1.319s. Loss: 3.2629. Loss_x: 2.5778. Loss_u: 0.6851. Mask: 0.33. :  41%|████      | 13/32 [00:17<00:24,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.179s. Batch: 1.319s. Loss: 3.2076. Loss_x: 2.5232. Loss_u: 0.6844. Mask: 0.33. :  41%|████      | 13/32 [00:18<00:24,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.179s. Batch: 1.319s. Loss: 3.2076. Loss_x: 2.5232. Loss_u: 0.6844. Mask: 0.33. :  44%|████▍     | 14/32 [00:18<00:23,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.175s. Batch: 1.315s. Loss: 3.2291. Loss_x: 2.5465. Loss_u: 0.6826. Mask: 0.33. :  44%|████▍     | 14/32 [00:19<00:23,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.175s. Batch: 1.315s. Loss: 3.2291. Loss_x: 2.5465. Loss_u: 0.6826. Mask: 0.33. :  47%|████▋     | 15/32 [00:19<00:21,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.173s. Batch: 1.314s. Loss: 3.3108. Loss_x: 2.6353. Loss_u: 0.6756. Mask: 0.34. :  47%|████▋     | 15/32 [00:21<00:21,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.173s. Batch: 1.314s. Loss: 3.3108. Loss_x: 2.6353. Loss_u: 0.6756. Mask: 0.34. :  50%|█████     | 16/32 [00:21<00:20,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.172s. Batch: 1.313s. Loss: 3.3560. Loss_x: 2.6779. Loss_u: 0.6782. Mask: 0.34. :  50%|█████     | 16/32 [00:22<00:20,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.172s. Batch: 1.313s. Loss: 3.3560. Loss_x: 2.6779. Loss_u: 0.6782. Mask: 0.34. :  53%|█████▎    | 17/32 [00:22<00:19,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.170s. Batch: 1.311s. Loss: 3.3857. Loss_x: 2.7201. Loss_u: 0.6656. Mask: 0.34. :  53%|█████▎    | 17/32 [00:23<00:19,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.170s. Batch: 1.311s. Loss: 3.3857. Loss_x: 2.7201. Loss_u: 0.6656. Mask: 0.34. :  56%|█████▋    | 18/32 [00:23<00:17,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.174s. Batch: 1.315s. Loss: 3.4473. Loss_x: 2.7790. Loss_u: 0.6683. Mask: 0.34. :  56%|█████▋    | 18/32 [00:24<00:17,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.174s. Batch: 1.315s. Loss: 3.4473. Loss_x: 2.7790. Loss_u: 0.6683. Mask: 0.34. :  59%|█████▉    | 19/32 [00:24<00:17,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.178s. Batch: 1.320s. Loss: 3.3833. Loss_x: 2.7205. Loss_u: 0.6628. Mask: 0.34. :  59%|█████▉    | 19/32 [00:26<00:17,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.178s. Batch: 1.320s. Loss: 3.3833. Loss_x: 2.7205. Loss_u: 0.6628. Mask: 0.34. :  62%|██████▎   | 20/32 [00:26<00:16,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.176s. Batch: 1.320s. Loss: 3.3615. Loss_x: 2.6958. Loss_u: 0.6657. Mask: 0.34. :  62%|██████▎   | 20/32 [00:27<00:16,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.176s. Batch: 1.320s. Loss: 3.3615. Loss_x: 2.6958. Loss_u: 0.6657. Mask: 0.34. :  66%|██████▌   | 21/32 [00:27<00:14,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.174s. Batch: 1.318s. Loss: 3.3338. Loss_x: 2.6651. Loss_u: 0.6687. Mask: 0.33. :  66%|██████▌   | 21/32 [00:28<00:14,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.174s. Batch: 1.318s. Loss: 3.3338. Loss_x: 2.6651. Loss_u: 0.6687. Mask: 0.33. :  69%|██████▉   | 22/32 [00:28<00:13,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.173s. Batch: 1.317s. Loss: 3.3125. Loss_x: 2.6438. Loss_u: 0.6687. Mask: 0.33. :  69%|██████▉   | 22/32 [00:30<00:13,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.173s. Batch: 1.317s. Loss: 3.3125. Loss_x: 2.6438. Loss_u: 0.6687. Mask: 0.33. :  72%|███████▏  | 23/32 [00:30<00:11,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.173s. Batch: 1.316s. Loss: 3.3223. Loss_x: 2.6534. Loss_u: 0.6689. Mask: 0.34. :  72%|███████▏  | 23/32 [00:31<00:11,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.173s. Batch: 1.316s. Loss: 3.3223. Loss_x: 2.6534. Loss_u: 0.6689. Mask: 0.34. :  75%|███████▌  | 24/32 [00:31<00:10,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.171s. Batch: 1.314s. Loss: 3.3259. Loss_x: 2.6496. Loss_u: 0.6764. Mask: 0.34. :  75%|███████▌  | 24/32 [00:32<00:10,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.171s. Batch: 1.314s. Loss: 3.3259. Loss_x: 2.6496. Loss_u: 0.6764. Mask: 0.34. :  78%|███████▊  | 25/32 [00:32<00:09,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.169s. Batch: 1.312s. Loss: 3.3393. Loss_x: 2.6602. Loss_u: 0.6791. Mask: 0.33. :  78%|███████▊  | 25/32 [00:34<00:09,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.169s. Batch: 1.312s. Loss: 3.3393. Loss_x: 2.6602. Loss_u: 0.6791. Mask: 0.33. :  81%|████████▏ | 26/32 [00:34<00:07,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.176s. Batch: 1.319s. Loss: 3.3883. Loss_x: 2.7163. Loss_u: 0.6721. Mask: 0.34. :  81%|████████▏ | 26/32 [00:35<00:07,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.176s. Batch: 1.319s. Loss: 3.3883. Loss_x: 2.7163. Loss_u: 0.6721. Mask: 0.34. :  84%|████████▍ | 27/32 [00:35<00:06,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.184s. Batch: 1.327s. Loss: 3.3837. Loss_x: 2.7124. Loss_u: 0.6713. Mask: 0.34. :  84%|████████▍ | 27/32 [00:37<00:06,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.184s. Batch: 1.327s. Loss: 3.3837. Loss_x: 2.7124. Loss_u: 0.6713. Mask: 0.34. :  88%|████████▊ | 28/32 [00:37<00:05,  1.41s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.181s. Batch: 1.324s. Loss: 3.3904. Loss_x: 2.7171. Loss_u: 0.6733. Mask: 0.34. :  88%|████████▊ | 28/32 [00:38<00:05,  1.41s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.181s. Batch: 1.324s. Loss: 3.3904. Loss_x: 2.7171. Loss_u: 0.6733. Mask: 0.34. :  91%|█████████ | 29/32 [00:38<00:04,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.179s. Batch: 1.322s. Loss: 3.3801. Loss_x: 2.7077. Loss_u: 0.6724. Mask: 0.34. :  91%|█████████ | 29/32 [00:39<00:04,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.179s. Batch: 1.322s. Loss: 3.3801. Loss_x: 2.7077. Loss_u: 0.6724. Mask: 0.34. :  94%|█████████▍| 30/32 [00:39<00:02,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.178s. Batch: 1.321s. Loss: 3.3790. Loss_x: 2.7105. Loss_u: 0.6685. Mask: 0.34. :  94%|█████████▍| 30/32 [00:40<00:02,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.178s. Batch: 1.321s. Loss: 3.3790. Loss_x: 2.7105. Loss_u: 0.6685. Mask: 0.34. :  97%|█████████▋| 31/32 [00:40<00:01,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.178s. Batch: 1.321s. Loss: 3.3795. Loss_x: 2.7136. Loss_u: 0.6659. Mask: 0.34. :  97%|█████████▋| 31/32 [00:42<00:01,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 6/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.178s. Batch: 1.321s. Loss: 3.3795. Loss_x: 2.7136. Loss_u: 0.6659. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.135s. Batch: 1.303s. Loss: 3.3914. Loss_x: 2.8625. Loss_u: 0.5289. Mask: 0.34. :   0%|          | 0/32 [00:01<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    1/  32. LR: 0.0300. Data: 1.135s. Batch: 1.303s. Loss: 3.3914. Loss_x: 2.8625. Loss_u: 0.5289. Mask: 0.34. :   3%|▎         | 1/32 [00:01<00:40,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.150s. Batch: 1.302s. Loss: 3.2475. Loss_x: 2.6308. Loss_u: 0.6167. Mask: 0.35. :   3%|▎         | 1/32 [00:02<00:40,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    2/  32. LR: 0.0300. Data: 1.150s. Batch: 1.302s. Loss: 3.2475. Loss_x: 2.6308. Loss_u: 0.6167. Mask: 0.35. :   6%|▋         | 2/32 [00:02<00:38,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.157s. Batch: 1.313s. Loss: 3.2715. Loss_x: 2.6614. Loss_u: 0.6101. Mask: 0.34. :   6%|▋         | 2/32 [00:03<00:38,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    3/  32. LR: 0.0300. Data: 1.157s. Batch: 1.313s. Loss: 3.2715. Loss_x: 2.6614. Loss_u: 0.6101. Mask: 0.34. :   9%|▉         | 3/32 [00:03<00:38,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.176s. Batch: 1.326s. Loss: 3.3370. Loss_x: 2.7235. Loss_u: 0.6135. Mask: 0.35. :   9%|▉         | 3/32 [00:05<00:38,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    4/  32. LR: 0.0300. Data: 1.176s. Batch: 1.326s. Loss: 3.3370. Loss_x: 2.7235. Loss_u: 0.6135. Mask: 0.35. :  12%|█▎        | 4/32 [00:05<00:37,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.188s. Batch: 1.336s. Loss: 3.3404. Loss_x: 2.7076. Loss_u: 0.6328. Mask: 0.35. :  12%|█▎        | 4/32 [00:06<00:37,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    5/  32. LR: 0.0300. Data: 1.188s. Batch: 1.336s. Loss: 3.3404. Loss_x: 2.7076. Loss_u: 0.6328. Mask: 0.35. :  16%|█▌        | 5/32 [00:06<00:36,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.177s. Batch: 1.323s. Loss: 3.3440. Loss_x: 2.7081. Loss_u: 0.6359. Mask: 0.35. :  16%|█▌        | 5/32 [00:07<00:36,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    6/  32. LR: 0.0300. Data: 1.177s. Batch: 1.323s. Loss: 3.3440. Loss_x: 2.7081. Loss_u: 0.6359. Mask: 0.35. :  19%|█▉        | 6/32 [00:07<00:34,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.167s. Batch: 1.311s. Loss: 3.3093. Loss_x: 2.6912. Loss_u: 0.6180. Mask: 0.34. :  19%|█▉        | 6/32 [00:09<00:34,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    7/  32. LR: 0.0300. Data: 1.167s. Batch: 1.311s. Loss: 3.3093. Loss_x: 2.6912. Loss_u: 0.6180. Mask: 0.34. :  22%|██▏       | 7/32 [00:09<00:32,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.162s. Batch: 1.305s. Loss: 3.3303. Loss_x: 2.6738. Loss_u: 0.6565. Mask: 0.35. :  22%|██▏       | 7/32 [00:10<00:32,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    8/  32. LR: 0.0300. Data: 1.162s. Batch: 1.305s. Loss: 3.3303. Loss_x: 2.6738. Loss_u: 0.6565. Mask: 0.35. :  25%|██▌       | 8/32 [00:10<00:30,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.157s. Batch: 1.301s. Loss: 3.3107. Loss_x: 2.6560. Loss_u: 0.6547. Mask: 0.35. :  25%|██▌       | 8/32 [00:11<00:30,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:    9/  32. LR: 0.0300. Data: 1.157s. Batch: 1.301s. Loss: 3.3107. Loss_x: 2.6560. Loss_u: 0.6547. Mask: 0.35. :  28%|██▊       | 9/32 [00:11<00:29,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.154s. Batch: 1.298s. Loss: 3.3450. Loss_x: 2.6895. Loss_u: 0.6556. Mask: 0.35. :  28%|██▊       | 9/32 [00:12<00:29,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   10/  32. LR: 0.0300. Data: 1.154s. Batch: 1.298s. Loss: 3.3450. Loss_x: 2.6895. Loss_u: 0.6556. Mask: 0.35. :  31%|███▏      | 10/32 [00:12<00:28,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.150s. Batch: 1.293s. Loss: 3.2471. Loss_x: 2.5911. Loss_u: 0.6560. Mask: 0.34. :  31%|███▏      | 10/32 [00:14<00:28,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   11/  32. LR: 0.0300. Data: 1.150s. Batch: 1.293s. Loss: 3.2471. Loss_x: 2.5911. Loss_u: 0.6560. Mask: 0.34. :  34%|███▍      | 11/32 [00:14<00:26,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.153s. Batch: 1.295s. Loss: 3.3002. Loss_x: 2.6325. Loss_u: 0.6677. Mask: 0.34. :  34%|███▍      | 11/32 [00:15<00:26,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   12/  32. LR: 0.0300. Data: 1.153s. Batch: 1.295s. Loss: 3.3002. Loss_x: 2.6325. Loss_u: 0.6677. Mask: 0.34. :  38%|███▊      | 12/32 [00:15<00:25,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.150s. Batch: 1.292s. Loss: 3.2527. Loss_x: 2.5793. Loss_u: 0.6733. Mask: 0.34. :  38%|███▊      | 12/32 [00:16<00:25,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   13/  32. LR: 0.0300. Data: 1.150s. Batch: 1.292s. Loss: 3.2527. Loss_x: 2.5793. Loss_u: 0.6733. Mask: 0.34. :  41%|████      | 13/32 [00:16<00:24,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   14/  32. LR: 0.0300. Data: 1.148s. Batch: 1.290s. Loss: 3.2960. Loss_x: 2.6049. Loss_u: 0.6911. Mask: 0.34. :  41%|████      | 13/32 [00:18<00:24,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 1/   1. Iter:  193/ 512. LR: 0.0261. Data: 1.162s. Batch: 1.345s. Loss: 3.2404. Loss_x: 2.6929. Loss_u: 0.5475. Mask: 0.31. :  38%|███▊      | 193/512 [27:17<45:07,  8.49s/it]A\u001b[A\u001b[A\nTrain Epoch: 1/   1. Iter:   88/ 512. LR: 0.0292. Data: 1.193s. Batch: 1.337s. Loss: 3.2962. Loss_x: 2.6656. Loss_u: 0.6306. Mask: 0.32. :  17%|█▋        | 88/512 [15:53<1:16:36, 10.84s/it]\nTrain Epoch: 1/   1. Iter:  133/ 512. LR: 0.0281. Data: 1.202s. Batch: 1.345s. Loss: 3.1878. Loss_x: 2.5145. Loss_u: 0.6733. Mask: 0.33. :  26%|██▌       | 133/512 [13:53<39:35,  6.27s/it]\n\n\n\nTrain Epoch: 7/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.173s. Batch: 1.315s. Loss: 3.2589. Loss_x: 2.5498. Loss_u: 0.7091. Mask: 0.34. :  44%|████▍     | 14/32 [00:19<00:22,  1.27s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   15/  32. LR: 0.0300. Data: 1.173s. Batch: 1.315s. Loss: 3.2589. Loss_x: 2.5498. Loss_u: 0.7091. Mask: 0.34. :  47%|████▋     | 15/32 [00:19<00:23,  1.39s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.171s. Batch: 1.314s. Loss: 3.2363. Loss_x: 2.5288. Loss_u: 0.7074. Mask: 0.34. :  47%|████▋     | 15/32 [00:21<00:23,  1.39s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   16/  32. LR: 0.0300. Data: 1.171s. Batch: 1.314s. Loss: 3.2363. Loss_x: 2.5288. Loss_u: 0.7074. Mask: 0.34. :  50%|█████     | 16/32 [00:21<00:21,  1.36s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.169s. Batch: 1.312s. Loss: 3.2529. Loss_x: 2.5500. Loss_u: 0.7029. Mask: 0.34. :  50%|█████     | 16/32 [00:22<00:21,  1.36s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   17/  32. LR: 0.0300. Data: 1.169s. Batch: 1.312s. Loss: 3.2529. Loss_x: 2.5500. Loss_u: 0.7029. Mask: 0.34. :  53%|█████▎    | 17/32 [00:22<00:20,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.169s. Batch: 1.311s. Loss: 3.2178. Loss_x: 2.5200. Loss_u: 0.6978. Mask: 0.34. :  53%|█████▎    | 17/32 [00:23<00:20,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   18/  32. LR: 0.0300. Data: 1.169s. Batch: 1.311s. Loss: 3.2178. Loss_x: 2.5200. Loss_u: 0.6978. Mask: 0.34. :  56%|█████▋    | 18/32 [00:23<00:18,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.186s. Batch: 1.328s. Loss: 3.2303. Loss_x: 2.5331. Loss_u: 0.6973. Mask: 0.34. :  56%|█████▋    | 18/32 [00:25<00:18,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   19/  32. LR: 0.0300. Data: 1.186s. Batch: 1.328s. Loss: 3.2303. Loss_x: 2.5331. Loss_u: 0.6973. Mask: 0.34. :  59%|█████▉    | 19/32 [00:25<00:18,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.192s. Batch: 1.334s. Loss: 3.3095. Loss_x: 2.6068. Loss_u: 0.7027. Mask: 0.34. :  59%|█████▉    | 19/32 [00:26<00:18,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   20/  32. LR: 0.0300. Data: 1.192s. Batch: 1.334s. Loss: 3.3095. Loss_x: 2.6068. Loss_u: 0.7027. Mask: 0.34. :  62%|██████▎   | 20/32 [00:26<00:17,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.190s. Batch: 1.332s. Loss: 3.3467. Loss_x: 2.6513. Loss_u: 0.6954. Mask: 0.35. :  62%|██████▎   | 20/32 [00:27<00:17,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   21/  32. LR: 0.0300. Data: 1.190s. Batch: 1.332s. Loss: 3.3467. Loss_x: 2.6513. Loss_u: 0.6954. Mask: 0.35. :  66%|██████▌   | 21/32 [00:27<00:15,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.189s. Batch: 1.332s. Loss: 3.3772. Loss_x: 2.6805. Loss_u: 0.6966. Mask: 0.35. :  66%|██████▌   | 21/32 [00:29<00:15,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   22/  32. LR: 0.0300. Data: 1.189s. Batch: 1.332s. Loss: 3.3772. Loss_x: 2.6805. Loss_u: 0.6966. Mask: 0.35. :  69%|██████▉   | 22/32 [00:29<00:13,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.187s. Batch: 1.329s. Loss: 3.3702. Loss_x: 2.6708. Loss_u: 0.6994. Mask: 0.35. :  69%|██████▉   | 22/32 [00:30<00:13,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   23/  32. LR: 0.0300. Data: 1.187s. Batch: 1.329s. Loss: 3.3702. Loss_x: 2.6708. Loss_u: 0.6994. Mask: 0.35. :  72%|███████▏  | 23/32 [00:30<00:12,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.185s. Batch: 1.329s. Loss: 3.3715. Loss_x: 2.6644. Loss_u: 0.7071. Mask: 0.35. :  72%|███████▏  | 23/32 [00:31<00:12,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   24/  32. LR: 0.0300. Data: 1.185s. Batch: 1.329s. Loss: 3.3715. Loss_x: 2.6644. Loss_u: 0.7071. Mask: 0.35. :  75%|███████▌  | 24/32 [00:31<00:10,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.184s. Batch: 1.327s. Loss: 3.4186. Loss_x: 2.7096. Loss_u: 0.7090. Mask: 0.35. :  75%|███████▌  | 24/32 [00:33<00:10,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   25/  32. LR: 0.0300. Data: 1.184s. Batch: 1.327s. Loss: 3.4186. Loss_x: 2.7096. Loss_u: 0.7090. Mask: 0.35. :  78%|███████▊  | 25/32 [00:33<00:09,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.182s. Batch: 1.327s. Loss: 3.4060. Loss_x: 2.6948. Loss_u: 0.7112. Mask: 0.35. :  78%|███████▊  | 25/32 [00:34<00:09,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   26/  32. LR: 0.0300. Data: 1.182s. Batch: 1.327s. Loss: 3.4060. Loss_x: 2.6948. Loss_u: 0.7112. Mask: 0.35. :  81%|████████▏ | 26/32 [00:34<00:07,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.180s. Batch: 1.324s. Loss: 3.4139. Loss_x: 2.7045. Loss_u: 0.7094. Mask: 0.35. :  81%|████████▏ | 26/32 [00:35<00:07,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   27/  32. LR: 0.0300. Data: 1.180s. Batch: 1.324s. Loss: 3.4139. Loss_x: 2.7045. Loss_u: 0.7094. Mask: 0.35. :  84%|████████▍ | 27/32 [00:35<00:06,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.185s. Batch: 1.328s. Loss: 3.3987. Loss_x: 2.6863. Loss_u: 0.7124. Mask: 0.35. :  84%|████████▍ | 27/32 [00:37<00:06,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   28/  32. LR: 0.0300. Data: 1.185s. Batch: 1.328s. Loss: 3.3987. Loss_x: 2.6863. Loss_u: 0.7124. Mask: 0.35. :  88%|████████▊ | 28/32 [00:37<00:05,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.188s. Batch: 1.332s. Loss: 3.4040. Loss_x: 2.6899. Loss_u: 0.7141. Mask: 0.35. :  88%|████████▊ | 28/32 [00:38<00:05,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   29/  32. LR: 0.0300. Data: 1.188s. Batch: 1.332s. Loss: 3.4040. Loss_x: 2.6899. Loss_u: 0.7141. Mask: 0.35. :  91%|█████████ | 29/32 [00:38<00:04,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.187s. Batch: 1.331s. Loss: 3.3856. Loss_x: 2.6645. Loss_u: 0.7210. Mask: 0.35. :  91%|█████████ | 29/32 [00:39<00:04,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   30/  32. LR: 0.0300. Data: 1.187s. Batch: 1.331s. Loss: 3.3856. Loss_x: 2.6645. Loss_u: 0.7210. Mask: 0.35. :  94%|█████████▍| 30/32 [00:39<00:02,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.186s. Batch: 1.330s. Loss: 3.3928. Loss_x: 2.6740. Loss_u: 0.7188. Mask: 0.34. :  94%|█████████▍| 30/32 [00:41<00:02,  1.35s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   31/  32. LR: 0.0300. Data: 1.186s. Batch: 1.330s. Loss: 3.3928. Loss_x: 2.6740. Loss_u: 0.7188. Mask: 0.34. :  97%|█████████▋| 31/32 [00:41<00:01,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.188s. Batch: 1.333s. Loss: 3.3943. Loss_x: 2.6725. Loss_u: 0.7218. Mask: 0.34. :  97%|█████████▋| 31/32 [00:42<00:01,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\nTrain Epoch: 7/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.188s. Batch: 1.333s. Loss: 3.3943. Loss_x: 2.6725. Loss_u: 0.7218. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\u001b[A\u001b[A\u001b[A\nTrain Epoch: 8/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.172s. Batch: 1.316s. Loss: 3.2318. Loss_x: 2.5132. Loss_u: 0.7185. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 9/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.176s. Batch: 1.319s. Loss: 3.5470. Loss_x: 2.8260. Loss_u: 0.7210. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 10/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.172s. Batch: 1.311s. Loss: 3.4787. Loss_x: 2.7600. Loss_u: 0.7186. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 11/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.174s. Batch: 1.317s. Loss: 3.2414. Loss_x: 2.4762. Loss_u: 0.7652. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 12/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.180s. Batch: 1.324s. Loss: 3.2391. Loss_x: 2.5445. Loss_u: 0.6946. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 13/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.178s. Batch: 1.321s. Loss: 3.5838. Loss_x: 2.8243. Loss_u: 0.7595. Mask: 0.36. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 14/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.164s. Batch: 1.303s. Loss: 3.2975. Loss_x: 2.5817. Loss_u: 0.7158. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 15/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.194s. Batch: 1.335s. Loss: 3.3296. Loss_x: 2.5757. Loss_u: 0.7539. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 16/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.174s. Batch: 1.316s. Loss: 3.3726. Loss_x: 2.5910. Loss_u: 0.7816. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 17/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.179s. Batch: 1.318s. Loss: 3.1937. Loss_x: 2.4625. Loss_u: 0.7312. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 18/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.173s. Batch: 1.315s. Loss: 3.4566. Loss_x: 2.7311. Loss_u: 0.7254. Mask: 0.35. : 100%|██████████| 32/32 [00:42<00:00,  1.31s/it]\nTrain Epoch: 19/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.170s. Batch: 1.310s. Loss: 3.3385. Loss_x: 2.5976. Loss_u: 0.7409. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 20/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.190s. Batch: 1.332s. Loss: 3.3909. Loss_x: 2.6971. Loss_u: 0.6938. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 21/ 512. Iter:   32/  32. LR: 0.0300. Data: 1.169s. Batch: 1.311s. Loss: 3.4701. Loss_x: 2.7387. Loss_u: 0.7314. Mask: 0.37. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 22/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.150s. Batch: 1.291s. Loss: 3.4096. Loss_x: 2.6304. Loss_u: 0.7791. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 23/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.181s. Batch: 1.322s. Loss: 3.4162. Loss_x: 2.6469. Loss_u: 0.7692. Mask: 0.35. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 24/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.165s. Batch: 1.306s. Loss: 2.9857. Loss_x: 2.2703. Loss_u: 0.7154. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 25/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.171s. Batch: 1.312s. Loss: 3.1696. Loss_x: 2.4458. Loss_u: 0.7238. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 26/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.162s. Batch: 1.303s. Loss: 3.5980. Loss_x: 2.8085. Loss_u: 0.7896. Mask: 0.38. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 27/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.160s. Batch: 1.299s. Loss: 3.3325. Loss_x: 2.5820. Loss_u: 0.7505. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 28/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.170s. Batch: 1.309s. Loss: 3.2317. Loss_x: 2.5287. Loss_u: 0.7030. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 29/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.175s. Batch: 1.318s. Loss: 3.2521. Loss_x: 2.5670. Loss_u: 0.6851. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 30/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.167s. Batch: 1.310s. Loss: 3.6449. Loss_x: 2.8422. Loss_u: 0.8028. Mask: 0.37. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 31/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.181s. Batch: 1.320s. Loss: 3.2655. Loss_x: 2.5319. Loss_u: 0.7335. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 32/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.165s. Batch: 1.311s. Loss: 3.4840. Loss_x: 2.7353. Loss_u: 0.7487. Mask: 0.36. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 33/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.157s. Batch: 1.301s. Loss: 3.6647. Loss_x: 2.8448. Loss_u: 0.8199. Mask: 0.37. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 34/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.173s. Batch: 1.314s. Loss: 3.2344. Loss_x: 2.4965. Loss_u: 0.7379. Mask: 0.37. : 100%|██████████| 32/32 [00:42<00:00,  1.31s/it]\nTrain Epoch: 35/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.189s. Batch: 1.332s. Loss: 3.4048. Loss_x: 2.6418. Loss_u: 0.7630. Mask: 0.36. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 36/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.161s. Batch: 1.305s. Loss: 3.2123. Loss_x: 2.4668. Loss_u: 0.7455. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 37/ 512. Iter:   32/  32. LR: 0.0299. Data: 1.162s. Batch: 1.301s. Loss: 3.2216. Loss_x: 2.5163. Loss_u: 0.7053. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 38/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.174s. Batch: 1.314s. Loss: 2.9891. Loss_x: 2.2756. Loss_u: 0.7136. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.31s/it]\nTrain Epoch: 39/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.154s. Batch: 1.296s. Loss: 3.2035. Loss_x: 2.5041. Loss_u: 0.6994. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 40/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.149s. Batch: 1.293s. Loss: 3.4071. Loss_x: 2.6465. Loss_u: 0.7606. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 41/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.178s. Batch: 1.320s. Loss: 3.1991. Loss_x: 2.4694. Loss_u: 0.7298. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 42/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.148s. Batch: 1.287s. Loss: 3.4884. Loss_x: 2.7652. Loss_u: 0.7233. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 43/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.150s. Batch: 1.292s. Loss: 3.7544. Loss_x: 2.9794. Loss_u: 0.7749. Mask: 0.37. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 44/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.183s. Batch: 1.323s. Loss: 3.1802. Loss_x: 2.4750. Loss_u: 0.7052. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 45/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.145s. Batch: 1.285s. Loss: 3.3569. Loss_x: 2.6562. Loss_u: 0.7006. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 46/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.157s. Batch: 1.300s. Loss: 3.4239. Loss_x: 2.6814. Loss_u: 0.7425. Mask: 0.36. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 47/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.160s. Batch: 1.301s. Loss: 3.2586. Loss_x: 2.5050. Loss_u: 0.7535. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 48/ 512. Iter:   32/  32. LR: 0.0298. Data: 1.159s. Batch: 1.301s. Loss: 3.0069. Loss_x: 2.3517. Loss_u: 0.6551. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 49/ 512. Iter:   32/  32. LR: 0.0297. Data: 1.165s. Batch: 1.307s. Loss: 3.4269. Loss_x: 2.7247. Loss_u: 0.7022. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 50/ 512. Iter:   32/  32. LR: 0.0297. Data: 1.166s. Batch: 1.305s. Loss: 3.2634. Loss_x: 2.5433. Loss_u: 0.7201. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 51/ 512. Iter:   32/  32. LR: 0.0297. Data: 1.170s. Batch: 1.310s. Loss: 3.2066. Loss_x: 2.3999. Loss_u: 0.8066. Mask: 0.37. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 52/ 512. Iter:   32/  32. LR: 0.0297. Data: 1.149s. Batch: 1.287s. Loss: 3.3658. Loss_x: 2.6914. Loss_u: 0.6744. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 53/ 512. Iter:   32/  32. LR: 0.0297. Data: 1.149s. Batch: 1.286s. Loss: 3.5576. Loss_x: 2.8293. Loss_u: 0.7283. Mask: 0.37. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 54/ 512. Iter:   32/  32. LR: 0.0297. Data: 1.167s. Batch: 1.306s. Loss: 3.2550. Loss_x: 2.5239. Loss_u: 0.7310. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 55/ 512. Iter:   32/  32. LR: 0.0297. Data: 1.148s. Batch: 1.287s. Loss: 3.2438. Loss_x: 2.5511. Loss_u: 0.6927. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 56/ 512. Iter:   32/  32. LR: 0.0297. Data: 1.146s. Batch: 1.288s. Loss: 3.4320. Loss_x: 2.6693. Loss_u: 0.7627. Mask: 0.38. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 57/ 512. Iter:   32/  32. LR: 0.0296. Data: 1.158s. Batch: 1.303s. Loss: 3.3733. Loss_x: 2.5592. Loss_u: 0.8141. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 58/ 512. Iter:   32/  32. LR: 0.0296. Data: 1.164s. Batch: 1.304s. Loss: 3.3487. Loss_x: 2.5857. Loss_u: 0.7629. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 59/ 512. Iter:   32/  32. LR: 0.0296. Data: 1.140s. Batch: 1.281s. Loss: 3.0957. Loss_x: 2.3802. Loss_u: 0.7156. Mask: 0.33. : 100%|██████████| 32/32 [00:40<00:00,  1.28s/it]\nTrain Epoch: 60/ 512. Iter:   32/  32. LR: 0.0296. Data: 1.175s. Batch: 1.318s. Loss: 3.1943. Loss_x: 2.5094. Loss_u: 0.6848. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 61/ 512. Iter:   32/  32. LR: 0.0296. Data: 1.159s. Batch: 1.300s. Loss: 3.5225. Loss_x: 2.7572. Loss_u: 0.7653. Mask: 0.37. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 62/ 512. Iter:   32/  32. LR: 0.0296. Data: 1.185s. Batch: 1.321s. Loss: 3.5915. Loss_x: 2.8450. Loss_u: 0.7465. Mask: 0.37. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 63/ 512. Iter:   32/  32. LR: 0.0296. Data: 1.255s. Batch: 1.401s. Loss: 3.3280. Loss_x: 2.5846. Loss_u: 0.7433. Mask: 0.34. : 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\nTrain Epoch: 64/ 512. Iter:   32/  32. LR: 0.0296. Data: 1.169s. Batch: 1.311s. Loss: 3.3366. Loss_x: 2.6407. Loss_u: 0.6959. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 65/ 512. Iter:   32/  32. LR: 0.0295. Data: 1.178s. Batch: 1.322s. Loss: 3.5777. Loss_x: 2.8417. Loss_u: 0.7359. Mask: 0.36. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 66/ 512. Iter:   32/  32. LR: 0.0295. Data: 1.157s. Batch: 1.297s. Loss: 3.4064. Loss_x: 2.6726. Loss_u: 0.7338. Mask: 0.36. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 67/ 512. Iter:   32/  32. LR: 0.0295. Data: 1.159s. Batch: 1.301s. Loss: 3.3542. Loss_x: 2.6381. Loss_u: 0.7161. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 68/ 512. Iter:   32/  32. LR: 0.0295. Data: 1.155s. Batch: 1.297s. Loss: 3.0742. Loss_x: 2.3776. Loss_u: 0.6966. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 69/ 512. Iter:   32/  32. LR: 0.0295. Data: 1.184s. Batch: 1.328s. Loss: 3.1372. Loss_x: 2.4085. Loss_u: 0.7287. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 70/ 512. Iter:   32/  32. LR: 0.0295. Data: 1.153s. Batch: 1.295s. Loss: 3.2311. Loss_x: 2.5236. Loss_u: 0.7075. Mask: 0.36. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 71/ 512. Iter:   32/  32. LR: 0.0295. Data: 1.151s. Batch: 1.296s. Loss: 3.1769. Loss_x: 2.4368. Loss_u: 0.7401. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 72/ 512. Iter:   32/  32. LR: 0.0294. Data: 1.169s. Batch: 1.310s. Loss: 3.3152. Loss_x: 2.6006. Loss_u: 0.7146. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 73/ 512. Iter:   32/  32. LR: 0.0294. Data: 1.159s. Batch: 1.298s. Loss: 3.2482. Loss_x: 2.5668. Loss_u: 0.6814. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 74/ 512. Iter:   32/  32. LR: 0.0294. Data: 1.164s. Batch: 1.305s. Loss: 3.2959. Loss_x: 2.5637. Loss_u: 0.7323. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 75/ 512. Iter:   32/  32. LR: 0.0294. Data: 1.182s. Batch: 1.322s. Loss: 3.5312. Loss_x: 2.7831. Loss_u: 0.7481. Mask: 0.36. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 76/ 512. Iter:   32/  32. LR: 0.0294. Data: 1.154s. Batch: 1.293s. Loss: 3.2756. Loss_x: 2.5498. Loss_u: 0.7258. Mask: 0.36. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 77/ 512. Iter:   32/  32. LR: 0.0294. Data: 1.166s. Batch: 1.312s. Loss: 3.3262. Loss_x: 2.6300. Loss_u: 0.6962. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 78/ 512. Iter:   32/  32. LR: 0.0293. Data: 1.176s. Batch: 1.319s. Loss: 3.4792. Loss_x: 2.7599. Loss_u: 0.7193. Mask: 0.36. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 79/ 512. Iter:   32/  32. LR: 0.0293. Data: 1.151s. Batch: 1.293s. Loss: 3.2885. Loss_x: 2.5249. Loss_u: 0.7636. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 80/ 512. Iter:   32/  32. LR: 0.0293. Data: 1.162s. Batch: 1.303s. Loss: 3.0601. Loss_x: 2.3926. Loss_u: 0.6675. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 81/ 512. Iter:   32/  32. LR: 0.0293. Data: 1.158s. Batch: 1.298s. Loss: 3.3591. Loss_x: 2.6569. Loss_u: 0.7022. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 82/ 512. Iter:   32/  32. LR: 0.0293. Data: 1.145s. Batch: 1.287s. Loss: 3.4261. Loss_x: 2.6852. Loss_u: 0.7409. Mask: 0.37. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 83/ 512. Iter:   32/  32. LR: 0.0293. Data: 1.225s. Batch: 1.371s. Loss: 3.0715. Loss_x: 2.3510. Loss_u: 0.7205. Mask: 0.33. : 100%|██████████| 32/32 [00:43<00:00,  1.37s/it]\nTrain Epoch: 84/ 512. Iter:   32/  32. LR: 0.0292. Data: 1.305s. Batch: 1.447s. Loss: 3.1101. Loss_x: 2.3808. Loss_u: 0.7293. Mask: 0.35. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 85/ 512. Iter:   32/  32. LR: 0.0292. Data: 1.301s. Batch: 1.441s. Loss: 3.1389. Loss_x: 2.4308. Loss_u: 0.7081. Mask: 0.33. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 86/ 512. Iter:   32/  32. LR: 0.0292. Data: 1.275s. Batch: 1.415s. Loss: 3.2050. Loss_x: 2.5365. Loss_u: 0.6686. Mask: 0.32. : 100%|██████████| 32/32 [00:45<00:00,  1.41s/it]\nTrain Epoch: 87/ 512. Iter:   32/  32. LR: 0.0292. Data: 1.288s. Batch: 1.428s. Loss: 3.5286. Loss_x: 2.8477. Loss_u: 0.6809. Mask: 0.35. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 88/ 512. Iter:   32/  32. LR: 0.0292. Data: 1.282s. Batch: 1.431s. Loss: 3.0987. Loss_x: 2.3825. Loss_u: 0.7162. Mask: 0.33. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 89/ 512. Iter:   32/  32. LR: 0.0291. Data: 1.277s. Batch: 1.420s. Loss: 3.3993. Loss_x: 2.7173. Loss_u: 0.6820. Mask: 0.36. : 100%|██████████| 32/32 [00:45<00:00,  1.42s/it]\nTrain Epoch: 90/ 512. Iter:   32/  32. LR: 0.0291. Data: 1.299s. Batch: 1.443s. Loss: 3.1953. Loss_x: 2.4804. Loss_u: 0.7149. Mask: 0.34. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 91/ 512. Iter:   32/  32. LR: 0.0291. Data: 1.341s. Batch: 1.484s. Loss: 3.0477. Loss_x: 2.3570. Loss_u: 0.6907. Mask: 0.33. : 100%|██████████| 32/32 [00:47<00:00,  1.48s/it]\nTrain Epoch: 92/ 512. Iter:   32/  32. LR: 0.0291. Data: 1.305s. Batch: 1.452s. Loss: 3.1368. Loss_x: 2.4745. Loss_u: 0.6622. Mask: 0.33. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 93/ 512. Iter:   32/  32. LR: 0.0291. Data: 1.264s. Batch: 1.403s. Loss: 3.1219. Loss_x: 2.5072. Loss_u: 0.6147. Mask: 0.31. : 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\nTrain Epoch: 94/ 512. Iter:   32/  32. LR: 0.0290. Data: 1.265s. Batch: 1.411s. Loss: 3.2490. Loss_x: 2.5977. Loss_u: 0.6514. Mask: 0.32. : 100%|██████████| 32/32 [00:45<00:00,  1.41s/it]\nTrain Epoch: 95/ 512. Iter:   32/  32. LR: 0.0290. Data: 1.259s. Batch: 1.402s. Loss: 3.4319. Loss_x: 2.7139. Loss_u: 0.7180. Mask: 0.35. : 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\nTrain Epoch: 96/ 512. Iter:   32/  32. LR: 0.0290. Data: 1.247s. Batch: 1.387s. Loss: 3.3239. Loss_x: 2.6528. Loss_u: 0.6711. Mask: 0.33. : 100%|██████████| 32/32 [00:44<00:00,  1.39s/it]\nTrain Epoch: 97/ 512. Iter:   32/  32. LR: 0.0290. Data: 1.243s. Batch: 1.382s. Loss: 3.4479. Loss_x: 2.7256. Loss_u: 0.7223. Mask: 0.35. : 100%|██████████| 32/32 [00:44<00:00,  1.38s/it]\nTrain Epoch: 98/ 512. Iter:   32/  32. LR: 0.0290. Data: 1.221s. Batch: 1.361s. Loss: 3.3760. Loss_x: 2.6114. Loss_u: 0.7646. Mask: 0.35. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 99/ 512. Iter:   32/  32. LR: 0.0289. Data: 1.245s. Batch: 1.386s. Loss: 3.2076. Loss_x: 2.5355. Loss_u: 0.6720. Mask: 0.34. : 100%|██████████| 32/32 [00:44<00:00,  1.39s/it]\nTrain Epoch: 100/ 512. Iter:   32/  32. LR: 0.0289. Data: 1.208s. Batch: 1.358s. Loss: 3.0346. Loss_x: 2.3596. Loss_u: 0.6751. Mask: 0.31. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 101/ 512. Iter:   32/  32. LR: 0.0289. Data: 1.191s. Batch: 1.331s. Loss: 3.2954. Loss_x: 2.6373. Loss_u: 0.6580. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 102/ 512. Iter:   32/  32. LR: 0.0289. Data: 1.197s. Batch: 1.337s. Loss: 3.2655. Loss_x: 2.5619. Loss_u: 0.7036. Mask: 0.35. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 103/ 512. Iter:   32/  32. LR: 0.0289. Data: 1.165s. Batch: 1.308s. Loss: 3.2444. Loss_x: 2.5456. Loss_u: 0.6988. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 104/ 512. Iter:   32/  32. LR: 0.0288. Data: 1.163s. Batch: 1.304s. Loss: 3.3829. Loss_x: 2.6927. Loss_u: 0.6902. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 105/ 512. Iter:   32/  32. LR: 0.0288. Data: 1.210s. Batch: 1.350s. Loss: 3.0032. Loss_x: 2.3021. Loss_u: 0.7011. Mask: 0.33. : 100%|██████████| 32/32 [00:43<00:00,  1.35s/it]\nTrain Epoch: 106/ 512. Iter:   32/  32. LR: 0.0288. Data: 1.168s. Batch: 1.307s. Loss: 3.0952. Loss_x: 2.4203. Loss_u: 0.6749. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 107/ 512. Iter:   32/  32. LR: 0.0288. Data: 1.183s. Batch: 1.325s. Loss: 3.3239. Loss_x: 2.6352. Loss_u: 0.6887. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 108/ 512. Iter:   32/  32. LR: 0.0287. Data: 1.186s. Batch: 1.331s. Loss: 3.1870. Loss_x: 2.5135. Loss_u: 0.6736. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 109/ 512. Iter:   32/  32. LR: 0.0287. Data: 1.178s. Batch: 1.320s. Loss: 3.2303. Loss_x: 2.5470. Loss_u: 0.6833. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 110/ 512. Iter:   32/  32. LR: 0.0287. Data: 1.178s. Batch: 1.318s. Loss: 3.2017. Loss_x: 2.5136. Loss_u: 0.6882. Mask: 0.35. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 111/ 512. Iter:   32/  32. LR: 0.0287. Data: 1.184s. Batch: 1.329s. Loss: 3.1390. Loss_x: 2.4403. Loss_u: 0.6987. Mask: 0.32. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 112/ 512. Iter:   32/  32. LR: 0.0287. Data: 1.188s. Batch: 1.331s. Loss: 3.2716. Loss_x: 2.5536. Loss_u: 0.7180. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 113/ 512. Iter:   32/  32. LR: 0.0286. Data: 1.178s. Batch: 1.319s. Loss: 3.1425. Loss_x: 2.4657. Loss_u: 0.6769. Mask: 0.35. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 114/ 512. Iter:   32/  32. LR: 0.0286. Data: 1.192s. Batch: 1.337s. Loss: 3.1157. Loss_x: 2.4502. Loss_u: 0.6655. Mask: 0.31. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 115/ 512. Iter:   32/  32. LR: 0.0286. Data: 1.170s. Batch: 1.312s. Loss: 3.1877. Loss_x: 2.5491. Loss_u: 0.6386. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 116/ 512. Iter:   32/  32. LR: 0.0286. Data: 1.159s. Batch: 1.302s. Loss: 3.0232. Loss_x: 2.3400. Loss_u: 0.6832. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 117/ 512. Iter:   32/  32. LR: 0.0285. Data: 1.175s. Batch: 1.320s. Loss: 3.3325. Loss_x: 2.6818. Loss_u: 0.6507. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 118/ 512. Iter:   32/  32. LR: 0.0285. Data: 1.190s. Batch: 1.332s. Loss: 3.0172. Loss_x: 2.2574. Loss_u: 0.7599. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 119/ 512. Iter:   32/  32. LR: 0.0285. Data: 1.209s. Batch: 1.351s. Loss: 3.0573. Loss_x: 2.4376. Loss_u: 0.6196. Mask: 0.32. : 100%|██████████| 32/32 [00:43<00:00,  1.35s/it]\nTrain Epoch: 120/ 512. Iter:   32/  32. LR: 0.0285. Data: 1.159s. Batch: 1.298s. Loss: 3.1856. Loss_x: 2.5293. Loss_u: 0.6563. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 121/ 512. Iter:   32/  32. LR: 0.0284. Data: 1.145s. Batch: 1.288s. Loss: 2.9258. Loss_x: 2.2376. Loss_u: 0.6882. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 122/ 512. Iter:   32/  32. LR: 0.0284. Data: 1.152s. Batch: 1.294s. Loss: 3.0937. Loss_x: 2.3947. Loss_u: 0.6990. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 123/ 512. Iter:   32/  32. LR: 0.0284. Data: 1.178s. Batch: 1.319s. Loss: 3.4571. Loss_x: 2.7680. Loss_u: 0.6891. Mask: 0.35. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 124/ 512. Iter:   32/  32. LR: 0.0284. Data: 1.187s. Batch: 1.327s. Loss: 3.0048. Loss_x: 2.2725. Loss_u: 0.7323. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 125/ 512. Iter:   32/  32. LR: 0.0283. Data: 1.163s. Batch: 1.305s. Loss: 3.2023. Loss_x: 2.4634. Loss_u: 0.7390. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 126/ 512. Iter:   32/  32. LR: 0.0283. Data: 1.163s. Batch: 1.306s. Loss: 3.1150. Loss_x: 2.4329. Loss_u: 0.6821. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 127/ 512. Iter:   32/  32. LR: 0.0283. Data: 1.170s. Batch: 1.313s. Loss: 2.9486. Loss_x: 2.2765. Loss_u: 0.6721. Mask: 0.32. : 100%|██████████| 32/32 [00:42<00:00,  1.31s/it]\nTrain Epoch: 128/ 512. Iter:   32/  32. LR: 0.0282. Data: 1.177s. Batch: 1.321s. Loss: 3.2418. Loss_x: 2.5972. Loss_u: 0.6446. Mask: 0.32. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 129/ 512. Iter:   32/  32. LR: 0.0282. Data: 1.175s. Batch: 1.319s. Loss: 3.1387. Loss_x: 2.4500. Loss_u: 0.6887. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 130/ 512. Iter:   32/  32. LR: 0.0282. Data: 1.166s. Batch: 1.307s. Loss: 3.4227. Loss_x: 2.6916. Loss_u: 0.7310. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 131/ 512. Iter:   32/  32. LR: 0.0282. Data: 1.149s. Batch: 1.289s. Loss: 3.1060. Loss_x: 2.4173. Loss_u: 0.6887. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 132/ 512. Iter:   32/  32. LR: 0.0281. Data: 1.147s. Batch: 1.286s. Loss: 2.9040. Loss_x: 2.2447. Loss_u: 0.6593. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 133/ 512. Iter:   32/  32. LR: 0.0281. Data: 1.148s. Batch: 1.290s. Loss: 2.9254. Loss_x: 2.2597. Loss_u: 0.6657. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 134/ 512. Iter:   32/  32. LR: 0.0281. Data: 1.180s. Batch: 1.322s. Loss: 3.3740. Loss_x: 2.7160. Loss_u: 0.6580. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 135/ 512. Iter:   32/  32. LR: 0.0281. Data: 1.168s. Batch: 1.310s. Loss: 3.4023. Loss_x: 2.6815. Loss_u: 0.7208. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 136/ 512. Iter:   32/  32. LR: 0.0280. Data: 1.150s. Batch: 1.290s. Loss: 3.1691. Loss_x: 2.4467. Loss_u: 0.7225. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 137/ 512. Iter:   32/  32. LR: 0.0280. Data: 1.151s. Batch: 1.292s. Loss: 2.9942. Loss_x: 2.3242. Loss_u: 0.6700. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 138/ 512. Iter:   32/  32. LR: 0.0280. Data: 1.166s. Batch: 1.308s. Loss: 3.1987. Loss_x: 2.5878. Loss_u: 0.6109. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 139/ 512. Iter:   32/  32. LR: 0.0279. Data: 1.146s. Batch: 1.285s. Loss: 2.9989. Loss_x: 2.2894. Loss_u: 0.7096. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.28s/it]\nTrain Epoch: 140/ 512. Iter:   32/  32. LR: 0.0279. Data: 1.147s. Batch: 1.290s. Loss: 2.8691. Loss_x: 2.2212. Loss_u: 0.6480. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 141/ 512. Iter:   32/  32. LR: 0.0279. Data: 1.168s. Batch: 1.312s. Loss: 3.2967. Loss_x: 2.6863. Loss_u: 0.6103. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 142/ 512. Iter:   32/  32. LR: 0.0278. Data: 1.143s. Batch: 1.286s. Loss: 3.1132. Loss_x: 2.4209. Loss_u: 0.6923. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 143/ 512. Iter:   32/  32. LR: 0.0278. Data: 1.150s. Batch: 1.291s. Loss: 2.8232. Loss_x: 2.1866. Loss_u: 0.6365. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 144/ 512. Iter:   32/  32. LR: 0.0278. Data: 1.164s. Batch: 1.304s. Loss: 2.9923. Loss_x: 2.3427. Loss_u: 0.6496. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 145/ 512. Iter:   32/  32. LR: 0.0278. Data: 1.149s. Batch: 1.290s. Loss: 3.3342. Loss_x: 2.6714. Loss_u: 0.6628. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 146/ 512. Iter:   32/  32. LR: 0.0277. Data: 1.148s. Batch: 1.286s. Loss: 2.9788. Loss_x: 2.3508. Loss_u: 0.6280. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 147/ 512. Iter:   32/  32. LR: 0.0277. Data: 1.149s. Batch: 1.289s. Loss: 2.9316. Loss_x: 2.3693. Loss_u: 0.5623. Mask: 0.29. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 148/ 512. Iter:   32/  32. LR: 0.0277. Data: 1.150s. Batch: 1.290s. Loss: 3.2498. Loss_x: 2.5748. Loss_u: 0.6750. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 149/ 512. Iter:   32/  32. LR: 0.0276. Data: 1.133s. Batch: 1.275s. Loss: 3.1585. Loss_x: 2.4301. Loss_u: 0.7284. Mask: 0.35. : 100%|██████████| 32/32 [00:40<00:00,  1.28s/it]\nTrain Epoch: 150/ 512. Iter:   32/  32. LR: 0.0276. Data: 1.149s. Batch: 1.292s. Loss: 3.1002. Loss_x: 2.4249. Loss_u: 0.6753. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 151/ 512. Iter:   32/  32. LR: 0.0276. Data: 1.134s. Batch: 1.274s. Loss: 2.8714. Loss_x: 2.2368. Loss_u: 0.6347. Mask: 0.32. : 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\nTrain Epoch: 152/ 512. Iter:   32/  32. LR: 0.0275. Data: 1.144s. Batch: 1.285s. Loss: 2.9885. Loss_x: 2.3570. Loss_u: 0.6316. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 153/ 512. Iter:   32/  32. LR: 0.0275. Data: 1.154s. Batch: 1.297s. Loss: 2.8260. Loss_x: 2.1508. Loss_u: 0.6752. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 154/ 512. Iter:   32/  32. LR: 0.0275. Data: 1.129s. Batch: 1.269s. Loss: 2.9871. Loss_x: 2.3807. Loss_u: 0.6065. Mask: 0.32. : 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\nTrain Epoch: 155/ 512. Iter:   32/  32. LR: 0.0274. Data: 1.151s. Batch: 1.290s. Loss: 3.1005. Loss_x: 2.4513. Loss_u: 0.6492. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 156/ 512. Iter:   32/  32. LR: 0.0274. Data: 1.130s. Batch: 1.271s. Loss: 2.9895. Loss_x: 2.3243. Loss_u: 0.6653. Mask: 0.33. : 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\nTrain Epoch: 157/ 512. Iter:   32/  32. LR: 0.0274. Data: 1.152s. Batch: 1.290s. Loss: 2.9379. Loss_x: 2.3188. Loss_u: 0.6191. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 158/ 512. Iter:   32/  32. LR: 0.0273. Data: 1.144s. Batch: 1.283s. Loss: 2.9451. Loss_x: 2.3148. Loss_u: 0.6303. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.28s/it]\nTrain Epoch: 159/ 512. Iter:   32/  32. LR: 0.0273. Data: 1.144s. Batch: 1.284s. Loss: 3.0499. Loss_x: 2.4087. Loss_u: 0.6412. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.28s/it]\nTrain Epoch: 160/ 512. Iter:   32/  32. LR: 0.0273. Data: 1.156s. Batch: 1.295s. Loss: 2.9403. Loss_x: 2.2675. Loss_u: 0.6729. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 161/ 512. Iter:   32/  32. LR: 0.0272. Data: 1.143s. Batch: 1.286s. Loss: 2.8597. Loss_x: 2.1784. Loss_u: 0.6813. Mask: 0.33. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 162/ 512. Iter:   32/  32. LR: 0.0272. Data: 1.147s. Batch: 1.292s. Loss: 2.9509. Loss_x: 2.3300. Loss_u: 0.6209. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 163/ 512. Iter:   32/  32. LR: 0.0272. Data: 1.159s. Batch: 1.299s. Loss: 2.7714. Loss_x: 2.1935. Loss_u: 0.5779. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 164/ 512. Iter:   32/  32. LR: 0.0271. Data: 1.153s. Batch: 1.298s. Loss: 3.2619. Loss_x: 2.6019. Loss_u: 0.6600. Mask: 0.34. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 165/ 512. Iter:   32/  32. LR: 0.0271. Data: 1.181s. Batch: 1.322s. Loss: 2.8114. Loss_x: 2.2064. Loss_u: 0.6051. Mask: 0.31. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 166/ 512. Iter:   32/  32. LR: 0.0271. Data: 1.196s. Batch: 1.338s. Loss: 3.1204. Loss_x: 2.4821. Loss_u: 0.6383. Mask: 0.32. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 167/ 512. Iter:   32/  32. LR: 0.0270. Data: 1.195s. Batch: 1.335s. Loss: 3.0488. Loss_x: 2.3890. Loss_u: 0.6598. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 168/ 512. Iter:   32/  32. LR: 0.0270. Data: 1.202s. Batch: 1.343s. Loss: 2.9771. Loss_x: 2.3314. Loss_u: 0.6457. Mask: 0.30. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 169/ 512. Iter:   32/  32. LR: 0.0270. Data: 1.207s. Batch: 1.350s. Loss: 2.9439. Loss_x: 2.3313. Loss_u: 0.6125. Mask: 0.31. : 100%|██████████| 32/32 [00:43<00:00,  1.35s/it]\nTrain Epoch: 170/ 512. Iter:   32/  32. LR: 0.0269. Data: 1.169s. Batch: 1.311s. Loss: 2.7652. Loss_x: 2.1551. Loss_u: 0.6101. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 171/ 512. Iter:   32/  32. LR: 0.0269. Data: 1.180s. Batch: 1.318s. Loss: 3.0093. Loss_x: 2.3335. Loss_u: 0.6758. Mask: 0.34. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 172/ 512. Iter:   32/  32. LR: 0.0269. Data: 1.190s. Batch: 1.328s. Loss: 3.0394. Loss_x: 2.3458. Loss_u: 0.6937. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 173/ 512. Iter:   32/  32. LR: 0.0268. Data: 1.175s. Batch: 1.317s. Loss: 2.9165. Loss_x: 2.3138. Loss_u: 0.6027. Mask: 0.30. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 174/ 512. Iter:   32/  32. LR: 0.0268. Data: 1.195s. Batch: 1.340s. Loss: 2.8584. Loss_x: 2.2533. Loss_u: 0.6051. Mask: 0.30. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 175/ 512. Iter:   32/  32. LR: 0.0268. Data: 1.191s. Batch: 1.333s. Loss: 2.9692. Loss_x: 2.3537. Loss_u: 0.6155. Mask: 0.32. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 176/ 512. Iter:   32/  32. LR: 0.0267. Data: 1.173s. Batch: 1.312s. Loss: 2.9650. Loss_x: 2.3543. Loss_u: 0.6106. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 177/ 512. Iter:   32/  32. LR: 0.0267. Data: 1.183s. Batch: 1.324s. Loss: 3.2087. Loss_x: 2.5446. Loss_u: 0.6641. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 178/ 512. Iter:   32/  32. LR: 0.0266. Data: 1.166s. Batch: 1.306s. Loss: 3.1563. Loss_x: 2.4769. Loss_u: 0.6794. Mask: 0.35. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 179/ 512. Iter:   32/  32. LR: 0.0266. Data: 1.177s. Batch: 1.318s. Loss: 3.1865. Loss_x: 2.4999. Loss_u: 0.6865. Mask: 0.33. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 180/ 512. Iter:   32/  32. LR: 0.0266. Data: 1.233s. Batch: 1.373s. Loss: 2.8492. Loss_x: 2.2780. Loss_u: 0.5712. Mask: 0.30. : 100%|██████████| 32/32 [00:43<00:00,  1.37s/it]\nTrain Epoch: 181/ 512. Iter:   32/  32. LR: 0.0265. Data: 1.213s. Batch: 1.359s. Loss: 2.9562. Loss_x: 2.3329. Loss_u: 0.6233. Mask: 0.32. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 182/ 512. Iter:   32/  32. LR: 0.0265. Data: 1.216s. Batch: 1.360s. Loss: 2.9782. Loss_x: 2.3391. Loss_u: 0.6391. Mask: 0.31. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 183/ 512. Iter:   32/  32. LR: 0.0265. Data: 1.166s. Batch: 1.308s. Loss: 2.9151. Loss_x: 2.3476. Loss_u: 0.5675. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 184/ 512. Iter:   32/  32. LR: 0.0264. Data: 1.196s. Batch: 1.338s. Loss: 2.7396. Loss_x: 2.1689. Loss_u: 0.5707. Mask: 0.29. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 185/ 512. Iter:   32/  32. LR: 0.0264. Data: 1.144s. Batch: 1.286s. Loss: 2.8741. Loss_x: 2.2883. Loss_u: 0.5858. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 186/ 512. Iter:   32/  32. LR: 0.0263. Data: 1.232s. Batch: 1.374s. Loss: 3.0338. Loss_x: 2.3833. Loss_u: 0.6505. Mask: 0.32. : 100%|██████████| 32/32 [00:43<00:00,  1.37s/it]\nTrain Epoch: 187/ 512. Iter:   32/  32. LR: 0.0263. Data: 1.158s. Batch: 1.300s. Loss: 2.8481. Loss_x: 2.2294. Loss_u: 0.6187. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 188/ 512. Iter:   32/  32. LR: 0.0263. Data: 1.159s. Batch: 1.297s. Loss: 2.8931. Loss_x: 2.2958. Loss_u: 0.5973. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 189/ 512. Iter:   32/  32. LR: 0.0262. Data: 1.191s. Batch: 1.334s. Loss: 2.8959. Loss_x: 2.2735. Loss_u: 0.6225. Mask: 0.31. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 190/ 512. Iter:   32/  32. LR: 0.0262. Data: 1.169s. Batch: 1.309s. Loss: 2.6729. Loss_x: 2.0894. Loss_u: 0.5835. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 191/ 512. Iter:   32/  32. LR: 0.0261. Data: 1.153s. Batch: 1.294s. Loss: 2.7537. Loss_x: 2.1677. Loss_u: 0.5860. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 192/ 512. Iter:   32/  32. LR: 0.0261. Data: 1.173s. Batch: 1.313s. Loss: 3.0056. Loss_x: 2.3849. Loss_u: 0.6207. Mask: 0.32. : 100%|██████████| 32/32 [00:42<00:00,  1.31s/it]\nTrain Epoch: 193/ 512. Iter:   32/  32. LR: 0.0261. Data: 1.159s. Batch: 1.299s. Loss: 2.9194. Loss_x: 2.2688. Loss_u: 0.6506. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 194/ 512. Iter:   32/  32. LR: 0.0260. Data: 1.149s. Batch: 1.289s. Loss: 2.8593. Loss_x: 2.2633. Loss_u: 0.5959. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 195/ 512. Iter:   32/  32. LR: 0.0260. Data: 1.167s. Batch: 1.309s. Loss: 2.9213. Loss_x: 2.3619. Loss_u: 0.5594. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 196/ 512. Iter:   32/  32. LR: 0.0259. Data: 1.147s. Batch: 1.288s. Loss: 2.8588. Loss_x: 2.2974. Loss_u: 0.5614. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 197/ 512. Iter:   32/  32. LR: 0.0259. Data: 1.143s. Batch: 1.286s. Loss: 2.9115. Loss_x: 2.2910. Loss_u: 0.6205. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 198/ 512. Iter:   32/  32. LR: 0.0259. Data: 1.161s. Batch: 1.302s. Loss: 2.8785. Loss_x: 2.2844. Loss_u: 0.5942. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 199/ 512. Iter:   32/  32. LR: 0.0258. Data: 1.132s. Batch: 1.274s. Loss: 3.0013. Loss_x: 2.4348. Loss_u: 0.5665. Mask: 0.30. : 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\nTrain Epoch: 200/ 512. Iter:   32/  32. LR: 0.0258. Data: 1.154s. Batch: 1.293s. Loss: 2.8857. Loss_x: 2.2467. Loss_u: 0.6390. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 201/ 512. Iter:   32/  32. LR: 0.0257. Data: 1.178s. Batch: 1.320s. Loss: 2.8378. Loss_x: 2.2156. Loss_u: 0.6222. Mask: 0.31. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 202/ 512. Iter:   32/  32. LR: 0.0257. Data: 1.149s. Batch: 1.291s. Loss: 2.8442. Loss_x: 2.2033. Loss_u: 0.6409. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 203/ 512. Iter:   32/  32. LR: 0.0257. Data: 1.151s. Batch: 1.294s. Loss: 2.8144. Loss_x: 2.2220. Loss_u: 0.5924. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 204/ 512. Iter:   32/  32. LR: 0.0256. Data: 1.166s. Batch: 1.307s. Loss: 3.0397. Loss_x: 2.4097. Loss_u: 0.6300. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 205/ 512. Iter:   32/  32. LR: 0.0256. Data: 1.153s. Batch: 1.294s. Loss: 2.9532. Loss_x: 2.3675. Loss_u: 0.5857. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 206/ 512. Iter:   32/  32. LR: 0.0255. Data: 1.141s. Batch: 1.283s. Loss: 2.7819. Loss_x: 2.2101. Loss_u: 0.5719. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.28s/it]\nTrain Epoch: 207/ 512. Iter:   32/  32. LR: 0.0255. Data: 1.159s. Batch: 1.300s. Loss: 2.7753. Loss_x: 2.1720. Loss_u: 0.6034. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 208/ 512. Iter:   32/  32. LR: 0.0254. Data: 1.135s. Batch: 1.274s. Loss: 2.7786. Loss_x: 2.1918. Loss_u: 0.5868. Mask: 0.31. : 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\nTrain Epoch: 209/ 512. Iter:   32/  32. LR: 0.0254. Data: 1.155s. Batch: 1.297s. Loss: 2.6300. Loss_x: 2.0324. Loss_u: 0.5976. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 210/ 512. Iter:   32/  32. LR: 0.0254. Data: 1.149s. Batch: 1.289s. Loss: 2.6650. Loss_x: 2.1009. Loss_u: 0.5641. Mask: 0.29. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 211/ 512. Iter:   32/  32. LR: 0.0253. Data: 1.148s. Batch: 1.288s. Loss: 2.8227. Loss_x: 2.2463. Loss_u: 0.5764. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 212/ 512. Iter:   32/  32. LR: 0.0253. Data: 1.150s. Batch: 1.292s. Loss: 2.7931. Loss_x: 2.2208. Loss_u: 0.5724. Mask: 0.28. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 213/ 512. Iter:   32/  32. LR: 0.0252. Data: 1.168s. Batch: 1.311s. Loss: 2.8287. Loss_x: 2.2234. Loss_u: 0.6053. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 214/ 512. Iter:   32/  32. LR: 0.0252. Data: 1.164s. Batch: 1.304s. Loss: 2.8145. Loss_x: 2.1872. Loss_u: 0.6273. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 215/ 512. Iter:   32/  32. LR: 0.0251. Data: 1.172s. Batch: 1.312s. Loss: 2.8227. Loss_x: 2.2562. Loss_u: 0.5665. Mask: 0.29. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 216/ 512. Iter:   32/  32. LR: 0.0251. Data: 1.183s. Batch: 1.325s. Loss: 2.5323. Loss_x: 2.0097. Loss_u: 0.5226. Mask: 0.28. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 217/ 512. Iter:   32/  32. LR: 0.0251. Data: 1.166s. Batch: 1.308s. Loss: 2.8881. Loss_x: 2.3554. Loss_u: 0.5327. Mask: 0.29. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 218/ 512. Iter:   32/  32. LR: 0.0250. Data: 1.159s. Batch: 1.298s. Loss: 2.9317. Loss_x: 2.3698. Loss_u: 0.5618. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 219/ 512. Iter:   32/  32. LR: 0.0250. Data: 1.184s. Batch: 1.325s. Loss: 2.9757. Loss_x: 2.3771. Loss_u: 0.5986. Mask: 0.32. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 220/ 512. Iter:   32/  32. LR: 0.0249. Data: 1.164s. Batch: 1.303s. Loss: 2.9225. Loss_x: 2.3698. Loss_u: 0.5527. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 221/ 512. Iter:   32/  32. LR: 0.0249. Data: 1.152s. Batch: 1.297s. Loss: 2.8857. Loss_x: 2.2958. Loss_u: 0.5898. Mask: 0.32. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 222/ 512. Iter:   32/  32. LR: 0.0248. Data: 1.167s. Batch: 1.309s. Loss: 2.8170. Loss_x: 2.2951. Loss_u: 0.5219. Mask: 0.28. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 223/ 512. Iter:   32/  32. LR: 0.0248. Data: 1.158s. Batch: 1.298s. Loss: 2.8059. Loss_x: 2.2534. Loss_u: 0.5525. Mask: 0.30. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 224/ 512. Iter:   32/  32. LR: 0.0247. Data: 1.164s. Batch: 1.305s. Loss: 2.9171. Loss_x: 2.3790. Loss_u: 0.5381. Mask: 0.31. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 225/ 512. Iter:   32/  32. LR: 0.0247. Data: 1.183s. Batch: 1.323s. Loss: 2.8028. Loss_x: 2.2803. Loss_u: 0.5225. Mask: 0.29. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 226/ 512. Iter:   32/  32. LR: 0.0246. Data: 1.175s. Batch: 1.316s. Loss: 2.6297. Loss_x: 2.0524. Loss_u: 0.5773. Mask: 0.28. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 227/ 512. Iter:   32/  32. LR: 0.0246. Data: 1.194s. Batch: 1.334s. Loss: 2.6292. Loss_x: 2.0821. Loss_u: 0.5471. Mask: 0.28. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 228/ 512. Iter:   32/  32. LR: 0.0246. Data: 1.182s. Batch: 1.326s. Loss: 2.6740. Loss_x: 2.1259. Loss_u: 0.5481. Mask: 0.28. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 229/ 512. Iter:   32/  32. LR: 0.0245. Data: 1.177s. Batch: 1.317s. Loss: 2.5585. Loss_x: 2.0604. Loss_u: 0.4981. Mask: 0.26. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 230/ 512. Iter:   32/  32. LR: 0.0245. Data: 1.187s. Batch: 1.332s. Loss: 2.7772. Loss_x: 2.2502. Loss_u: 0.5270. Mask: 0.28. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 231/ 512. Iter:   32/  32. LR: 0.0244. Data: 1.209s. Batch: 1.354s. Loss: 2.8520. Loss_x: 2.3262. Loss_u: 0.5258. Mask: 0.28. : 100%|██████████| 32/32 [00:43<00:00,  1.35s/it]\nTrain Epoch: 232/ 512. Iter:   32/  32. LR: 0.0244. Data: 1.217s. Batch: 1.356s. Loss: 2.5866. Loss_x: 2.0380. Loss_u: 0.5486. Mask: 0.28. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 233/ 512. Iter:   32/  32. LR: 0.0243. Data: 1.213s. Batch: 1.356s. Loss: 3.0069. Loss_x: 2.4469. Loss_u: 0.5600. Mask: 0.31. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 234/ 512. Iter:   32/  32. LR: 0.0243. Data: 1.220s. Batch: 1.361s. Loss: 2.7412. Loss_x: 2.2269. Loss_u: 0.5144. Mask: 0.27. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 235/ 512. Iter:   32/  32. LR: 0.0242. Data: 1.207s. Batch: 1.346s. Loss: 2.6742. Loss_x: 2.0920. Loss_u: 0.5822. Mask: 0.30. : 100%|██████████| 32/32 [00:43<00:00,  1.35s/it]\nTrain Epoch: 236/ 512. Iter:   32/  32. LR: 0.0242. Data: 1.228s. Batch: 1.370s. Loss: 2.6200. Loss_x: 2.1243. Loss_u: 0.4957. Mask: 0.26. : 100%|██████████| 32/32 [00:43<00:00,  1.37s/it]\nTrain Epoch: 237/ 512. Iter:   32/  32. LR: 0.0241. Data: 1.223s. Batch: 1.365s. Loss: 2.6836. Loss_x: 2.1481. Loss_u: 0.5355. Mask: 0.28. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 238/ 512. Iter:   32/  32. LR: 0.0241. Data: 1.222s. Batch: 1.365s. Loss: 2.6318. Loss_x: 2.1191. Loss_u: 0.5127. Mask: 0.28. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 239/ 512. Iter:   32/  32. LR: 0.0240. Data: 1.256s. Batch: 1.398s. Loss: 2.7197. Loss_x: 2.1942. Loss_u: 0.5255. Mask: 0.28. : 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\nTrain Epoch: 240/ 512. Iter:   32/  32. LR: 0.0240. Data: 1.222s. Batch: 1.361s. Loss: 2.8149. Loss_x: 2.2675. Loss_u: 0.5474. Mask: 0.29. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 241/ 512. Iter:   32/  32. LR: 0.0239. Data: 1.249s. Batch: 1.391s. Loss: 2.7937. Loss_x: 2.2119. Loss_u: 0.5818. Mask: 0.29. : 100%|██████████| 32/32 [00:44<00:00,  1.39s/it]\nTrain Epoch: 242/ 512. Iter:   32/  32. LR: 0.0239. Data: 1.201s. Batch: 1.343s. Loss: 2.7432. Loss_x: 2.2202. Loss_u: 0.5229. Mask: 0.28. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 243/ 512. Iter:   32/  32. LR: 0.0238. Data: 1.182s. Batch: 1.329s. Loss: 2.7477. Loss_x: 2.2422. Loss_u: 0.5055. Mask: 0.30. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 244/ 512. Iter:   32/  32. LR: 0.0238. Data: 1.197s. Batch: 1.340s. Loss: 2.8084. Loss_x: 2.2515. Loss_u: 0.5569. Mask: 0.31. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 245/ 512. Iter:   32/  32. LR: 0.0237. Data: 1.145s. Batch: 1.284s. Loss: 2.4972. Loss_x: 1.9370. Loss_u: 0.5602. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.28s/it]\nTrain Epoch: 246/ 512. Iter:   32/  32. LR: 0.0237. Data: 1.151s. Batch: 1.290s. Loss: 2.8106. Loss_x: 2.3260. Loss_u: 0.4846. Mask: 0.28. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 247/ 512. Iter:   32/  32. LR: 0.0236. Data: 1.183s. Batch: 1.323s. Loss: 2.7417. Loss_x: 2.2073. Loss_u: 0.5344. Mask: 0.28. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 248/ 512. Iter:   32/  32. LR: 0.0236. Data: 1.161s. Batch: 1.303s. Loss: 2.7827. Loss_x: 2.2296. Loss_u: 0.5531. Mask: 0.29. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 249/ 512. Iter:   32/  32. LR: 0.0235. Data: 1.151s. Batch: 1.294s. Loss: 2.5060. Loss_x: 2.0541. Loss_u: 0.4519. Mask: 0.25. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 250/ 512. Iter:   32/  32. LR: 0.0235. Data: 1.158s. Batch: 1.301s. Loss: 2.5023. Loss_x: 2.0411. Loss_u: 0.4611. Mask: 0.26. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 251/ 512. Iter:   32/  32. LR: 0.0234. Data: 1.149s. Batch: 1.290s. Loss: 2.6112. Loss_x: 2.0782. Loss_u: 0.5330. Mask: 0.28. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 252/ 512. Iter:   32/  32. LR: 0.0234. Data: 1.146s. Batch: 1.286s. Loss: 2.4978. Loss_x: 2.0111. Loss_u: 0.4867. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 253/ 512. Iter:   32/  32. LR: 0.0233. Data: 1.154s. Batch: 1.294s. Loss: 2.5625. Loss_x: 2.0735. Loss_u: 0.4890. Mask: 0.26. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 254/ 512. Iter:   32/  32. LR: 0.0233. Data: 1.127s. Batch: 1.267s. Loss: 2.7090. Loss_x: 2.1825. Loss_u: 0.5265. Mask: 0.27. : 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\nTrain Epoch: 255/ 512. Iter:   32/  32. LR: 0.0232. Data: 1.147s. Batch: 1.289s. Loss: 2.6785. Loss_x: 2.1531. Loss_u: 0.5254. Mask: 0.29. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 256/ 512. Iter:   32/  32. LR: 0.0232. Data: 1.155s. Batch: 1.295s. Loss: 2.3915. Loss_x: 1.9497. Loss_u: 0.4419. Mask: 0.25. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 257/ 512. Iter:   32/  32. LR: 0.0231. Data: 1.161s. Batch: 1.304s. Loss: 2.6440. Loss_x: 2.1631. Loss_u: 0.4809. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 258/ 512. Iter:   32/  32. LR: 0.0231. Data: 1.171s. Batch: 1.315s. Loss: 2.4600. Loss_x: 2.0008. Loss_u: 0.4591. Mask: 0.26. : 100%|██████████| 32/32 [00:42<00:00,  1.31s/it]\nTrain Epoch: 259/ 512. Iter:   32/  32. LR: 0.0230. Data: 1.161s. Batch: 1.306s. Loss: 2.4566. Loss_x: 1.9787. Loss_u: 0.4779. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 260/ 512. Iter:   32/  32. LR: 0.0230. Data: 1.160s. Batch: 1.304s. Loss: 2.6262. Loss_x: 2.1305. Loss_u: 0.4957. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 261/ 512. Iter:   32/  32. LR: 0.0229. Data: 1.157s. Batch: 1.298s. Loss: 2.5594. Loss_x: 2.0851. Loss_u: 0.4743. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 262/ 512. Iter:   32/  32. LR: 0.0229. Data: 1.144s. Batch: 1.289s. Loss: 2.5377. Loss_x: 2.0600. Loss_u: 0.4777. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 263/ 512. Iter:   32/  32. LR: 0.0228. Data: 1.173s. Batch: 1.312s. Loss: 2.5491. Loss_x: 2.0858. Loss_u: 0.4633. Mask: 0.26. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 264/ 512. Iter:   32/  32. LR: 0.0228. Data: 1.157s. Batch: 1.299s. Loss: 2.5472. Loss_x: 2.0776. Loss_u: 0.4696. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 265/ 512. Iter:   32/  32. LR: 0.0227. Data: 1.137s. Batch: 1.277s. Loss: 2.6334. Loss_x: 2.1302. Loss_u: 0.5031. Mask: 0.28. : 100%|██████████| 32/32 [00:40<00:00,  1.28s/it]\nTrain Epoch: 266/ 512. Iter:   32/  32. LR: 0.0227. Data: 1.164s. Batch: 1.304s. Loss: 2.7546. Loss_x: 2.2603. Loss_u: 0.4943. Mask: 0.28. : 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\nTrain Epoch: 267/ 512. Iter:   32/  32. LR: 0.0226. Data: 1.151s. Batch: 1.294s. Loss: 2.6382. Loss_x: 2.1178. Loss_u: 0.5204. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 268/ 512. Iter:   32/  32. LR: 0.0226. Data: 1.143s. Batch: 1.282s. Loss: 2.5699. Loss_x: 2.1240. Loss_u: 0.4459. Mask: 0.25. : 100%|██████████| 32/32 [00:41<00:00,  1.28s/it]\nTrain Epoch: 269/ 512. Iter:   32/  32. LR: 0.0225. Data: 1.169s. Batch: 1.311s. Loss: 2.4987. Loss_x: 2.0336. Loss_u: 0.4651. Mask: 0.25. : 100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\nTrain Epoch: 270/ 512. Iter:   32/  32. LR: 0.0225. Data: 1.153s. Batch: 1.295s. Loss: 2.5333. Loss_x: 2.0404. Loss_u: 0.4929. Mask: 0.27. : 100%|██████████| 32/32 [00:41<00:00,  1.29s/it]\nTrain Epoch: 271/ 512. Iter:   32/  32. LR: 0.0224. Data: 1.223s. Batch: 1.363s. Loss: 2.4456. Loss_x: 1.9810. Loss_u: 0.4645. Mask: 0.26. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 272/ 512. Iter:   32/  32. LR: 0.0224. Data: 1.269s. Batch: 1.410s. Loss: 2.3466. Loss_x: 1.8939. Loss_u: 0.4528. Mask: 0.24. : 100%|██████████| 32/32 [00:45<00:00,  1.41s/it]\nTrain Epoch: 273/ 512. Iter:   32/  32. LR: 0.0223. Data: 1.257s. Batch: 1.400s. Loss: 2.5796. Loss_x: 2.1246. Loss_u: 0.4550. Mask: 0.26. : 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\nTrain Epoch: 274/ 512. Iter:   32/  32. LR: 0.0222. Data: 1.299s. Batch: 1.446s. Loss: 2.5266. Loss_x: 2.0310. Loss_u: 0.4956. Mask: 0.27. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 275/ 512. Iter:   32/  32. LR: 0.0222. Data: 1.284s. Batch: 1.425s. Loss: 2.4437. Loss_x: 1.9803. Loss_u: 0.4634. Mask: 0.27. : 100%|██████████| 32/32 [00:45<00:00,  1.42s/it]\nTrain Epoch: 276/ 512. Iter:   32/  32. LR: 0.0221. Data: 1.299s. Batch: 1.442s. Loss: 2.5914. Loss_x: 2.0950. Loss_u: 0.4964. Mask: 0.27. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 277/ 512. Iter:   32/  32. LR: 0.0221. Data: 1.285s. Batch: 1.424s. Loss: 2.5031. Loss_x: 2.0689. Loss_u: 0.4342. Mask: 0.26. : 100%|██████████| 32/32 [00:45<00:00,  1.42s/it]\nTrain Epoch: 278/ 512. Iter:   32/  32. LR: 0.0220. Data: 1.283s. Batch: 1.429s. Loss: 2.7170. Loss_x: 2.2180. Loss_u: 0.4990. Mask: 0.28. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 279/ 512. Iter:   32/  32. LR: 0.0220. Data: 1.195s. Batch: 1.333s. Loss: 2.4248. Loss_x: 1.9758. Loss_u: 0.4490. Mask: 0.25. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 280/ 512. Iter:   32/  32. LR: 0.0219. Data: 1.199s. Batch: 1.343s. Loss: 2.5008. Loss_x: 2.0549. Loss_u: 0.4460. Mask: 0.26. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 281/ 512. Iter:   32/  32. LR: 0.0219. Data: 1.202s. Batch: 1.344s. Loss: 2.3154. Loss_x: 1.8506. Loss_u: 0.4648. Mask: 0.25. : 100%|██████████| 32/32 [00:43<00:00,  1.34s/it]\nTrain Epoch: 282/ 512. Iter:   32/  32. LR: 0.0218. Data: 1.183s. Batch: 1.325s. Loss: 2.4341. Loss_x: 1.9912. Loss_u: 0.4428. Mask: 0.26. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 283/ 512. Iter:   32/  32. LR: 0.0218. Data: 1.184s. Batch: 1.325s. Loss: 2.7107. Loss_x: 2.2312. Loss_u: 0.4794. Mask: 0.27. : 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\nTrain Epoch: 284/ 512. Iter:   32/  32. LR: 0.0217. Data: 1.200s. Batch: 1.339s. Loss: 2.5358. Loss_x: 2.0561. Loss_u: 0.4797. Mask: 0.27. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 285/ 512. Iter:   32/  32. LR: 0.0216. Data: 1.180s. Batch: 1.321s. Loss: 2.3210. Loss_x: 1.8673. Loss_u: 0.4537. Mask: 0.25. : 100%|██████████| 32/32 [00:42<00:00,  1.32s/it]\nTrain Epoch: 286/ 512. Iter:   32/  32. LR: 0.0216. Data: 1.196s. Batch: 1.336s. Loss: 2.4433. Loss_x: 2.0359. Loss_u: 0.4074. Mask: 0.24. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 287/ 512. Iter:   32/  32. LR: 0.0215. Data: 1.262s. Batch: 1.404s. Loss: 2.4434. Loss_x: 2.0155. Loss_u: 0.4279. Mask: 0.26. : 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\nTrain Epoch: 288/ 512. Iter:   32/  32. LR: 0.0215. Data: 1.259s. Batch: 1.400s. Loss: 2.3053. Loss_x: 1.8757. Loss_u: 0.4296. Mask: 0.25. : 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\nTrain Epoch: 289/ 512. Iter:   32/  32. LR: 0.0214. Data: 1.320s. Batch: 1.466s. Loss: 2.3702. Loss_x: 1.9518. Loss_u: 0.4184. Mask: 0.25. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 290/ 512. Iter:   32/  32. LR: 0.0214. Data: 1.284s. Batch: 1.428s. Loss: 2.3695. Loss_x: 1.9260. Loss_u: 0.4434. Mask: 0.25. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 291/ 512. Iter:   32/  32. LR: 0.0213. Data: 1.299s. Batch: 1.439s. Loss: 2.3994. Loss_x: 2.0012. Loss_u: 0.3981. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 292/ 512. Iter:   32/  32. LR: 0.0212. Data: 1.300s. Batch: 1.443s. Loss: 2.3940. Loss_x: 1.9506. Loss_u: 0.4434. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 293/ 512. Iter:   32/  32. LR: 0.0212. Data: 1.311s. Batch: 1.456s. Loss: 2.3814. Loss_x: 1.9345. Loss_u: 0.4470. Mask: 0.25. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 294/ 512. Iter:   32/  32. LR: 0.0211. Data: 1.289s. Batch: 1.440s. Loss: 2.4208. Loss_x: 1.9834. Loss_u: 0.4374. Mask: 0.25. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 295/ 512. Iter:   32/  32. LR: 0.0211. Data: 1.285s. Batch: 1.429s. Loss: 2.3603. Loss_x: 1.9309. Loss_u: 0.4293. Mask: 0.24. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 296/ 512. Iter:   32/  32. LR: 0.0210. Data: 1.316s. Batch: 1.458s. Loss: 2.4239. Loss_x: 1.9877. Loss_u: 0.4363. Mask: 0.25. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 297/ 512. Iter:   32/  32. LR: 0.0210. Data: 1.301s. Batch: 1.445s. Loss: 2.3389. Loss_x: 1.9089. Loss_u: 0.4301. Mask: 0.25. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 298/ 512. Iter:   32/  32. LR: 0.0209. Data: 1.301s. Batch: 1.444s. Loss: 2.2993. Loss_x: 1.9099. Loss_u: 0.3894. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 299/ 512. Iter:   32/  32. LR: 0.0208. Data: 1.278s. Batch: 1.420s. Loss: 2.5530. Loss_x: 2.1009. Loss_u: 0.4520. Mask: 0.27. : 100%|██████████| 32/32 [00:45<00:00,  1.42s/it]\nTrain Epoch: 300/ 512. Iter:   32/  32. LR: 0.0208. Data: 1.310s. Batch: 1.455s. Loss: 2.4940. Loss_x: 2.0365. Loss_u: 0.4575. Mask: 0.25. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 301/ 512. Iter:   32/  32. LR: 0.0207. Data: 1.300s. Batch: 1.441s. Loss: 2.5320. Loss_x: 2.0889. Loss_u: 0.4431. Mask: 0.26. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 302/ 512. Iter:   32/  32. LR: 0.0207. Data: 1.355s. Batch: 1.501s. Loss: 2.3152. Loss_x: 1.9059. Loss_u: 0.4093. Mask: 0.24. : 100%|██████████| 32/32 [00:48<00:00,  1.50s/it]\nTrain Epoch: 303/ 512. Iter:   32/  32. LR: 0.0206. Data: 1.310s. Batch: 1.455s. Loss: 2.2290. Loss_x: 1.8362. Loss_u: 0.3928. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 304/ 512. Iter:   32/  32. LR: 0.0206. Data: 1.325s. Batch: 1.471s. Loss: 2.1920. Loss_x: 1.7637. Loss_u: 0.4283. Mask: 0.24. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 305/ 512. Iter:   32/  32. LR: 0.0205. Data: 1.338s. Batch: 1.482s. Loss: 2.2612. Loss_x: 1.8914. Loss_u: 0.3698. Mask: 0.22. : 100%|██████████| 32/32 [00:47<00:00,  1.48s/it]\nTrain Epoch: 306/ 512. Iter:   32/  32. LR: 0.0204. Data: 1.332s. Batch: 1.473s. Loss: 2.3281. Loss_x: 1.9149. Loss_u: 0.4132. Mask: 0.25. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 307/ 512. Iter:   32/  32. LR: 0.0204. Data: 1.311s. Batch: 1.453s. Loss: 2.3649. Loss_x: 1.9666. Loss_u: 0.3983. Mask: 0.24. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 308/ 512. Iter:   32/  32. LR: 0.0203. Data: 1.284s. Batch: 1.430s. Loss: 2.5782. Loss_x: 2.1232. Loss_u: 0.4550. Mask: 0.27. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 309/ 512. Iter:   32/  32. LR: 0.0203. Data: 1.200s. Batch: 1.342s. Loss: 2.3704. Loss_x: 1.9563. Loss_u: 0.4140. Mask: 0.25. : 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\nTrain Epoch: 310/ 512. Iter:   32/  32. LR: 0.0202. Data: 1.217s. Batch: 1.359s. Loss: 2.3607. Loss_x: 1.9458. Loss_u: 0.4148. Mask: 0.25. : 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\nTrain Epoch: 311/ 512. Iter:   32/  32. LR: 0.0201. Data: 1.206s. Batch: 1.347s. Loss: 2.3750. Loss_x: 1.9709. Loss_u: 0.4042. Mask: 0.24. : 100%|██████████| 32/32 [00:43<00:00,  1.35s/it]\nTrain Epoch: 312/ 512. Iter:   32/  32. LR: 0.0201. Data: 1.259s. Batch: 1.401s. Loss: 2.2905. Loss_x: 1.8657. Loss_u: 0.4248. Mask: 0.25. : 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\nTrain Epoch: 313/ 512. Iter:   32/  32. LR: 0.0200. Data: 1.318s. Batch: 1.461s. Loss: 2.2747. Loss_x: 1.8800. Loss_u: 0.3946. Mask: 0.24. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 314/ 512. Iter:   32/  32. LR: 0.0200. Data: 1.331s. Batch: 1.474s. Loss: 2.2214. Loss_x: 1.8445. Loss_u: 0.3769. Mask: 0.23. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 315/ 512. Iter:   32/  32. LR: 0.0199. Data: 1.334s. Batch: 1.482s. Loss: 2.1269. Loss_x: 1.7239. Loss_u: 0.4031. Mask: 0.25. : 100%|██████████| 32/32 [00:47<00:00,  1.48s/it]\nTrain Epoch: 316/ 512. Iter:   32/  32. LR: 0.0198. Data: 1.313s. Batch: 1.455s. Loss: 2.2379. Loss_x: 1.8511. Loss_u: 0.3868. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 317/ 512. Iter:   32/  32. LR: 0.0198. Data: 1.330s. Batch: 1.471s. Loss: 2.3228. Loss_x: 1.9146. Loss_u: 0.4082. Mask: 0.24. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 318/ 512. Iter:   32/  32. LR: 0.0197. Data: 1.310s. Batch: 1.450s. Loss: 2.2321. Loss_x: 1.8459. Loss_u: 0.3862. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 319/ 512. Iter:   32/  32. LR: 0.0197. Data: 1.311s. Batch: 1.458s. Loss: 2.2209. Loss_x: 1.8526. Loss_u: 0.3683. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 320/ 512. Iter:   32/  32. LR: 0.0196. Data: 1.295s. Batch: 1.446s. Loss: 2.1871. Loss_x: 1.7893. Loss_u: 0.3978. Mask: 0.24. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 321/ 512. Iter:   32/  32. LR: 0.0195. Data: 1.303s. Batch: 1.449s. Loss: 2.1812. Loss_x: 1.8015. Loss_u: 0.3797. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 322/ 512. Iter:   32/  32. LR: 0.0195. Data: 1.287s. Batch: 1.432s. Loss: 2.2775. Loss_x: 1.8994. Loss_u: 0.3781. Mask: 0.23. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 323/ 512. Iter:   32/  32. LR: 0.0194. Data: 1.288s. Batch: 1.430s. Loss: 2.1499. Loss_x: 1.7538. Loss_u: 0.3961. Mask: 0.22. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 324/ 512. Iter:   32/  32. LR: 0.0194. Data: 1.309s. Batch: 1.451s. Loss: 2.4550. Loss_x: 2.0693. Loss_u: 0.3856. Mask: 0.24. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 325/ 512. Iter:   32/  32. LR: 0.0193. Data: 1.302s. Batch: 1.442s. Loss: 2.2265. Loss_x: 1.8387. Loss_u: 0.3878. Mask: 0.25. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 326/ 512. Iter:   32/  32. LR: 0.0192. Data: 1.316s. Batch: 1.457s. Loss: 2.2424. Loss_x: 1.8750. Loss_u: 0.3674. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 327/ 512. Iter:   32/  32. LR: 0.0192. Data: 1.314s. Batch: 1.456s. Loss: 2.1633. Loss_x: 1.8248. Loss_u: 0.3385. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 328/ 512. Iter:   32/  32. LR: 0.0191. Data: 1.328s. Batch: 1.469s. Loss: 2.1232. Loss_x: 1.7670. Loss_u: 0.3562. Mask: 0.23. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 329/ 512. Iter:   32/  32. LR: 0.0190. Data: 1.322s. Batch: 1.467s. Loss: 2.2773. Loss_x: 1.9126. Loss_u: 0.3647. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 330/ 512. Iter:   32/  32. LR: 0.0190. Data: 1.322s. Batch: 1.467s. Loss: 2.0885. Loss_x: 1.7499. Loss_u: 0.3387. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 331/ 512. Iter:   32/  32. LR: 0.0189. Data: 1.326s. Batch: 1.466s. Loss: 2.1505. Loss_x: 1.7772. Loss_u: 0.3733. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 332/ 512. Iter:   32/  32. LR: 0.0189. Data: 1.322s. Batch: 1.466s. Loss: 2.2690. Loss_x: 1.9385. Loss_u: 0.3305. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 333/ 512. Iter:   32/  32. LR: 0.0188. Data: 1.315s. Batch: 1.460s. Loss: 2.1263. Loss_x: 1.7927. Loss_u: 0.3336. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 334/ 512. Iter:   32/  32. LR: 0.0187. Data: 1.331s. Batch: 1.472s. Loss: 2.0447. Loss_x: 1.7067. Loss_u: 0.3380. Mask: 0.21. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 335/ 512. Iter:   32/  32. LR: 0.0187. Data: 1.314s. Batch: 1.456s. Loss: 2.3328. Loss_x: 1.9700. Loss_u: 0.3628. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 336/ 512. Iter:   32/  32. LR: 0.0186. Data: 1.322s. Batch: 1.464s. Loss: 2.2014. Loss_x: 1.8456. Loss_u: 0.3558. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 337/ 512. Iter:   32/  32. LR: 0.0185. Data: 1.314s. Batch: 1.459s. Loss: 2.1731. Loss_x: 1.8181. Loss_u: 0.3550. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 338/ 512. Iter:   32/  32. LR: 0.0185. Data: 1.322s. Batch: 1.465s. Loss: 2.1592. Loss_x: 1.7843. Loss_u: 0.3749. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 339/ 512. Iter:   32/  32. LR: 0.0184. Data: 1.314s. Batch: 1.460s. Loss: 2.2080. Loss_x: 1.8111. Loss_u: 0.3969. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 340/ 512. Iter:   32/  32. LR: 0.0183. Data: 1.326s. Batch: 1.475s. Loss: 2.2624. Loss_x: 1.9005. Loss_u: 0.3619. Mask: 0.23. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 341/ 512. Iter:   32/  32. LR: 0.0183. Data: 1.324s. Batch: 1.465s. Loss: 2.1428. Loss_x: 1.8284. Loss_u: 0.3144. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 342/ 512. Iter:   32/  32. LR: 0.0182. Data: 1.321s. Batch: 1.464s. Loss: 2.1479. Loss_x: 1.8167. Loss_u: 0.3313. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 343/ 512. Iter:   32/  32. LR: 0.0182. Data: 1.315s. Batch: 1.455s. Loss: 2.0845. Loss_x: 1.7385. Loss_u: 0.3460. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 344/ 512. Iter:   32/  32. LR: 0.0181. Data: 1.329s. Batch: 1.469s. Loss: 2.2304. Loss_x: 1.8680. Loss_u: 0.3624. Mask: 0.23. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 345/ 512. Iter:   32/  32. LR: 0.0180. Data: 1.333s. Batch: 1.478s. Loss: 2.2082. Loss_x: 1.8377. Loss_u: 0.3704. Mask: 0.22. : 100%|██████████| 32/32 [00:47<00:00,  1.48s/it]\nTrain Epoch: 346/ 512. Iter:   32/  32. LR: 0.0180. Data: 1.343s. Batch: 1.484s. Loss: 2.1072. Loss_x: 1.7591. Loss_u: 0.3480. Mask: 0.22. : 100%|██████████| 32/32 [00:47<00:00,  1.48s/it]\nTrain Epoch: 347/ 512. Iter:   32/  32. LR: 0.0179. Data: 1.319s. Batch: 1.458s. Loss: 2.0480. Loss_x: 1.7524. Loss_u: 0.2956. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 348/ 512. Iter:   32/  32. LR: 0.0178. Data: 1.311s. Batch: 1.455s. Loss: 2.1706. Loss_x: 1.8526. Loss_u: 0.3179. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 349/ 512. Iter:   32/  32. LR: 0.0178. Data: 1.303s. Batch: 1.445s. Loss: 2.2550. Loss_x: 1.9177. Loss_u: 0.3373. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 350/ 512. Iter:   32/  32. LR: 0.0177. Data: 1.322s. Batch: 1.462s. Loss: 2.0255. Loss_x: 1.6859. Loss_u: 0.3396. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 351/ 512. Iter:   32/  32. LR: 0.0176. Data: 1.308s. Batch: 1.449s. Loss: 1.9981. Loss_x: 1.6975. Loss_u: 0.3007. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 352/ 512. Iter:   32/  32. LR: 0.0176. Data: 1.327s. Batch: 1.472s. Loss: 2.0624. Loss_x: 1.7559. Loss_u: 0.3065. Mask: 0.18. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 353/ 512. Iter:   32/  32. LR: 0.0175. Data: 1.311s. Batch: 1.454s. Loss: 2.0546. Loss_x: 1.7397. Loss_u: 0.3149. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 354/ 512. Iter:   32/  32. LR: 0.0174. Data: 1.289s. Batch: 1.435s. Loss: 2.0566. Loss_x: 1.7477. Loss_u: 0.3090. Mask: 0.20. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 355/ 512. Iter:   32/  32. LR: 0.0174. Data: 1.304s. Batch: 1.449s. Loss: 2.1409. Loss_x: 1.8262. Loss_u: 0.3147. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 356/ 512. Iter:   32/  32. LR: 0.0173. Data: 1.283s. Batch: 1.431s. Loss: 2.0321. Loss_x: 1.7117. Loss_u: 0.3204. Mask: 0.20. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 357/ 512. Iter:   32/  32. LR: 0.0172. Data: 1.288s. Batch: 1.431s. Loss: 2.0289. Loss_x: 1.6954. Loss_u: 0.3335. Mask: 0.21. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 358/ 512. Iter:   32/  32. LR: 0.0172. Data: 1.286s. Batch: 1.430s. Loss: 2.0641. Loss_x: 1.7579. Loss_u: 0.3062. Mask: 0.20. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 359/ 512. Iter:   32/  32. LR: 0.0171. Data: 1.319s. Batch: 1.461s. Loss: 2.0957. Loss_x: 1.8091. Loss_u: 0.2866. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 360/ 512. Iter:   32/  32. LR: 0.0170. Data: 1.295s. Batch: 1.440s. Loss: 2.2794. Loss_x: 1.9497. Loss_u: 0.3297. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 361/ 512. Iter:   32/  32. LR: 0.0170. Data: 1.317s. Batch: 1.456s. Loss: 2.1759. Loss_x: 1.8529. Loss_u: 0.3229. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 362/ 512. Iter:   32/  32. LR: 0.0169. Data: 1.322s. Batch: 1.460s. Loss: 2.0796. Loss_x: 1.7626. Loss_u: 0.3170. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 363/ 512. Iter:   32/  32. LR: 0.0168. Data: 1.299s. Batch: 1.444s. Loss: 2.1613. Loss_x: 1.8204. Loss_u: 0.3409. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 364/ 512. Iter:   32/  32. LR: 0.0168. Data: 1.304s. Batch: 1.447s. Loss: 2.0615. Loss_x: 1.7630. Loss_u: 0.2985. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 365/ 512. Iter:   32/  32. LR: 0.0167. Data: 1.308s. Batch: 1.449s. Loss: 2.0032. Loss_x: 1.6904. Loss_u: 0.3128. Mask: 0.22. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 366/ 512. Iter:   32/  32. LR: 0.0166. Data: 1.317s. Batch: 1.465s. Loss: 2.0145. Loss_x: 1.6849. Loss_u: 0.3297. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 367/ 512. Iter:   32/  32. LR: 0.0166. Data: 1.306s. Batch: 1.448s. Loss: 1.9637. Loss_x: 1.6725. Loss_u: 0.2912. Mask: 0.18. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 368/ 512. Iter:   32/  32. LR: 0.0165. Data: 1.303s. Batch: 1.452s. Loss: 1.9729. Loss_x: 1.6497. Loss_u: 0.3232. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 369/ 512. Iter:   32/  32. LR: 0.0164. Data: 1.297s. Batch: 1.443s. Loss: 2.0329. Loss_x: 1.7272. Loss_u: 0.3057. Mask: 0.21. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 370/ 512. Iter:   32/  32. LR: 0.0164. Data: 1.311s. Batch: 1.458s. Loss: 1.9473. Loss_x: 1.6772. Loss_u: 0.2701. Mask: 0.19. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 371/ 512. Iter:   32/  32. LR: 0.0163. Data: 1.307s. Batch: 1.454s. Loss: 1.8736. Loss_x: 1.6033. Loss_u: 0.2702. Mask: 0.18. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 372/ 512. Iter:   32/  32. LR: 0.0162. Data: 1.321s. Batch: 1.461s. Loss: 1.9182. Loss_x: 1.6126. Loss_u: 0.3055. Mask: 0.19. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 373/ 512. Iter:   32/  32. LR: 0.0162. Data: 1.324s. Batch: 1.468s. Loss: 1.9688. Loss_x: 1.6911. Loss_u: 0.2778. Mask: 0.18. : 100%|██████████| 32/32 [00:46<00:00,  1.47s/it]\nTrain Epoch: 374/ 512. Iter:   32/  32. LR: 0.0161. Data: 1.314s. Batch: 1.455s. Loss: 1.9427. Loss_x: 1.6469. Loss_u: 0.2958. Mask: 0.19. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 375/ 512. Iter:   32/  32. LR: 0.0160. Data: 1.312s. Batch: 1.453s. Loss: 1.8304. Loss_x: 1.5795. Loss_u: 0.2509. Mask: 0.18. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 376/ 512. Iter:   32/  32. LR: 0.0160. Data: 1.300s. Batch: 1.443s. Loss: 1.7419. Loss_x: 1.4968. Loss_u: 0.2451. Mask: 0.17. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 377/ 512. Iter:   32/  32. LR: 0.0159. Data: 1.302s. Batch: 1.448s. Loss: 1.9859. Loss_x: 1.6914. Loss_u: 0.2945. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 378/ 512. Iter:   32/  32. LR: 0.0158. Data: 1.330s. Batch: 1.471s. Loss: 1.9946. Loss_x: 1.7123. Loss_u: 0.2823. Mask: 0.19. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 379/ 512. Iter:   32/  32. LR: 0.0158. Data: 1.311s. Batch: 1.453s. Loss: 1.9475. Loss_x: 1.6818. Loss_u: 0.2657. Mask: 0.19. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 380/ 512. Iter:   32/  32. LR: 0.0157. Data: 1.328s. Batch: 1.472s. Loss: 1.9538. Loss_x: 1.6704. Loss_u: 0.2834. Mask: 0.20. : 100%|██████████| 32/32 [00:47<00:00,  1.47s/it]\nTrain Epoch: 381/ 512. Iter:   32/  32. LR: 0.0156. Data: 1.306s. Batch: 1.452s. Loss: 1.8676. Loss_x: 1.5769. Loss_u: 0.2907. Mask: 0.19. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 382/ 512. Iter:   32/  32. LR: 0.0156. Data: 1.314s. Batch: 1.455s. Loss: 1.9418. Loss_x: 1.6617. Loss_u: 0.2801. Mask: 0.18. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 383/ 512. Iter:   32/  32. LR: 0.0155. Data: 1.297s. Batch: 1.439s. Loss: 1.8309. Loss_x: 1.5405. Loss_u: 0.2904. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.44s/it]\nTrain Epoch: 384/ 512. Iter:   32/  32. LR: 0.0154. Data: 1.317s. Batch: 1.456s. Loss: 1.9480. Loss_x: 1.6895. Loss_u: 0.2585. Mask: 0.18. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 385/ 512. Iter:   32/  32. LR: 0.0154. Data: 1.307s. Batch: 1.452s. Loss: 1.9217. Loss_x: 1.6566. Loss_u: 0.2651. Mask: 0.19. : 100%|██████████| 32/32 [00:46<00:00,  1.45s/it]\nTrain Epoch: 386/ 512. Iter:   32/  32. LR: 0.0153. Data: 1.306s. Batch: 1.459s. Loss: 1.8237. Loss_x: 1.5528. Loss_u: 0.2709. Mask: 0.20. : 100%|██████████| 32/32 [00:46<00:00,  1.46s/it]\nTrain Epoch: 387/ 512. Iter:   32/  32. LR: 0.0152. Data: 1.291s. Batch: 1.435s. Loss: 1.8637. Loss_x: 1.6013. Loss_u: 0.2625. Mask: 0.17. : 100%|██████████| 32/32 [00:45<00:00,  1.43s/it]\nTrain Epoch: 388/ 512. Iter:   22/  32. LR: 0.0152. Data: 1.348s. Batch: 1.489s. Loss: 1.8216. Loss_x: 1.5630. Loss_u: 0.2586. Mask: 0.18. :  69%|██████▉   | 22/32 [00:32<00:14,  1.43s/it]","output_type":"stream"}]},{"cell_type":"code","source":"\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\ncm =torch.tensor([[181.,  16.,  62.,  41.],\n        [172.,  10.,  82.,  42.],\n        [163.,  22., 168.,  52.],\n        [170.,   3.,  75.,  52.]])\n\naccuracy = get_accuracy(cm)\nprint(f\"Accuracy: {accuracy:.2f}%\")\ncm = cm.to(torch.int)\n# Define the confusion matrix (replace this with your own)\nconfusion_matrix = cm\n\n# Define the class names (replace this with your own)\nclass_names = ['0', '1', '2','3']\n\n# Normalize the confusion matrix\n#confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n\n# Define the figure and axes\nfig, ax = plt.subplots()\nim = ax.imshow(confusion_matrix, cmap='Blues')\n\n# Set the tick labels\nax.set_xticks(np.arange(len(class_names)))\nax.set_yticks(np.arange(len(class_names)))\nax.set_xticklabels(class_names)\nax.set_yticklabels(class_names)\n\n# Rotate the tick labels and set their alignment\nplt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n\n# Add the values to the heatmap\nfor i in range(len(class_names)):\n    for j in range(len(class_names)):\n        text = ax.text(j, i, '{:.2f}'.format(confusion_matrix[i, j]),\n                       ha='center', va='center', color='black')\n\n# Set the title and axis labels\nax.set_title('Confusion Matrix')\nax.set_xlabel('Predicted label')\nax.set_ylabel('True label')\n\n# Add a colorbar\ncbar = ax.figure.colorbar(im, ax=ax)\n\n# Show the plot\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.550319Z","iopub.status.idle":"2023-05-08T23:01:15.550800Z","shell.execute_reply.started":"2023-05-08T23:01:15.550550Z","shell.execute_reply":"2023-05-08T23:01:15.550589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" print(f\"Accuracy: {accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.552209Z","iopub.status.idle":"2023-05-08T23:01:15.552806Z","shell.execute_reply.started":"2023-05-08T23:01:15.552537Z","shell.execute_reply":"2023-05-08T23:01:15.552581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = np.trace(cm) / float(np.sum(cm))\nprecision = np.diagonal(cm) / np.sum(cm, axis=0)\nrecall = np.diagonal(cm) / np.sum(cm, axis=1)\nf1_score = 2 * precision * recall / (precision + recall)\n\nprint(\"Accuracy: {:.3f}\".format(accuracy))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1_score))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:01:15.554467Z","iopub.status.idle":"2023-05-08T23:01:15.555132Z","shell.execute_reply.started":"2023-05-08T23:01:15.554879Z","shell.execute_reply":"2023-05-08T23:01:15.554902Z"},"trusted":true},"execution_count":null,"outputs":[]}]}